{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding using TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. There are various different embedding techniques present. In this notebook we make use of TF-IDF embedding technique. TF-IDF stands for Term frequency - Inverse Document Frequency. TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we extract text data from a wikipedia page and apply TF-IDF word embedding technique to the extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We import TfidfVectorizer from sklearn for TF-IDF computations\n",
    "* Other libraries like requests , BeautifulSoup are used to extract text data from the Wikipedia page\n",
    "* Nltk is used to process the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gaura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the different functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to the Wikipedia page is the first step in the whole process. initResp() function helps in achieving the first step. This is then used in the extractText function which is the function used to extract the text from the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initResp(url):\n",
    "    response = requests.get(url)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are connected to the Wikipedia page we need to extract the title and the data written about the topic for further processing. titleExtractor and contextExtractor do this by using the beautifulSoup library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titleExtractor(soup):\n",
    "    title = soup.find('title')\n",
    "    return title.string\n",
    "\n",
    "def contextExtractor(soup):\n",
    "    context = \" \"\n",
    "    for i in soup.select('p'):\n",
    "        context = context + i.getText()\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before computing TF-IDF we have to remove all the stopwords or else even the stop words will be considered while performing the computations. stopWordDel uses the stopword set from nltk to remove all the stopwords present in the textdata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordDel(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filteredSent = \" \"\n",
    "    pageWord = text.split()\n",
    "    for r in pageWord:\n",
    "        if not r in stop_words:\n",
    "            filteredSent = filteredSent + \" \" + r\n",
    "    return filteredSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data and Computing TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the sub functions are defined for performing basic functionalities we can start focusing on the functions which perform the extraction and computing.<br>\n",
    "* extractText() function first connects with the wikipedia page and then extracts the title and context using the titleExtractor() and contextExtractor() . The extracted data is then converted into lowercase before feeding it to the stopWordDel() function. After removal of stopwords, numerical characters are also deleted from the text data. At the end the processed text data is then split into different sentences using nltk library and stored in a list. This list is then returned when the function is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractText(url):\n",
    "    response=initResp(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = titleExtractor(soup)\n",
    "    context = contextExtractor(soup)\n",
    "    res = stopWordDel(context.lower())\n",
    "    filteredContext = ''.join([i for i in res if not i.isdigit()])\n",
    "    sentenceList = nltk.tokenize.sent_tokenize(filteredContext)\n",
    "    return sentenceList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* computeIDF makes use of the extractText() function to extract the sentence list from the wikipedia page. TfidVectorizer from sklearn is used. The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. A dataframe is returned which contains the TFIDF computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(url):\n",
    "    sentenceList= extractText(url)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectors = vectorizer.fit_transform(sentenceList)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    dense = vectors.todense()\n",
    "    denselist = dense.tolist()\n",
    "    df = pd.DataFrame(denselist, columns=feature_names)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass a wikipedia url for genocide to the computeIDF function. It returns a dataframe containing all the computations. We have displayed the head of that dataframe for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abetting</th>\n",
       "      <th>abortion</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abstained</th>\n",
       "      <th>academics</th>\n",
       "      <th>acceptance</th>\n",
       "      <th>accepted</th>\n",
       "      <th>access</th>\n",
       "      <th>accomplished</th>\n",
       "      <th>according</th>\n",
       "      <th>...</th>\n",
       "      <th>yazidi</th>\n",
       "      <th>yazidis</th>\n",
       "      <th>years</th>\n",
       "      <th>yemen</th>\n",
       "      <th>yet</th>\n",
       "      <th>york</th>\n",
       "      <th>yugoslavia</th>\n",
       "      <th>zdravko</th>\n",
       "      <th>zepa</th>\n",
       "      <th>γένος</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.209056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1537 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abetting  abortion  absolute  abstained  academics  acceptance  accepted  \\\n",
       "0       0.0       0.0       0.0        0.0        0.0         0.0       0.0   \n",
       "1       0.0       0.0       0.0        0.0        0.0         0.0       0.0   \n",
       "2       0.0       0.0       0.0        0.0        0.0         0.0       0.0   \n",
       "3       0.0       0.0       0.0        0.0        0.0         0.0       0.0   \n",
       "4       0.0       0.0       0.0        0.0        0.0         0.0       0.0   \n",
       "\n",
       "   access  accomplished  according  ...  yazidi  yazidis  years  yemen  yet  \\\n",
       "0     0.0           0.0        0.0  ...     0.0      0.0    0.0    0.0  0.0   \n",
       "1     0.0           0.0        0.0  ...     0.0      0.0    0.0    0.0  0.0   \n",
       "2     0.0           0.0        0.0  ...     0.0      0.0    0.0    0.0  0.0   \n",
       "3     0.0           0.0        0.0  ...     0.0      0.0    0.0    0.0  0.0   \n",
       "4     0.0           0.0        0.0  ...     0.0      0.0    0.0    0.0  0.0   \n",
       "\n",
       "   york  yugoslavia  zdravko  zepa     γένος  \n",
       "0   0.0         0.0      0.0   0.0  0.000000  \n",
       "1   0.0         0.0      0.0   0.0  0.209056  \n",
       "2   0.0         0.0      0.0   0.0  0.000000  \n",
       "3   0.0         0.0      0.0   0.0  0.000000  \n",
       "4   0.0         0.0      0.0   0.0  0.000000  \n",
       "\n",
       "[5 rows x 1537 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = computeIDF('https://en.wikipedia.org/wiki/Genocide')\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
