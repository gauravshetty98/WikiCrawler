Web scraping - Wikipedia," Web scraping, web harvesting, or web data extraction is data scraping used for extracting data from websites. Web scraping software may access the World Wide Web directly using the Hypertext Transfer Protocol, or through a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a bot or web crawler. It is a form of copying, in which specific data is gathered and copied from the web, typically into a central local database or spreadsheet, for later retrieval or analysis.
Web scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, then extraction can take place. The content of a page may be parsed, searched, reformatted, its data copied into a spreadsheet, and so on. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be to find and copy names and telephone numbers, or companies and their URLs, to a list (contact scraping).
Web scraping is used for contact scraping, and as a component of applications used for web indexing, web mining and data mining, online price change monitoring and price comparison, product review scraping (to watch the competition), gathering real estate listings, weather data monitoring, website change detection, research, tracking online presence and reputation, web mashup and, web data integration.
Web pages are built using text-based mark-up languages (HTML and XHTML), and frequently contain a wealth of useful data in text form. However, most web pages are designed for human end-users and not for ease of automated use. As a result, specialized tools and software have been developed to facilitate the scraping of web pages.
Newer forms of web scraping involve listening to data feeds from web servers.  For example, JSON is commonly used as a transport storage mechanism between the client and the web server.
There are methods that some websites use to prevent web scraping, such as detecting and disallowing bots from crawling (viewing) their pages. In response, there are web scraping systems that rely on using techniques in  DOM parsing, computer vision and natural language processing to simulate human browsing to enable gathering web page content for offline parsing.
The history of the web scraping dates back nearly to the time when the Internet was born.
Web scraping is the process of automatically mining data or collecting information from the World Wide Web. It is a field with active developments sharing a common goal with the semantic web vision, an ambitious initiative that still requires breakthroughs in text processing, semantic understanding, artificial intelligence and human-computer interactions. Current web scraping solutions range from the ad-hoc, requiring human effort, to fully automated systems that are able to convert entire web sites into structured information, with limitations.
The simplest form of web scraping is manually copying and pasting data from a web page into a text file or spreadsheet. Sometimes even the best web-scraping technology cannot replace a human's manual examination and copy-and-paste, and sometimes this may be the only workable solution when the websites for scraping explicitly set up barriers to prevent machine automation.
A simple yet powerful approach to extract information from web pages can be based on the UNIX grep command or regular expression-matching facilities of programming languages (for instance Perl or Python).
Static and dynamic web pages can be retrieved by posting HTTP requests to the remote web server using socket programming.
Many websites have large collections of pages generated dynamically from an underlying structured source like a database. Data of the same category are typically encoded into similar pages by a common script or template. In data mining, a program that detects such templates in a particular information source, extracts its content and translates it into a relational form, is called a wrapper. Wrapper generation algorithms assume that input pages of a wrapper induction system conform to a common template and that they can be easily identified in terms of a URL common scheme.[2] Moreover, some semi-structured data query languages, such as XQuery and the HTQL, can be used to parse HTML pages and to retrieve and transform page content.
By embedding a full-fledged web browser, such as the Internet Explorer or the Mozilla browser control, programs can retrieve the dynamic content generated by client-side scripts. These browser controls also parse web pages into a DOM tree, based on which programs can retrieve parts of the pages. Languages such as Xpath can be used to parse the resulting DOM tree.
There are several companies that have developed vertical specific harvesting platforms. These platforms create and monitor a multitude of ""bots"" for specific verticals with no ""man in the loop"" (no direct human involvement), and no work related to a specific target site. The preparation involves establishing the knowledge base for the entire vertical and then the platform creates the bots automatically. The platform's robustness is measured by the quality of the information it retrieves (usually number of fields) and its scalability (how quick it can scale up to hundreds or thousands of sites). This scalability is mostly used to target the Long Tail of sites that common aggregators find complicated or too labor-intensive to harvest content from.
The pages being scraped may embrace metadata or semantic markups and annotations, which can be used to locate specific data snippets. If the annotations are embedded in the pages, as Microformat does, this technique can be viewed as a special case of DOM parsing. In another case, the annotations, organized into a semantic layer,[3] are stored and managed separately from the web pages, so the scrapers can retrieve data schema and instructions from this layer before scraping the pages.
There are efforts using machine learning and computer vision that attempt to identify and extract information from web pages by interpreting pages visually as a human being might.[4]
There are many software tools available that can be used to customize web-scraping solutions. This software may attempt to automatically recognize the data structure of a page or provide a recording interface that removes the necessity to manually write web-scraping code, or some scripting functions that can be used to extract and transform content, and database interfaces that can store the scraped data in local databases. Some web scraping software can also be used to extract data from an API directly.
The legality of web scraping varies across the world. In general, web scraping may be against the terms of use of some websites, but the enforceability of these terms is unclear.[5]
In the United States, website owners can use three major legal claims to prevent undesired web scraping: (1) copyright infringement (compilation), (2) violation of the Computer Fraud and Abuse Act (""CFAA""), and (3) trespass to chattel.[6] However, the effectiveness of these claims relies upon meeting various criteria, and the case law is still evolving. For example, with regard to copyright, while outright duplication of original expression will in many cases be illegal, in the United States the courts ruled in Feist Publications v. Rural Telephone Service that duplication of facts is allowable.
U.S. courts have acknowledged that users of ""scrapers"" or ""robots"" may be held liable for committing trespass to chattels,[7][8] which involves a computer system itself being considered personal property upon which the user of a scraper is trespassing. The best known of these cases, eBay v. Bidder's Edge, resulted in an injunction ordering Bidder's Edge to stop accessing, collecting, and indexing auctions from the eBay web site. This case involved automatic placing of bids, known as auction sniping. However, in order to succeed on a claim of trespass to chattels, the plaintiff must demonstrate that the defendant intentionally and without authorization interfered with the plaintiff's possessory interest in the computer system and that the defendant's unauthorized use caused damage to the plaintiff. Not all cases of web spidering brought before the courts have been considered trespass to chattels.[9]
One of the first major tests of screen scraping involved American Airlines (AA), and a firm called FareChase.[10] AA successfully obtained an injunction from a Texas trial court, stopping FareChase from selling software that enables users to compare online fares if the software also searches AA's website. The airline argued that FareChase's websearch software trespassed on AA's servers when it collected the publicly available data. FareChase filed an appeal in March 2003. By June, FareChase and AA agreed to settle and the appeal was dropped.[11]
Southwest Airlines has also challenged screen-scraping practices, and has involved both FareChase and another firm, Outtask, in a legal claim. Southwest Airlines charged that the screen-scraping is Illegal since it is an example of ""Computer Fraud and Abuse"" and has led to ""Damage and Loss"" and ""Unauthorized Access"" of Southwest's site. It also constitutes ""Interference with Business Relations"", ""Trespass"", and ""Harmful Access by Computer"". They also claimed that screen-scraping constitutes what is legally known as ""Misappropriation and Unjust Enrichment"", as well as being a breach of the web site's user agreement. Outtask denied all these claims, claiming that the prevailing law, in this case, should be US Copyright law and that under copyright, the pieces of information being scraped would not be subject to copyright protection. Although the cases were never resolved in the Supreme Court of the United States, FareChase was eventually shuttered by parent company Yahoo!, and Outtask was purchased by travel expense company Concur.[12]
In 2012, a startup called 3Taps scraped classified housing ads from Craigslist. Craigslist sent 3Taps a cease-and-desist letter and blocked their IP addresses and later sued, in Craigslist v. 3Taps. The court held that the cease-and-desist letter and IP blocking was sufficient for Craigslist to properly claim that 3Taps had violated the Computer Fraud and Abuse Act.
Although these are early scraping decisions, and the theories of liability are not uniform, it is difficult to ignore a pattern emerging that the courts are prepared to protect proprietary content on commercial sites from uses which are undesirable to the owners of such sites. However, the degree of protection for such content is not settled and will depend on the type of access made by the scraper, the amount of information accessed and copied, the degree to which the access adversely affects the site owner's system and the types and manner of prohibitions on such conduct.[13]
While the law in this area becomes more settled, entities contemplating using scraping programs to access a public web site should also consider whether such action is authorized by reviewing the terms of use and other terms or notices posted on or made available through the site. In a 2010 ruling in the Cvent, Inc. v. Eventbrite, Inc. In the United States district court for the eastern district of Virginia, the court ruled that the terms of use should be brought to the users' attention In order for a browse wrap contract or license to be enforced.[14] In a 2014 case, filed in the United States District Court for the Eastern District of Pennsylvania,[15] e-commerce site QVC objected to the Pinterest-like shopping aggregator Resultly's 'scraping of QVC's site for real-time pricing data. QVC alleges that Resultly ""excessively crawled"" QVC's retail site (allegedly sending 200-300 search requests to QVC's website per minute, sometimes to up to 36,000 requests per minute) which caused QVC's site to crash for two days, resulting in lost sales for QVC.[16] QVC's complaint alleges that the defendant disguised its web crawler to mask its source IP address and thus prevented QVC from quickly repairing the problem. This is a particularly interesting scraping case because QVC is seeking damages for the unavailability of their website, which QVC claims was caused by Resultly.
In the plaintiff's web site during the period of this trial, the terms of use link are displayed among all the links of the site, at the bottom of the page as most sites on the internet. This ruling contradicts the Irish ruling described below. The court also rejected the plaintiff's argument that the browse-wrap restrictions were enforceable in view of Virginia's adoption of the Uniform Computer Information Transactions Act (UCITA)—a uniform law that many believed was in favor on common browse-wrap contracting practices.[17]
In Facebook, Inc. v. Power Ventures, Inc., a district court ruled in 2012 that Power Ventures could not scrape Facebook pages on behalf of a Facebook user. The case is on appeal, and the Electronic Frontier Foundation filed a brief in 2015 asking that it be overturned.[18][19] In Associated Press v. Meltwater U.S. Holdings, Inc., a court in the US held Meltwater liable for scraping and republishing news information from the Associated Press, but a court in the United Kingdom held in favor of Meltwater.
Internet Archive collects and distributes a significant number of publicly available web pages without being considered to be in violation of copyright laws.
In February 2006, the Danish Maritime and Commercial Court (Copenhagen) ruled that systematic crawling, indexing, and deep linking by portal site ofir.dk of estate site Home.dk does not conflict with Danish law or the database directive of the European Union.[20]
In a February 2010 case complicated by matters of jurisdiction, Ireland's High Court delivered a verdict that illustrates the inchoate state of developing case law. In the case of Ryanair Ltd v Billigfluege.de GmbH, Ireland's High Court ruled Ryanair's ""click-wrap"" agreement to be legally binding. In contrast to the findings of the United States District Court Eastern District of Virginia and those of the Danish Maritime and Commercial Court, Justice Michael Hanna ruled that the hyperlink to Ryanair's terms and conditions was plainly visible, and that placing the onus on the user to agree to terms and conditions in order to gain access to online services is sufficient to comprise a contractual relationship.
[21] The decision is under appeal in Ireland's Supreme Court.[22]
On April 30, 2020, the French Data Protection Authority (CNIL) released new guidelines on web scraping.[23] The CNIL guidelines made it clear that publicly available data is still personal data and cannot be repurposed without the knowledge of the person to whom that data belongs.[24]
In Australia, the Spam Act 2003 outlaws some forms of web harvesting, although this only applies to email addresses.[25][26]
Leaving a few cases dealing with IPR infringement, Indian courts have not expressly ruled on the legality of web scraping. However, since all common forms of electronic contracts are enforceable in India, violating the terms of use prohibiting data scraping will be a violation of the contract law. It will also violate the Information Technology Act, 2000, which penalizes unauthorized access to a computer resource or extracting data from a computer resource.
The administrator of a website can use various measures to stop or slow a bot. Some techniques include:
"
Perl - Wikipedia," 
Perl is a family of two high-level, general-purpose, interpreted, dynamic programming languages. ""Perl"" refers to Perl 5, but from 2000 to 2019 it also referred to its redesigned ""sister language"", Perl 6, before the latter's name was officially changed to Raku in October 2019.[8][9]
Though Perl is not officially an acronym,[10] there are various backronyms in use, including ""Practical Extraction and Reporting Language"".[11] Perl was originally developed by Larry Wall in 1987 as a general-purpose Unix scripting language to make report processing easier.[12] Since then, it has undergone many changes and revisions. Raku, which began as a redesign of Perl 5 in 2000, eventually evolved into a separate language. Both languages continue to be developed independently by different development teams and liberally borrow ideas from one another.
The Perl languages borrow features from other programming languages including C, shell script (sh), AWK, and sed;[13] Wall also alludes to BASIC and Lisp in the introduction to Learning Perl (Schwartz & Christiansen) and so on.[14] They provide text processing facilities without the arbitrary data-length limits of many contemporary Unix command line tools,[15] facilitating manipulation of text files. Perl 5 gained widespread popularity in the late 1990s as a CGI scripting language, in part due to its unsurpassed regular expression and string parsing abilities.[16][17][18][19]
In addition to CGI, Perl 5 is used for system administration, network programming, finance, bioinformatics, and other applications, such as for GUIs. It has been nicknamed ""the Swiss Army chainsaw of scripting languages"" because of its flexibility and power,[20] and also its ugliness.[21] In 1998, it was also referred to as the ""duct tape that holds the Internet together,"" in reference to both its ubiquitous use as a glue language and its perceived inelegance.[22]
Perl is a highly expressive programming language: source code for a given algorithm can be short and highly compressible.[23][24]
Larry Wall began work on Perl in 1987, while working as a programmer at Unisys,[15] and released version 1.0 to the comp.sources.misc newsgroup on December 18, 1987.[25] The language expanded rapidly over the next few years.
Perl 2, released in 1988, featured a better regular expression engine. Perl 3, released in 1989, added support for binary data streams.[citation needed]
Originally, the only documentation for Perl was a single lengthy man page. In 1991, Programming Perl, known to many Perl programmers as the ""Camel Book"" because of its cover, was published and became the de facto reference for the language. At the same time, the Perl version number was bumped to 4, not to mark a major change in the language but to identify the version that was well documented by the book.[citation needed]
Perl 4 went through a series of maintenance releases, culminating in Perl 4.036 in 1993, whereupon Wall abandoned Perl 4 to begin work on Perl 5. Initial design of Perl 5 continued into 1994. The perl5-porters mailing list was established in May 1994 to coordinate work on porting Perl 5 to different platforms. It remains the primary forum for development, maintenance, and porting of Perl 5.[26]
Perl 5.000 was released on October 17, 1994.[27] It was a nearly complete rewrite of the interpreter, and it added many new features to the language, including objects, references, lexical (my) variables, and modules. Importantly, modules provided a mechanism for extending the language without modifying the interpreter. This allowed the core interpreter to stabilize, even as it enabled ordinary Perl programmers to add new language features. Perl 5 has been in active development since then.[citation needed]
Perl 5.001 was released on March 13, 1995. Perl 5.002 was released on February 29, 1996 with the new prototypes feature. This allowed module authors to make subroutines that behaved like Perl builtins. Perl 5.003 was released June 25, 1996, as a security release.[citation needed]
One of the most important events in Perl 5 history took place outside of the language proper and was a consequence of its module support. On October 26, 1995, the Comprehensive Perl Archive Network (CPAN) was established as a repository for the Perl language and Perl modules; as of May 2017, it carries over 185,178 modules in 35,190 distributions, written by more than 13,071 authors, and is mirrored worldwide at more than 245 locations.[28]
Perl 5.004 was released on May 15, 1997, and included, among other things, the UNIVERSAL package, giving Perl a base object from which all classes were automatically derived and the ability to require versions of modules. Another significant development was the inclusion of the CGI.pm module,[29] which contributed to Perl's popularity as a CGI scripting language.[30]
Perl 5.004 also added support for Microsoft Windows and several other operating systems.[29]
Perl 5.005 was released on July 22, 1998. This release included several enhancements to the regex engine, new hooks into the backend through the B::* modules, the qr// regex quote operator, a large selection of other new core modules, and added support for several more operating systems, including BeOS.[31]
Perl 5.6 was released on March 22, 2000.  Major changes included 64-bit support, Unicode string representation, support for files over 2 GiB, and the ""our"" keyword.[34][35]  When developing Perl 5.6, the decision was made to switch the versioning scheme to one more similar to other open source projects; after 5.005_63, the next version became 5.5.640, with plans for development versions to have odd numbers and stable versions to have even numbers.[citation needed]
In 2000, Wall put forth a call for suggestions for a new version of Perl from the community. The process resulted in 361 RFC (request for comments) documents that were to be used in guiding development of Perl 6. In 2001,[36] work began on the ""Apocalypses"" for Perl 6, a series of documents meant to summarize the change requests and present the design of the next generation of Perl. They were presented as a digest of the RFCs, rather than a formal document. At this point, Perl 6 existed only as a description of a language.[citation needed]
Perl 5.8 was first released on July 18, 2002, and had nearly yearly updates since then. Perl 5.8 improved Unicode support, added a new I/O implementation, added a new thread implementation, improved numeric accuracy, and added several new modules.[37] As of 2013 this version still remains the most popular version of Perl and is used by Red Hat 5, Suse 10, Solaris 10, HP-UX 11.31 and AIX 5.[citation needed]
In 2004, work began on the ""Synopses"" – documents that originally summarized the Apocalypses, but which became the specification for the Perl 6 language. In February 2005, Audrey Tang began work on Pugs, a Perl 6 interpreter written in Haskell.[38] This was the first concerted effort towards making Perl 6 a reality. This effort stalled in 2006.[39]
On December 18, 2007, the 20th anniversary of Perl 1.0, Perl 5.10.0 was released. Perl 5.10.0 included notable new features, which brought it closer to Perl 6. These included a switch statement (called ""given""/""when""), regular expressions updates, and the 'smart match operator (~~).[40][41]
Around this same time, development began in earnest on another implementation of Perl 6 known as Rakudo Perl, developed in tandem with the Parrot virtual machine. As of November 2009, Rakudo Perl has had regular monthly releases and now is the most complete implementation of Perl 6.
A major change in the development process of Perl 5 occurred with Perl 5.11; the development community has switched to a monthly release cycle of development releases, with a yearly schedule of stable releases. By that plan, bugfix point releases will follow the stable releases every three months.[citation needed]
On April 12, 2010, Perl 5.12.0 was released. Notable core enhancements include new package NAME VERSION syntax, the Yada Yada operator (intended to mark placeholder code that is not yet implemented), implicit strictures, full Y2038 compliance, regex conversion overloading, DTrace support, and Unicode 5.2.[42] On January 21, 2011, Perl 5.12.3 was released; it contains updated modules and some documentation changes.[43] Version 5.12.4 was released on June 20, 2011. The latest version of that branch, 5.12.5, was released on November 10, 2012.[citation needed]
On May 14, 2011, Perl 5.14 was released. JSON support is built-in as of 5.14.0.[44] The latest version of that branch, 5.14.4, was released on March 10, 2013.[citation needed]
On May 20, 2012, Perl 5.16 was released. Notable new features include the ability to specify a given version of Perl that one wishes to emulate, allowing users to upgrade their version of Perl, but still run old scripts that would normally be incompatible.[45] Perl 5.16 also updates the core to support Unicode 6.1.[45]
On May 18, 2013, Perl 5.18 was released. Notable new features include the new dtrace hooks, lexical subs, more CORE:: subs, overhaul of the hash for security reasons, support for Unicode 6.2.[46]
On May 27, 2014, Perl 5.20 was released. Notable new features include subroutine signatures, hash slices/new slice syntax, postfix dereferencing (experimental), Unicode 6.3, rand() using consistent random number generator.[47]
Some observers credit the release of Perl 5.10 with the start of the Modern Perl movement.[48] In particular, this phrase describes a style of development that embraces the use of the CPAN, takes advantage of recent developments in the language, and is rigorous about creating high quality code.[49] While the book ""Modern Perl""[50] may be the most visible standard-bearer of this idea, other groups such as the Enlightened Perl Organization[51] have taken up the cause.
In late 2012 and 2013, several projects for alternative implementations for Perl 5 started: Perl5 in Perl6 by the Rakudo Perl team,[52] moe by Stevan Little and friends,[53] p2[54] by the Perl11 team under Reini Urban, gperl by goccy,[55] and rperl a kickstarter project led by Will Braswell and affiliated with the Perll11 project.[56]
In June 2020 Perl 7 was announced as the successor to Perl 5.[57] Perl 7 will initially be based on Perl 5.32 with release candidates expected later in 2020.[58] When Perl 7 is released, Perl 5 will go into long term maintenance. Supported Perl 5 versions however will continue to get important security and bug fixes.[59]
PONIE is an acronym for Perl On New Internal Engine. The PONIE Project existed from 2003 until 2006 and was to be a bridge between Perl 5 and Perl 6. It was an effort to rewrite the Perl 5 interpreter to run on Parrot, the Perl 6 virtual machine. The goal was to ensure the future of the millions of lines of Perl 5 code at thousands of companies around the world.[60]
The PONIE project ended in 2006 and is no longer being actively developed.  Some of the improvements made to the Perl 5 interpreter as part of PONIE were folded into that project.[61]
Perl was originally named ""Pearl"". Wall wanted to give the language a short name with positive connotations; he claims that he considered every three- and four-letter word in the dictionary. He also considered naming it after his wife Gloria. Wall discovered the existing PEARL programming language before Perl's official release and changed the spelling of the name.[62]
When referring to the language, the name is normally capitalized (Perl) as a proper noun. When referring to the interpreter program itself, the name is often uncapitalized (perl) because most Unix-like file systems are case-sensitive. Before the release of the first edition of Programming Perl, it was common to refer to the language as perl; Randal L. Schwartz, however, capitalized the language's name in the book to make it stand out better when typeset. This case distinction was subsequently documented as canonical.[63]
The name is occasionally expanded as Practical Extraction and Report Language, but this is a backronym.[64] Other expansions have been suggested as equally canonical, including Wall's own Pathologically Eclectic Rubbish Lister which is in the manual page for perl.[65] Indeed, Wall claims that the name was intended to inspire many different expansions.[66]
Programming Perl, published by O'Reilly Media, features a picture of a dromedary camel on the cover and is commonly called the ""Camel Book"".[67] This image of a camel has become an unofficial symbol of Perl as well as a general hacker emblem, appearing on T-shirts and other clothing items.[citation needed]
O'Reilly owns the image as a trademark but licenses it for non-commercial use, requiring only an acknowledgement and a link to www.perl.com. Licensing for commercial use is decided on a case-by-case basis.[68] O'Reilly also provides ""Programming Republic of Perl"" logos for non-commercial sites and ""Powered by Perl"" buttons for any site that uses Perl.[68]
The Perl Foundation owns an alternative symbol, an onion, which it licenses to its subsidiaries, Perl Mongers, PerlMonks, Perl.org, and others.[69] The symbol is a visual pun on pearl onion.[70]
Sebastian Riedel, the creator of Mojolicious, has created a logo depicting a raptor dinosaur, which is available under a CC-SA License, Version 4.0.[71] The logo is being remixed and used in different places and is symbolising Perl 5. The analogue of the raptor comes from a series of talks given by Matt S Trout beginning in 2010.[72] The talks were aimed at being more Perl 5 community-centric, in a period where Perl 6 was a hot topic.[citation needed]
According to Wall, Perl has two slogans. The first is ""There's more than one way to do it,"" commonly known as TMTOWTDI. The second slogan is ""Easy things should be easy and hard things should be possible"".[15]
The overall structure of Perl derives broadly from C. Perl is procedural in nature, with variables, expressions, assignment statements, brace-delimited blocks, control structures, and subroutines.[citation needed]
Perl also takes features from shell programming. All variables are marked with leading sigils, which allow variables to be interpolated directly into strings. However, unlike the shell, Perl uses sigils on all accesses to variables, and unlike most other programming languages that use sigils, the sigil doesn't denote the type of the variable but the type of the expression. So for example, to access a list of values in a hash, the sigil for an array (""@"") is used, not the sigil for a hash (""%"").
Perl also has many built-in functions that provide tools often used in shell programming (although many of these tools are implemented by programs external to the shell) such as sorting, and calling operating system facilities.[citation needed]
Perl takes lists from Lisp, hashes (""associative arrays"") from AWK, and regular expressions from sed. These simplify and facilitate many parsing, text-handling, and data-management tasks. Also shared with Lisp are the implicit return of the last value in a block, and the fact that all statements have a value, and thus are also expressions and can be used in larger expressions themselves.[citation needed]
Perl 5 added features that support complex data structures, first-class functions (that is, closures as values), and an object-oriented programming model. These include references, packages, class-based method dispatch, and lexically scoped variables, along with compiler directives (for example, the strict pragma). A major additional feature introduced with Perl 5 was the ability to package code as reusable modules. Wall later stated that ""The whole intent of Perl 5's module system was to encourage the growth of Perl culture rather than the Perl core.""[73]
All versions of Perl do automatic data-typing and automatic memory management. The interpreter knows the type and storage requirements of every data object in the program; it allocates and frees storage for them as necessary using reference counting (so it cannot deallocate circular data structures without manual intervention). Legal type conversions — for example, conversions from number to string — are done automatically at run time; illegal type conversions are fatal errors.[citation needed]
The design of Perl can be understood as a response to three broad trends in the computer industry: falling hardware costs, rising labor costs, and improvements in compiler technology. Many earlier computer languages, such as Fortran and C, aimed to make efficient use of expensive computer hardware. In contrast, Perl was designed so that computer programmers could write programs more quickly and easily.[citation needed]
Perl has many features that ease the task of the programmer at the expense of greater CPU and memory requirements. These include automatic memory management; dynamic typing; strings, lists, and hashes; regular expressions; introspection; and an eval() function. Perl follows the theory of ""no built-in limits,""[67] an idea similar to the Zero One Infinity rule.[citation needed]
Wall was trained as a linguist, and the design of Perl is very much informed by linguistic principles. Examples include Huffman coding (common constructions should be short), good end-weighting (the important information should come first), and a large collection of language primitives. Perl favors language constructs that are concise and natural for humans to write, even where they complicate the Perl interpreter.[citation needed]
Perl's syntax reflects the idea that ""things that are different should look different.""[74] For example, scalars, arrays, and hashes have different leading sigils. Array indices and hash keys use different kinds of braces. Strings and regular expressions have different standard delimiters. This approach can be contrasted with a language such as Lisp, where the same basic syntax, composed of simple and universal symbolic expressions, is used for all purposes.[citation needed]
Perl does not enforce any particular programming paradigm (procedural, object-oriented, functional, or others) or even require the programmer to choose among them.[citation needed]
There is a broad practical bent to both the Perl language and the community and culture that surround it. The preface to Programming Perl begins: ""Perl is a language for getting your job done.""[15] One consequence of this is that Perl is not a tidy language. It includes many features, tolerates exceptions to its rules, and employs heuristics to resolve syntactical ambiguities. Because of the forgiving nature of the compiler, bugs can sometimes be hard to find. Perl's function documentation remarks on the variant behavior of built-in functions in list and scalar contexts by saying, ""In general, they do what you want, unless you want consistency.""[75]
No written specification or standard for the Perl language exists for Perl versions through Perl 5, and there are no plans to create one for the current version of Perl. There has been only one implementation of the interpreter, and the language has evolved along with it. That interpreter, together with its functional tests, stands as a de facto specification of the language. Perl 6, however, started with a specification,[76] and several projects[77] aim to implement some or all of the specification.[citation needed]
Perl has many and varied applications, compounded by the availability of many standard and third-party modules.
Perl has chiefly been used to write CGI scripts: large projects written in Perl include cPanel, Slash, Bugzilla, RT, TWiki, and Movable Type; high-traffic websites that use Perl extensively include Priceline.com, Craigslist,[78] IMDb,[79] LiveJournal, DuckDuckGo,[80][81] Slashdot and Ticketmaster. 
It is also an optional component of the popular LAMP technology stack for Web development, in lieu of PHP or Python. Perl is used extensively as a system programming language in the Debian GNU/Linux distribution.[82]
Perl is often used as a glue language, tying together systems and interfaces that were not specifically designed to interoperate, and for ""data munging,""[83] that is, converting or processing large amounts of data for tasks such as creating reports. In fact, these strengths are intimately linked. The combination makes Perl a popular all-purpose language for system administrators, particularly because short programs, often called ""one-liner programs,"" can be entered and run on a single command line.[citation needed]
Perl code can be made portable across Windows and Unix; such code is often used by suppliers of software (both COTS and bespoke) to simplify packaging and maintenance of software build- and deployment-scripts.[citation needed]
Graphical user interfaces (GUIs) may be developed using Perl. For example, Perl/Tk and wxPerl are commonly used to enable user interaction with Perl scripts. Such interaction may be synchronous or asynchronous, using callbacks to update the GUI.[citation needed]
Perl is implemented as a core interpreter, written in C, together with a large collection of modules, written in Perl and C. As of 2010[update], the interpreter is 150,000 lines of C code and compiles to a 1 MB executable on typical machine architectures. Alternatively, the interpreter can be compiled to a link library and embedded in other programs. There are nearly 500 modules in the distribution, comprising 200,000 lines of Perl and an additional 350,000 lines of C code (much of the C code in the modules consists of character encoding tables).[citation needed]
The interpreter has an object-oriented architecture. All of the elements of the Perl language—scalars, arrays, hashes, coderefs, file handles—are represented in the interpreter by C structs. Operations on these structs are defined by a large collection of macros, typedefs, and functions; these constitute the Perl C API. The Perl API can be bewildering to the uninitiated, but its entry points follow a consistent naming scheme, which provides guidance to those who use it.[citation needed]
The life of a Perl interpreter divides broadly into a compile phase and a run phase.[84]  In Perl, the phases are the major stages in the interpreter's life-cycle. Each interpreter goes through each phase only once, and the phases follow in a fixed sequence.[citation needed]
Most of what happens in Perl's compile phase is compilation, and most of what happens in Perl's run phase is execution, but there are significant exceptions. Perl makes important use of its capability to execute Perl code during the compile phase. Perl will also delay compilation into the run phase. The terms that indicate the kind of processing that is actually occurring at any moment are compile time and run time.  Perl is in compile time at most points during the compile phase, but compile time may also be entered during the run phase. The compile time for code in a string argument passed to the eval built-in occurs during the run phase. Perl is often in run time during the compile phase and spends most of the run phase in run time.  Code in BEGIN blocks executes at run time but in the compile phase.
At compile time, the interpreter parses Perl code into a syntax tree. At run time, it executes the program by walking the tree. Text is parsed only once, and the syntax tree is subject to optimization before it is executed, so that execution is relatively efficient. Compile-time optimizations on the syntax tree include constant folding and context propagation, but peephole optimization is also performed.[citation needed]
Perl has a Turing-complete grammar because parsing can be affected by run-time code executed during the compile phase.[85] Therefore, Perl cannot be parsed by a straight Lex/Yacc lexer/parser combination. Instead, the interpreter implements its own lexer, which coordinates with a modified GNU bison parser to resolve ambiguities in the language.[citation needed]
It is often said that ""Only perl can parse Perl,""[86] meaning that only the Perl interpreter (perl) can parse the Perl language (Perl), but even this is not, in general, true. Because the Perl interpreter can simulate a Turing machine during its compile phase, it would need to decide the halting problem in order to complete parsing in every case. It is a long-standing result that the halting problem is undecidable, and therefore not even perl can always parse Perl. Perl makes the unusual choice of giving the user access to its full programming power in its own compile phase. The cost in terms of theoretical purity is high, but practical inconvenience seems to be rare.[citation needed]
Other programs that undertake to parse Perl, such as source-code analyzers and auto-indenters, have to contend not only with ambiguous syntactic constructs but also with the undecidability of Perl parsing in the general case. Adam Kennedy's PPI project focused on parsing Perl code as a document (retaining its integrity as a document), instead of parsing Perl as executable code (that not even Perl itself can always do). It was Kennedy who first conjectured that ""parsing Perl suffers from the 'halting problem',""[87] which was later proved.[88]
Perl is distributed with over 250,000 functional tests for core Perl language and over 250,000 functional tests for core modules. These run as part of the normal build process and extensively exercise the interpreter and its core modules. Perl developers rely on the functional tests to ensure that changes to the interpreter do not introduce software bugs; additionally, Perl users who see that the interpreter passes its functional tests on their system can have a high degree of confidence that it is working properly.[citation needed]
Perl is dual licensed under both the Artistic License 1.0[4][5] and the GNU General Public License.[6] Distributions are available for most operating systems. It is particularly prevalent on Unix and Unix-like systems, but it has been ported to most modern (and many obsolete) platforms. With only six[citation needed] reported exceptions, Perl can be compiled from source code on all POSIX-compliant, or otherwise-Unix-compatible platforms.[89]
Because of unusual changes required for the classic Mac OS environment, a special port called MacPerl was shipped independently.[90]
The Comprehensive Perl Archive Network carries a complete list of supported platforms with links to the distributions available on each.[91] CPAN is also the source for publicly available Perl modules that are not part of the core Perl distribution.[citation needed]
Users of Microsoft Windows typically install one of the native binary distributions of Perl for Win32, most commonly Strawberry Perl or ActivePerl. Compiling Perl from source code under Windows is possible, but most installations lack the requisite C compiler and build tools. This also makes it difficult to install modules from the CPAN, particularly those that are partially written in C.[citation needed]
ActivePerl is a closed source distribution from ActiveState that has regular releases that track the core Perl releases.[92] The distribution previously included the Perl package manager (PPM),[93] a popular tool for installing, removing, upgrading, and managing the use of common Perl modules, however this tool was discontinued as of ActivePerl 5.28.[94] Included also is PerlScript, a Windows Script Host (WSH) engine implementing the Perl language.  Visual Perl is an ActiveState tool that adds Perl to the Visual Studio .NET development suite.  A VBScript to Perl converter, as well as a Perl compiler for Windows, and converters of awk and sed to Perl have also been produced by this company and included on the ActiveState CD for Windows, which includes all of their distributions plus the Komodo IDE and all but the first on the Unix/Linux/Posix variant thereof in 2002 and subsequently.[95]
Strawberry Perl is an open source distribution for Windows.  It has had regular, quarterly releases since January 2008, including new modules as feedback and requests come in.  Strawberry Perl aims to be able to install modules like standard Perl distributions on other platforms, including compiling XS modules.[citation needed]
The Cygwin emulation layer is another way of running Perl under Windows. Cygwin provides a Unix-like environment on Windows, and both Perl and CPAN are available as standard pre-compiled packages in the Cygwin setup program. Since Cygwin also includes gcc, compiling Perl from source is also possible.[citation needed]
A perl executable is included in several Windows Resource kits in the directory with other scripting tools.[citation needed]
Implementations of Perl come with the MKS Toolkit, Interix (the base of earlier implementations of Windows Services for Unix), and UWIN.[citation needed]
Perl's text-handling capabilities can be used for generating SQL queries; arrays, hashes, and automatic memory management make it easy to collect and process the returned data. For example, in Tim Bunce's Perl DBI application programming interface (API), the arguments to the API can be the text of SQL queries; thus it is possible to program in multiple languages at the same time (e.g., for generating a Web page using HTML, JavaScript, and SQL in a here document). The use of Perl variable interpolation to programmatically customize each of the SQL queries, and the specification of Perl arrays or hashes as the structures to programmatically hold the resulting data sets from each SQL query, allows a high-level mechanism for handling large amounts of data for post-processing by a Perl subprogram.[96]
In early versions of Perl, database interfaces were created by relinking the interpreter with a client-side database library. This was sufficiently difficult that it was done for only a few of the most-important and most widely used databases, and it restricted the resulting perl executable to using just one database interface at a time.[citation needed]
In Perl 5, database interfaces are implemented by Perl DBI modules. The DBI (Database Interface) module presents a single, database-independent interface to Perl applications, while the DBD (Database Driver) modules handle the details of accessing some 50 different databases; there are DBD drivers for most ANSI SQL databases.[citation needed]
DBI provides caching for database handles and queries, which can greatly improve performance in long-lived execution environments such as mod perl,[97] helping high-volume systems avert load spikes as in the Slashdot effect.[citation needed]
In modern Perl applications, especially those written using web frameworks such as Catalyst, the DBI module is often used indirectly via object-relational mappers such as DBIx::Class, Class::DBI[98] or Rose::DB::Object[99] that generate SQL queries and handle data transparently to the application author.[citation needed]
The Computer Language Benchmarks Game compares the performance of implementations of typical programming problems in several programming languages.[100] The submitted Perl implementations typically perform toward the high end of the memory-usage spectrum and give varied speed results. Perl's performance in the benchmarks game is typical for interpreted languages.[101]
Large Perl programs start more slowly than similar programs in compiled languages because perl has to compile the source every time it runs. In a talk at the YAPC::Europe 2005 conference and subsequent article ""A Timely Start,"" Jean-Louis Leroy found that his Perl programs took much longer to run than expected because the perl interpreter spent significant time finding modules within his over-large include path.[102] Unlike Java, Python, and Ruby, Perl has only experimental support for pre-compiling.[103] Therefore, Perl programs pay this overhead penalty on every execution. The run phase of typical programs is long enough that amortized startup time is not substantial, but benchmarks that measure very short execution times are likely to be skewed due to this overhead.[citation needed]
A number of tools have been introduced to improve this situation. The first such tool was Apache's mod perl, which sought to address one of the most-common reasons that small Perl programs were invoked rapidly: CGI Web development. ActivePerl, via Microsoft ISAPI, provides similar performance improvements.[citation needed]
Once Perl code is compiled, there is additional overhead during the execution phase that typically isn't present for programs written in compiled languages such as C or C++. Examples of such overhead include bytecode interpretation, reference-counting memory management, and dynamic type-checking.[citation needed]
Because Perl is an interpreted language, it can give problems when efficiency is critical; in such situations, the most critical routines can be written in other languages (such as C), which can be connected to Perl via simple Inline modules or the more complex but flexible XS mechanism.[104]
Perl 5, the language usually referred to as ""Perl"", continues to be actively developed. Perl 5.12.0 was released in April 2010 with some new features influenced by the design of Perl 6,[42][105] followed by Perl 5.14.1 (released on June 17, 2011), Perl 5.16.1 (released on August 9, 2012.[106]), and Perl 5.18.0 (released on May 18, 2013). Perl 5 development versions are released on a monthly basis, with major releases coming out once per year.[107]
The relative proportion of Internet searches for ""Perl programming"", as compared with similar searches for other programming languages, steadily declined from about 10% in 2005 to about 2% in 2011, to about 0.7% in 2020.[108]
At the 2000 Perl Conference, Jon Orwant made a case for a major new language-initiative.[110] This led to a decision to begin work on a redesign of the language, to be called Perl 6. Proposals for new language features were solicited from the Perl community at large, which submitted more than 300 RFCs.[citation needed]
Wall spent the next few years digesting the RFCs and synthesizing them into a coherent framework for Perl 6. He presented his design for Perl 6 in a series of documents called ""apocalypses"" – numbered to correspond to chapters in Programming Perl. As of January 2011[update], the developing specification of Perl 6 was encapsulated in design documents called Synopses – numbered to correspond to Apocalypses.[111]
Thesis work by Bradley M. Kuhn, overseen by Wall, considered the possible use of the Java virtual machine as a runtime for Perl.[112] Kuhn's thesis showed this approach to be problematic. In 2001, it was decided that Perl 6 would run on a cross-language virtual machine called Parrot. This will mean that other languages targeting the Parrot will gain native access to CPAN, allowing some level of cross-language development.[citation needed]
In 2005, Audrey Tang created the Pugs project, an implementation of Perl 6 in Haskell. This acted as, and continues to act as, a test platform for the Perl 6 language (separate from the development of the actual implementation) – allowing the language designers to explore. The Pugs project spawned an active Perl/Haskell cross-language community centered around the freenode #perl6 IRC channel. Many functional programming influences were absorbed by the Perl 6 design team.[citation needed]
In 2012, Perl 6 development was centered primarily around two compilers:[113]
In 2013, MoarVM (“Metamodel On A Runtime”), a C language-based virtual machine designed primarily for Rakudo was announced.[115]
In October 2019, Perl 6 was renamed to Raku.[116]
As of 2017[update] only the Rakudo implementation and MoarVM are under active development, and other virtual machines, such as the Java Virtual Machine and JavaScript are supported.[117]
Perl 7 was announced on 24 June 2020 at ""The Perl Conference in the Cloud"" as the successor to Perl 5.[118][119] Based on Perl 5.32, Perl 7 is designed to be backwards compatible with Perl 5.
Perl's culture and community has developed alongside the language itself. Usenet was the first public venue in which Perl was introduced, but over the course of its evolution, Perl's community was shaped by the growth of broadening Internet-based services including the introduction of the World Wide Web. The community that surrounds Perl was, in fact, the topic of Wall's first ""State of the Onion"" talk.[120]
State of the Onion is the name for Wall's yearly keynote-style summaries on the progress of Perl and its community.  They are characterized by his hallmark humor, employing references to Perl's culture, the wider hacker culture, Wall's linguistic background, sometimes his family life, and occasionally even his Christian background.[121]
Each talk is first given at various Perl conferences and is eventually also published online.
There are a number of IRC channels that offer support for the language and some modules.
There are also many examples of code written purely for entertainment on the CPAN. Lingua::Romana::Perligata, for example, allows writing programs in Latin.[129] Upon execution of such a program, the module translates its source code into regular Perl and runs it.[citation needed]
The Perl community has set aside the ""Acme"" namespace for modules that are fun in nature (but its scope has widened to include exploratory or experimental code or any other module that is not meant to ever be used in production). Some of the Acme modules are deliberately implemented in amusing ways. This includes Acme::Bleach, one of the first modules in the Acme:: namespace,[130] which allows the program's source code to be ""whitened"" (i.e., all characters replaced with whitespace) and yet still work.[citation needed]
In older versions of Perl, one would write the Hello World program as:
Here is a more complex Perl program, that counts down the seconds up to a given threshold:
The perl interpreter can also be used for one-off scripts on the command line. The following example (as invoked from an sh-compatible shell, such as Bash) translates the string ""Bob"" in all files ending with .txt in the current directory to ""Robert"":
Perl has been referred to as ""line noise"" and a write-only language by its critics. The earliest such mention was in the first edition of the book Learning Perl, a Perl 4 tutorial book written by Randal L. Schwartz,[131] in the first chapter of which he states: ""Yes, sometimes Perl looks like line noise to the uninitiated, but to the seasoned Perl programmer, it looks like checksummed line noise with a mission in life.""[132] He also stated that the accusation that Perl is a write-only language could be avoided by coding with ""proper care"".[132] The Perl overview document perlintro states that the names of built-in ""magic"" scalar variables ""look like punctuation or line noise"".[133] However, the English module provides  both long and short English alternatives. perlstyle document states that line noise in regular expressions could be mitigated using the /x modifier to add whitespace.[134]
According to the Perl 6 FAQ, Perl 6 was designed to mitigate ""the usual suspects"" that elicit the ""line noise"" claim from Perl 5 critics, including the removal of ""the majority of the punctuation variables"" and the sanitization of the regex syntax.[135] The Perl 6 FAQ also states that what is sometimes referred to as Perl's line noise is ""the actual syntax of the language"" just as gerunds and prepositions are a part of the English language.[135] In a December 2012 blog posting, despite claiming that ""Rakudo Perl 6 has failed and will continue to fail unless it gets some adult supervision"", chromatic stated that the design of Perl 6 has a ""well-defined grammar"" as well as an ""improved type system, a unified object system with an intelligent metamodel, metaoperators, and a clearer system of context that provides for such niceties as pervasive laziness"".[136] He also stated that ""Perl 6 has a coherence and a consistency that Perl 5 lacks.""[136]
"
PerlMonks - Wikipedia," PerlMonks is a community website covering all aspects of Perl programming and other related topics such as web applications and system administration. It is often referred to by users as 'The Monastery'.[1]
The name PerlMonks, and the general style of the website, is designed to both humorously reflect the almost religious zeal that programmers sometimes have for their favorite language, and also to engender an atmosphere of calm reflection and consideration for other users.
Users (referred to as monks) create discussion topics which other monks can reply to and vote as good or bad. Users have an experience rating (XP) that roughly measures their participation in the PerlMonks website as perceived by the other monks, not necessarily their proficiency in the Perl language[	www.perl.org]. All monks have a 'home node', providing profile information and an area for Monks to personalize.
Notable members include the creator of the Perl language, the authors of several well-known Perl books[2]
and the authors of numerous CPAN modules[1]. CPAN authors frequently promote and provide support for their modules
at PerlMonks.
The site has tutorials, reviews, Q&A, poetry, obfuscated code, as well as sections for code snippets and entire scripts and modules.
Generally, the section of the site with the most traffic is Seekers of Perl Wisdom[2], where users of all experience levels ask Perl-related questions.  Some questions are from beginners trying to understand the basics of the language, while others are from seasoned veterans looking for methods to improve upon algorithms or to optimize performance.  Those who provide answers are also of varying experience levels.
Much of the site's content consists of specific code examples.  Some of these examples are for Perl's core features[3], as documented on the official Perl documentation website (http://perldoc.perl.org).  Other examples are for the Comprehensive Perl Archive Network (CPAN), which is a repository for Perl libraries (known as modules) that are not part of the core Perl distribution.
The code that the site runs on is a much hacked fork of an early version of the Everything Engine and was created by Nathan Oostendorp[3] as part of Blockstackers Intergalactic — the firm that also ran Slashdot. As a result, PerlMonks has many features in common with both Everything2 and Slashdot like its strong emphasis placed on user feedback.
Another feature that PerlMonks retains from Everything is the Chatterbox, which is a text chat area at the side of every page. Logged-in users can type in anything they want, and it appears for all users to see. Talk in the chatterbox is often Perl related, and various tools (written in Perl) have been written to improve the chatterbox experience. Some come to PerlMonks primarily for the chatterbox. Others find the chatterbox distracting and turn it off.
"
Damian Conway - Wikipedia," 
Damian Conway (born 5 October 1964 in Melbourne, Australia) is a computer scientist, a member of the Perl and Raku communities, a public speaker, and the author of several books. Until 2010, he was also an adjunct associate professor in the Faculty of Information Technology at Monash University.
Damian completed his BSc (with honours) and PhD at Monash. He is perhaps best known for his contributions to Comprehensive Perl Archive Network (CPAN) and Raku (Perl 6) language design, and his training courses, both on programming techniques and public speaking skills.
He has won the Larry Wall Award three times for CPAN contributions.[1] His involvement in Perl 6 language design has been as an interlocutor and explicator of Larry Wall.
He is one of the authors of the Significantly Prettier and Easier C++ Syntax (SPEC).[2]
"
AWStats - Wikipedia," AWStats is an open source Web analytics reporting tool, suitable for analyzing data from Internet services such as web, streaming media, mail, and FTP servers. AWStats parses and analyzes server log files, producing HTML reports. Data is visually presented within reports by tables and bar graphs. Static reports can be created through a command line interface, and on-demand reporting is supported through a Web browser CGI program.[1]
AWStats supports most major web server log file formats including Apache (NCSA combined/XLF/ELF log format or Common Log Format (CLF)), WebStar, IIS (W3C log format), and many other common web server log formats.
Development was moved from SourceForge to GitHub in 2014.[2]
Written in Perl, AWStats can be deployed on almost any operating system. It is a server administration tool, with packages available for most Linux distributions. AWStats can be installed on a workstation, such as Microsoft Windows, for local use in situations where log files can be downloaded from a remote server.[1]
AWStats is licensed under the GNU General Public License (GPL).[3]
Proper web log analysis tool configuration and report interpretation requires a bit of technical and business knowledge. AWStats support resources include documentation[4] and user community forums[5]
The on-demand CGI program has been the object of security exploits, as is the case of many CGI programs. Organizations wishing to provide public access to their Web analytics reports should consider generating static HTML reports. The on-demand facility can still be used by restricting its use to internal users. Precautions should be taken against referrer spam. Referrer spam filtering functionality was added in version 6.5.[6]
"
Software categories - Wikipedia," Software categories are groups of software. They allow software to be understood in terms of those categories. Instead of the particularities of each package. Different classification schemes consider different aspects of software.
Computer software can be put into categories based on common function, type, or field of use. There are three broad classifications:
The GNU Project categorizes software by copyright status: free software, open source software, public domain software, copylefted software, noncopylefted free software, lax permissive licensed software, GPL-covered software, the GNU operating system, GNU programs, GNU software, FSF-copyrighted GNU software, nonfree software, proprietary software, freeware, shareware, private software and commercial software.[1]
Free software is software that comes with permission for anyone to use, copy and distribute, either verbatim or with modifications, either gratis or for a fee. In particular, this means that source code must be available. ""If it's not the source, it's not software."" If a program is free, then it can potentially be included in a free operating system such as GNU, or free versions of the Linux system.
Free software in the sense of copyright license (and the GNU project) is a matter of freedom, not price. But proprietary software companies typically use the term ""free software"" to refer to price. Sometimes this means a binary copy can be obtained at no charge; sometimes this means a copy is bundled with a computer for sale at no additional charge.[1]
Open-source software is software with its source code made available under a certain license to its licensees. It can be used and disseminated at any point, the source code is open and can be modified as required. The one condition with this type of software is that when changes are made users should make these changes known to others. One of the key characteristics of open source software is that it is the shared intellectual property of all developers and users. The Linux operating system is one of the best-known examples of a collection of open-source software.[2]
Copylefted software is free software whose distribution terms ensure that all copies of all versions carry more or less the same distribution terms. This means, for instance, that copyleft licenses generally disallows others to add additional requirements to the software (though a limited set of safe added requirements can be allowed) and require making source code available. This shields the program, and its modified versions, from some of the common ways of making a program proprietary. Some copyleft licenses block other means of turning software proprietary.
Copyleft is a general concept. Copylefting an actual program requires a specific set of distribution terms. Different copyleft licenses are usually “incompatible” due to varying terms, which makes it illegal to merge the code using one license with the code using the other license. If two pieces of software use the same license, they are generally mergeable.[1]
Noncopylefted free software comes from the author with permission to redistribute and modify and to add license restrictions.
If a program is free but not copylefted, then some copies or modified versions may not be free. A software company can compile the program, with or without modifications, and distribute the executable file as a proprietary software product. The X Window System illustrates this approach. The X Consortium releases X11 with distribution terms that make it non-copylefted free software. If you wish, you can get a copy that has those distribution terms and is free. However, nonfree versions are available and workstations and PC graphics boards for which nonfree versions are the only ones that work. The developers of X11 made X11 nonfree for a while; they were able to do this because others had contributed their code under the same non-copyleft license.[1]
Shareware is software that comes with permission to redistribute copies but says that anyone who continues to use a copy is required to pay. Shareware is not free software or even semi-free. For most shareware, source code is not available; thus, the program cannot be modified. Shareware does not come with permission to make a copy and install it without paying a license fee, including for nonprofit activity.[1]
Like shareware, freeware is software available for download and distribution without any initial payment. Freeware never has an associated fee. Things like minor program updates and small games are commonly distributed as freeware. Though freeware is cost-free, it is copyrighted, so other people can't market the software as their own.[3]
This classification has seven major elements. They are: platform and management, education and reference, home and entertainment, content and communication, operations and professional, product manufacturing and service delivery, and line of business.
"
Software versioning - Wikipedia," Software upgrade versioning is the process of assigning either unique version names or unique version numbers to unique states of computer software. Within a given version number category (major, minor), these numbers are generally assigned in increasing order and correspond to new developments in the software. At a fine-grained level, revision control is often used for keeping track of incrementally different versions of information, whether or not this information is computer software.
Modern computer software is often tracked using two different software versioning schemes—internal version number that may be incremented many times in a single day, such as a revision control number, and a release version that typically changes far less often, such as semantic versioning[1] or a project code name.
A variety of version numbering schemes have been created to keep track of different versions of a piece of software. The ubiquity of computers has also led to these schemes being used in contexts outside computing.
In sequence-based software versioning schemes, each software release is assigned a unique identifier that consists of one or more sequences of numbers or letters. This is the extent of the commonality; schemes vary widely in areas such as the quantity of sequences, the attribution of meaning to individual sequences, and the means of incrementing the sequences.
In some schemes, sequence-based identifiers are used to convey the significance of changes between releases. Changes are classified by significance level, and the decision of which sequence to change between releases is based on the significance of the changes from the previous release, whereby the first sequence is changed for the most significant changes, and changes to sequences after the first represent changes of decreasing significance.
Depending on the scheme, significance may be assessed by lines of code changed, function points added or removed, potential impact on customers in terms of work required to adopt a newer version, risk of bugs or undeclared breaking changes, degree of changes in visual layout, quantity of new features, or almost anything the product developers or marketers deem to be significant, including marketing desire to stress the ""relative goodness"" of the new version.
Semantic versioning (aka SemVer),[1] is a widely adopted version scheme[2] that uses a sequence of three digits (Major.Minor.Patch), an optional pre-release tag and optional build meta tag. In this scheme, risk and functionality are the measures of significance. Breaking changes are indicated by increasing the major number (high risk), new non-breaking features increment the minor number (medium risk) and all other non-breaking changes increment the patch number (lowest risk). The presence of a pre-release tag (-alpha, -beta) indicates substantial risk, as does a major number of zero (0.y.z), which is used to indicate a work-in-progress that may contain any level of potentially breaking changes (highest risk).
Developers may choose to jump multiple minor versions at a time to indicate significant features have been added, but are not enough to warrant incrementing a major version number; for example Internet Explorer 5 from 5.1 to 5.5, or Adobe Photoshop 5 to 5.5. This may be done to emphasize the value of the upgrade to the software user, or, as in Adobe's case, to represent a release halfway between major versions (although levels of sequence based versioning are not limited to a single digit, as in Blender version 2.79).
A different approach is to use the major and minor numbers, along with an alphanumeric string denoting the release type, e.g. ""alpha"" (a), ""beta"" (b), or ""release candidate"" (rc). A software release train using this approach might look like 0.5, 0.6, 0.7, 0.8, 0.9 → 1.0b1, 1.0b2 (with some fixes), 1.0b3 (with more fixes) → 1.0rc1 (which, if it is stable enough), 1.0rc2 (if more bugs are found) → 1.0. It is a common practice in this scheme to lock-out new features and breaking changes during the release candidate phases and for some teams, even betas are lock-down to bug fixes only, in order to ensure convergence on the target release.
Other schemes impart meaning on individual sequences:
Again, in these examples, the definition of what constitutes a ""major"" as opposed to a ""minor"" change is entirely subjective and up to the author, as is what defines a ""build"", or how a ""revision"" differs from a ""minor"" change.
Shared libraries in Solaris and Linux may use the current.revision.age format where:[3][4]
A similar problem of relative change significance and versioning nomenclature exists in book publishing, where edition numbers or names can be chosen based on varying criteria.
In most proprietary software, the first released version of a software product has version 1.[according to whom?]
Some projects use the major version number to indicate incompatible releases. Two examples are Apache Portable Runtime (APR)[5] and the FarCry CMS.[6]
Semantic versioning[1] is a formal convention for specifying compatibility using a three-part version number: major version; minor version; and patch. The patch number is incremented for minor changes and bug fixes which do not change the software's application programming interface (API). The minor version is incremented for releases which add new, but backward-compatible, API features, and the major version is incremented for API changes which are not backward-compatible. For example, software which relies on version 2.1.5 of an API is compatible with version 2.2.3, but not necessarily with 3.2.4.
Often programmers write new software to be backward compatible, i.e., the new software is designed to interact correctly with older versions of the software (using old protocols and file formats) and the most recent version (using the latest protocols and file formats). For example, IBM z/OS is designed to work properly with 3 consecutive major versions of the operating system running in the same sysplex.
This enables people who run a high availability computer cluster to keep most of the computers up and running while one machine at a time is shut down, upgraded, and restored to service.[7]
Often packet headers and file format include a version number – sometimes the same as the version number of the software that wrote it; other times a ""protocol version number"" independent of the software version number.
The code to handle old deprecated protocols and file formats is often seen as cruft.
Software in the experimental stage (alpha or beta) often use a zero in the first (""major"") position of the sequence to designate its status. However, this scheme is only useful for the early stages, not for upcoming releases with established software where the version number has already progressed past 0.[1]
A number of schemes are used to denote the status of a newer release:
The two purely numeric forms removes the special logic required to handle the comparison of ""alpha < beta < rc < no prefix"" as found in semantic versioning, at the cost of clarity. (semantic versioning actually does not specify specific terms for development stages; the comparison is simply in lexicographical order.)
There are two schools of thought regarding how numeric version numbers are incremented. Most free and open-source software packages, including MediaWiki, treat versions as a series of individual numbers, separated by periods, with a progression such as 1.7.0, 1.8.0, 1.8.1, 1.9.0, 1.10.0, 1.11.0, 1.11.1, 1.11.2, and so on.
On the other hand, some software packages identify releases by decimal numbers: 1.7, 1.8, 1.81, 1.82, 1.9, etc. Decimal versions were common in the 1980s, for example with NetWare, DOS, and Microsoft Windows, but even in the 2000s have been for example used by Opera[8] and Movable Type.[9] In the decimal scheme, 1.81 is the minor version following 1.8, while maintenance releases (i.e. bug fixes only) may be denoted with an alphabetic suffix, such as 1.81a or 1.81b.
The standard GNU version numbering scheme is major.minor.revision,[10] but Emacs is a notable example using another scheme where the major number (1) was dropped and a user site revision was added which is always zero in original Emacs packages but increased by distributors.[11] Similarly, Debian package numbers are prefixed with an optional ""epoch"", which is used to allow the versioning scheme to be changed.[12]
In some cases, developers may decide to reset the major version number. This is sometimes used to denote a new development phase being released. For example, Minecraft Alpha ran from version 1.0.0 to 1.2.6, and when Beta was released, it reset the major version number, and ran from 1.0 to 1.8. Once the game was fully released, the major version number again reset to 1.0.0.[13]
When printed, the sequences may be separated with characters. The choice of characters and their usage varies by scheme. The following list shows hypothetical examples of separation schemes for the same release (the thirteenth third-level revision to the fourth second-level revision to the second first-level revision):[original research?]
When a period is used to separate sequences, it may or may not represent a decimal point, — see “Incrementing sequences” section for various interpretation styles.
There is sometimes a fourth, unpublished number which denotes the software build (as used by Microsoft). Adobe Flash is a notable case where a four-part version number is indicated publicly, as in 10.1.53.64. Some companies also include the build date. Version numbers may also include letters and other characters, such as Lotus 1-2-3 Release 1a.
Some projects use negative version numbers. One example is the SmartEiffel compiler which started from -1.0 and counted upwards to 0.0.[11]
Many projects use a date-based versioning scheme called Calendar Versioning (aka CalVer[14]).
Ubuntu Linux is one example of a project using calendar versioning; Ubuntu 18.04, for example, was released April 2018. This has the advantage of being easily relatable to development schedules and support timelines. Some video games also use date as versioning, for example the arcade game Street Fighter EX. At startup it displays the version number as a date plus a region code, for example 961219 ASIA.[citation needed]
When using dates in versioning, for instance, file names, it is common to use the ISO 8601 scheme:[15] YYYY-MM-DD, as this is easily string sorted to increasing/decreasing order. The hyphens are sometimes omitted. The Wine project formerly used a date versioning scheme, which used the year followed by the month followed by the day of the release; for example, ""Wine 20040505"".[citation needed]
Microsoft Office build numbers are an encoded date:[16] the first two digits indicate the number of months that have passed from the January of the year in which the project started (with each major Office release being a different project), while the last two digits indicate the day of that month. So 3419 is the 19th day of the 34th month after the month of January of the year the project started.[citation needed]
Other examples that identify versions by year include Adobe Illustrator 88 and WordPerfect Office 2003. When a date is used to denote version, it is generally for marketing purposes, and an actual version number also exists. For example, Microsoft Windows 95 is internally versioned as MS-DOS 7.00 and Windows 4.00; likewise, Microsoft Windows 2000 Server is internally versioned as Windows NT 5.0 (""NT"" being a reference to the original product name).[original research?]
The Python Software Foundation has published PEP 440 -- Version Identification and Dependency Specification,[17] outlining their own flexible scheme, that defines an epoch segment, a release segment, pre-release and post-release segments and a development release segment.
TeX has an idiosyncratic version numbering system. Since version 3, updates have been indicated by adding an extra digit at the end, so that the version number asymptotically approaches π; this is a form of unary numbering – the version number is the number of digits. The current version is 3.14159265. This is a reflection of TeX being very stable, and only minor updates are anticipated. TeX developer Donald Knuth has stated that the ""absolutely final change (to be made after [his] death)"" will be to change the version number to π, at which point all remaining bugs will become permanent features.[18]
In a similar way, the version number of METAFONT asymptotically approaches e.
During the era of the classic Mac OS, minor version numbers rarely went beyond "".1"". When they did, they usually jumped straight to "".5"", suggesting the release was ""more significant"".[a] Thus, ""8.5"" was marketed as its own release, representing ""Mac OS 8 and a half"", and 8.6 effectively meant ""8.5.1"".
Mac OS X departed from this trend, in large part because ""X"" (the Roman numeral for 10) was in the name of the product. As a result, all versions of OS X began with the number 10. The first major release of OS X was given the version number 10.0, but the next major release was not 11.0. Instead, it was numbered 10.1, followed by 10.2, 10.3, and so on for each subsequent major release. Thus the 11th major version of OS X was labeled ""10.10"". Even though the ""X"" was dropped from the name as of macOS 10.12, this numbering scheme continued through macOS 10.15. Under the ""X""-based versioning scheme, the third number (instead of the second) denoted a minor release, and additional updates below this level, as well as updates to a given major version of OS X coming after the release of a new major version, were titled Supplemental Updates.[19]
The Roman numeral X was concurrently leveraged for marketing purposes across multiple product lines. Both QuickTime and Final Cut Pro jumped from version 7 directly to version 10, QuickTime X and Final Cut Pro X. Like Mac OS X itself, the products were not upgrades to previous versions, but brand-new programs. As with OS X, major releases for these programs incremented the second digit and minor releases were denoted using a third digit. The ""X"" was dropped from Final Cut's name with the release of macOS 11.0 (see below), and QuickTime's branding became moot when the framework was deprecated in favor of AVFoundation in 2011 (the program for playing QuickTime video was only named QuickTime Player from the start).
Apple's next macOS release, provisionally numbered 10.16,[20] was officially announced as macOS 11.0 at WWDC in June 2020.[21]
The Microsoft Windows operating system was first labelled with standard version numbers for Windows 1.0 through Windows 3.11. After this Microsoft excluded the version number from the product name. For Windows 95 (version 4.0), Windows 98 (4.10) and Windows 2000 (5.0), year of the release was included in the product title. After Windows 2000, Microsoft created the Windows Server family which continued the year-based style with a difference: For minor releases, Microsoft suffixed ""R2"" to the title, e.g., Windows Server 2008 R2 (version 6.1). This style had remained consistent to this date. The client versions of Windows however did not adopt a consistent style. First, they received names with arbitrary alphanumeric suffixes as with Windows ME (4.90), Windows XP (5.1) and Windows Vista (6.0). Then, once again Microsoft adopted incremental numbers in the title, but this time, they were not version numbers; the version numbers of Windows 7, Windows 8 and Windows 8.1 are respectively 6.1, 6.2 and 6.3. In Windows 10, the version number leaped to 10.0[22] and subsequent updates to the OS only incremented build number and update build revision (UBR) number.
Some software producers use different schemes to denote releases of their software. The Debian project uses a major/minor versioning scheme for releases of its operating system, but uses code names from the movie Toy Story during development to refer to stable, unstable and testing releases.[23]
BLAG Linux and GNU features very large version numbers: major releases have numbers such as 50000 and 60000, while minor releases increase the number by 1 (e.g. 50001, 50002). Alpha and beta releases are given decimal version numbers slightly less than the major release number, such as 19999.00071 for alpha 1 of version 20000, and 29999.50000 for beta 2 of version 30000. Starting at 9001 in 2003, the most recent version as of 2011[update] is 140000.[24][25][26]
Software may have an ""internal"" version number which differs from the version number shown in the product name (and which typically follows version numbering rules more consistently). Java SE 5.0, for example, has the internal version number of 1.5.0, and versions of Windows from NT 4 on have continued the standard numerical versions internally: Windows 2000 is NT 5.0, XP is Windows NT 5.1, Windows Server 2003 and Windows XP Professional x64 Edition are NT 5.2, Windows Server 2008 and Vista are NT 6.0, Windows Server 2008 R2 and Windows 7 are NT 6.1, Windows Server 2012 and Windows 8 are NT 6.2, and Windows Server 2012 R2 and Windows 8.1 are NT 6.3, however the first version of Windows 10 was 10.0 (10.0.10240). Note, however, that Windows NT is only on its fifth major revision, as its first release was numbered 3.1 (to match the then-current Windows release number) and the Windows 10 launching made a version leap from 6.3 to 10.0.
In conjunction with the various versioning schemes listed above, a system for denoting pre-release versions is generally used, as the program makes its way through the stages of the software release life cycle.
Programs that are in an early stage are often called ""alpha"" software, after the first letter in the Greek alphabet. After they mature but are not yet ready for release, they may be called ""beta"" software, after the second letter in the Greek alphabet. Generally alpha software is tested by developers only, while beta software is distributed for community testing.
Some systems use numerical versions less than 1 (such as 0.9), to suggest their approach toward a final ""1.0"" release. This is a common convention in open source software.[27][28] However, if the pre-release version is for an existing software package (e.g. version 2.5), then an ""a"" or ""alpha"" may be appended to the version number. So the alpha version of the 2.5 release might be identified as 2.5a or 2.5.a.
An alternative is to refer to pre-release versions as ""release candidates"", so that software packages which are soon to be released as a particular version may carry that version tag followed by ""rc-#"", indicating the number of the release candidate; when the final version is released, the ""rc"" tag is removed.
A software release train is a form of software release schedule in which a number of distinct series of versioned software releases for multiple products are released as a number of different ""trains"" on a regular schedule. Generally, for each product line, a number of different release trains are running at a given time, with each train moving from initial release to eventual maturity and retirement on a planned schedule. Users may experiment with a newer release train before adopting it for production, allowing them to experiment with newer, ""raw"", releases early, while continuing to follow the previous train's point releases for their production systems prior to moving to the new release train as it becomes mature.
Cisco's IOS software platform used a release train schedule with many distinct trains for many years. More recently, a number of other platforms including Firefox and Fenix for Android,[29] Eclipse,[30] LibreOffice,[31] Ubuntu,[32] Fedora,[33] Python,[34] digiKam[35] and VMware[36] have adopted the release train model.
Between the 1.0 and the 2.6.x series, the Linux kernel used odd minor version numbers to denote development releases and even minor version numbers to denote stable releases; see Linux kernel § Version numbering. For example, Linux 2.3 was a development family of the second major design of the Linux kernel, and Linux 2.4 was the stable release family that Linux 2.3 matured into. After the minor version number in the Linux kernel is the release number, in ascending order; for example, Linux 2.4.0 → Linux 2.4.22. Since the 2004 release of the 2.6 kernel, Linux no longer uses this system, and has a much shorter release cycle.
The same odd-even system is used by some other software with long release cycles, such as Node.js up to version 0.12 as well as GNOME and WineHQ.[37]
The free-software and open source communities tend to release software early and often. Initial versions are numbers less than 1, with these 0.x version used to convey that the software is incomplete and not reliable enough for general release or usable in its current state. 
Version 1.0 is used as a major milestone, indicating that the software has at least all major features plus functions the developers wanted to get into that version, and is considered reliable enough for general release.[27][28] A good example of this is the Linux kernel, which was first released as version 0.01 in 1991,[38] and took until 1994 to reach version 1.0.0.[39]
The developers of the arcade game emulator MAME do not ever intend to release a version 1.0 of the program because there will always be more arcade games to emulate and thus the project can never be truly completed. Accordingly, version 0.99 was followed by version 0.100.[40]
Since the internet has become widespread, most commercial software vendors no longer follow the maxim that a major version should be ""complete"" and instead rely on patches with bugfixes to sort out the known issues which a solution has been found for and could be fixed.[citation needed]
A relatively common practice is to make major jumps in version numbers for marketing reasons. Sometimes software vendors sometimes just bypass the 1.0 release or quickly release a release with a subsequent version number because 1.0 software is considered by many customers too immature to trust with production deployments.[citation needed] For example, as in the case of dBase II, a product is launched with a version number that implies that it is more mature than it is.
Other times version numbers are increased to match those of competitors. This can be seen in many examples of product version numbering by Microsoft, America Online, Sun Solaris, Java Virtual Machine, SCO Unix, WordPerfect. Microsoft Access jumped from version 2.0 to version 7.0, to match the version number of Microsoft Word.
Microsoft has also been the target of 'catch-up' versioning, with the Netscape browsers skipping version 5 to 6, in line with Microsoft's Internet Explorer, but also because the Mozilla application suite inherited version 5 in its user agent string during pre-1.0 development and Netscape 6.x was built upon Mozilla's code base.
Another example of keeping up with competitors is when Slackware Linux jumped from version 4 to version 7 in 1999.[41]
Sun's Java has at times had a hybrid system, where the internal version number has always been 1.x but has been marketed by reference only to the x:
Sun also dropped the first digit for Solaris, where Solaris 2.8 (or 2.9) is referred to as Solaris 8 (or 9) in marketing materials.
A similar jump took place with the Asterisk open-source PBX construction kit in the early 2010s, whose project leads announced that the current version 1.8.x would soon be followed by version 10.[42]
This approach, panned by many[according to whom?] because it breaks the semantic significance of the sections of the version number, has been adopted by an increasing number of vendors including Mozilla (for Firefox).[citation needed]
In the mid-1990s, the rapidly growing CMMS, Maximo, moved from Maximo Series 3 directly to Series 5, skipping Series 4 due to that number's perceived marketing difficulties in the Chinese market, where the number 4 is associated with ""death"" (see tetraphobia). This did not, however, stop Maximo Series 5 version 4.0 being released. (The ""Series"" versioning has since been dropped, effectively resetting version numbers after Series 5 version 1.0's release.)
Version numbers are used in practical terms by the consumer, or client, to identify or compare their copy of the software product against another copy, such as the newest version released by the developer. For the programmer or company, versioning is often used on a revision-by-revision basis, where individual parts of the software are compared and contrasted with newer or older revisions of those same parts, often in a collaborative version control system.
In the 21st century, more programmers started to use a formalized version policy, such as the semantic versioning policy.[1] The purpose of such policies is to make it easier for other programmers to know when code changes are likely to break things they have written. Such policies are especially important for software libraries and frameworks, but may also be very useful to follow for command-line applications (which may be called from other applications) and indeed any other applications (which may be scripted and/or extended by third parties).
Versioning is also a required practice to enable many schemes of patching and upgrading software, especially to automatically decide what and where to upgrade to.
Version numbers allow people providing support to ascertain exactly which code a user is running, so that they can rule out bugs that have already been fixed as a cause of an issue, and the like. This is especially important when a program has a substantial user community, especially when that community is large enough that the people providing technical support are not the people who wrote the code. The semantic meaning[1] of version.revision.change style numbering is also important to information technology staff, who often use it to determine how much attention and research they need to pay to a new release before deploying it in their facility. As a rule of thumb, the bigger the changes, the larger the chances that something might break (although examining the Changelog, if any, may reveal only superficial or irrelevant changes). This is one reason for some of the distaste expressed in the ""drop the major release"" approach taken by Asterisk et alia: now, staff must (or at least should) do a full regression test for every update.
Some computer file systems, such as the OpenVMS Filesystem, also keep versions for files.
Versioning amongst documents is relatively similar to the routine used with computers and software engineering, where with each small change in the structure, contents, or conditions, the version number is incremented by 1, or a smaller or larger value, again depending on the personal preference of the author and the size or importance of changes made.
Version numbers very quickly evolve from simple integers (1, 2, ...) to rational numbers (2.08, 2.09, 2.10)
and then to non-numeric ""numbers"" such as 4:3.4.3-2. These complex version numbers are therefore better treated as character strings. Operating systems that include package management facilities (such as all non-trivial Linux or BSD distributions) will use a distribution-specific algorithm for comparing version numbers of different software packages. For example, the ordering algorithms of Red Hat and derived distributions differ to those of the Debian-like distributions.
As an example of surprising version number ordering implementation behavior, in Debian, leading zeroes are ignored in chunks, so that 5.0005 and 5.5 are considered as equal, and 5.5 < 5.0006. This can confuse users; string-matching tools may fail to find a given version number; and this can cause subtle bugs in package management if the programmers use string-indexed data structures such as version-number indexed hash tables.
In order to ease sorting, some software packages represent each component of the major.minor.release scheme with a fixed width. Perl represents its version numbers as a floating-point number; for example, Perl's 5.8.7 release can also be represented as 5.008007. This allows a theoretical version of 5.8.10 to be represented as 5.008010. Other software packages pack each segment into a fixed bit width; for example, on Microsoft Windows, version number 6.3.9600.16384 would be represented as hexadecimal 0x0006000325804000. The floating-point scheme breaks down if any segment of the version number exceeds 999; a packed-binary scheme employing 16 bits apiece breaks down after 65535.
Software-style version numbers can be found in other media.
In some cases, the use is a direct analogy (for example: Jackass 2.5, a version of Jackass Number Two with additional special features; the second album by Garbage, titled Version 2.0; or Dungeons & Dragons 3.5, where the rules were revised from the third edition, but not so much as to be considered the fourth).
More often it's used to play on an association with high technology, and doesn't literally indicate a 'version' (e.g., Tron 2.0, a video game followup to the film Tron, or the television series The IT Crowd, which refers to the second season as Version 2.0). A particularly notable usage is Web 2.0, referring to websites from the early 2000s that emphasized user-generated content, usability and interoperability.
Phish 1.0, 2.0, 3.0 and possibly 4.0 after the Covid 19 forced hiatus.
"
Wine (software) - Wikipedia," 
Wine (recursive backronym for Wine Is Not an Emulator) is a free and open-source compatibility layer that aims to allow application software and computer games developed for Microsoft Windows to run on Unix-like operating systems. Wine also provides a software library, known as ""Winelib"", against which developers can compile Windows applications to help port them to Unix-like systems.[8]
Wine provides its compatibility layer for Windows runtime system (also called runtime environment) which translates Windows system calls into POSIX-compliant system calls,[9] recreating the directory structure of Windows, and providing alternative implementations of Windows system libraries,[10] system services through wineserver[11] and various other components (such as Internet Explorer, the Windows Registry Editor,[12] and msiexec[13]). Wine is predominantly written using black-box testing reverse-engineering, to avoid copyright issues.[14]
The selection of ""Wine is Not an Emulator"" as the name of the Wine Project was the result of a naming discussion in August 1993[15] and credited to David Niemi. There is some confusion caused by an early FAQ using Windows Emulator and other invalid sources that appear after the Wine Project name being set. No code emulation or virtualization occurs when running a Windows application under Wine.[16] ""Emulation"" usually would refer to execution of compiled code intended for one processor (such as x86) by interpreting/recompiling software running on a different processor (such as PowerPC). 
While the name sometimes appears in the forms WINE and wine, the project developers have agreed to standardize on the form Wine.[17]
Wine is primarily developed for Linux and macOS,[18] and there are, as of July 2020[update], well-maintained packages available for both platforms.[19]
In a 2007 survey by desktoplinux.com of 38,500 Linux desktop users, 31.5% of respondents reported using Wine to run Windows applications.[20] This plurality was larger than all x86 virtualization programs combined, as well as larger than the 27.9% who reported not running Windows applications.[21]
Bob Amstadt, the initial project leader, and Eric Youngdale started the Wine project in 1993 as a way to run Windows applications on Linux. It was inspired by two Sun Microsystems' products, the Wabi for the Solaris operating system, and the Public Windows Initiative,[22] which was an attempt to get the Windows API fully reimplemented in the public domain as an ISO standard but rejected due to pressure from Microsoft in 1996.[23] Wine originally targeted 16-bit applications for Windows 3.x, but  as of 2010[update] focuses on 32-bit and 64-bit versions which have become the standard on newer operating systems. The project originated in discussions on Usenet in comp.os.linux in June 1993.[24] Alexandre Julliard has led the project since 1994.
The project has proven time-consuming and difficult for the developers, mostly because of incomplete and incorrect documentation of the Windows API. While Microsoft extensively documents most Win32 functions, some areas such as file formats and protocols have no publicly available specification from Microsoft, and Windows also includes undocumented low-level functions, undocumented behavior and obscure bugs that Wine must duplicate precisely in order to allow some applications to work properly.[25] Consequently, the Wine team has reverse-engineered many function calls and file formats in such areas as thunking.[citation needed]
The Wine project originally released Wine under the same MIT License as the X Window System, but owing to concern about proprietary versions of Wine not contributing their changes back to the core project,[26] work as of March 2002 has used the LGPL for its licensing.[27]
Wine officially entered beta with version 0.9 on 25 October 2005.[28] Version 1.0 was released on 17 June 2008,[29] after 15 years of development. Version 1.2 was released on 16 July 2010,[30] version 1.4 on 7 March 2012,[31] version 1.6 on 18 July 2013.[32] and version 1.8 on 19 December 2015.[33] Development versions are released roughly every two weeks.
Wine-staging is an independently maintained set of aggressive patches not deemed ready by WineHQ developers for merging into the Wine repository, but still considered useful by the wine-compholio fork. It mainly covers experimental functions and bug fixes. Since January 2017, patches in wine-staging begins to be actively merged into the WineHQ upstream as wine-compholio transferred the project to Alistair Leslie-Hughes, a key WineHQ developer.[34]

The main corporate sponsor of Wine is CodeWeavers, which employs Julliard and many other Wine developers to work on Wine and on CrossOver, CodeWeavers' supported version of Wine. CrossOver includes some application-specific tweaks not considered suitable for the upstream version, as well as some additional proprietary components.[35]
The involvement of Corel for a time assisted the project, chiefly by employing Julliard and others to work on it. Corel had an interest in porting WordPerfect Office, its office suite, to Linux (especially Corel Linux). Corel later cancelled all Linux-related projects after Microsoft made major investments in Corel, stopping their Wine effort.[36]
Other corporate sponsors include Google, which hired CodeWeavers to fix Wine so Picasa ran well enough to be ported directly to Linux using the same binary as on Windows; Google later paid for improvements to Wine's support for Adobe Photoshop CS2. Wine is also a regular beneficiary of Google's Summer of Code program.[37][38]
The goal of Wine is to implement the Windows APIs fully or partially that are required by programs that the users of Wine wish to run on top of a Unix-like system.
The programming interface of Microsoft Windows consists largely of dynamic-link libraries (DLLs). These contain a huge number of wrapper sub-routines for the system calls of the kernel, the NTOS kernel-mode program (ntoskrnl.exe). A typical Windows program calls some Windows DLLs, which in turn calls user-mode gdi/user32 libraries, which in turn uses the kernel32.dll (win32 subsystem) responsible for dealing with the kernel through system calls. The system-call layer is considered private to Microsoft programmers as documentation is not publicly available, and published interfaces all rely on subsystems running on top of the kernel. Besides these, there are a number of programming interfaces implemented as services that run as separate processes. Applications communicate with user-mode services through RPCs.[39]
Wine implements the Windows application binary interface (ABI) entirely in user space, rather than as a kernel module. Wine mostly mirrors the hierarchy, with services normally provided by the kernel in Windows[40] instead provided by a daemon known as the wineserver, whose task is to implement basic Windows functionality, as well as integration with the X Window System, and translation of signals into native Windows exceptions. Although Wineserver implements some aspects of the Windows kernel, it is not possible to use native Windows drivers with it, due to Wine's underlying architecture.[39] This prevents certain applications and games from working, for example those using StarForce copy-protection which requires virtual device drivers to be installed.[citation needed]
Wine allows for loading both Windows DLLs and Unix shared objects for its Windows programs. Its built-in implementation of the most basic Windows DLLs, namely NTDLL, KERNEL32, GDI32, and USER32, uses the shared object method because they must use functions in the host operating system as well. Higher-level libraries, such as WineD3D, are free to use the DLL format. In many cases users can choose to load a DLL from Windows instead of the one implemented by Wine. Doing so can provide functionalities not yet implemented by Wine, but may also cause malfunctions if it relies on something else not present in Wine.[39]
Wine tracks its state of implementation through automated unit testing done at every git commit.[41]
While most office software does not make use of complex GPU-accelerated graphics APIs, computer games do. To run these games properly, Wine would have to forward the drawing instructions to the host OS, and even translate them to something the host can understand.
DirectX is a collection of Microsoft APIs for rendering, audio and input. As of 2019, Wine 4.0 contains a DirectX 12 implementation for Vulkan API, and DirectX 11.2 for OpenGL.[42] Wine 4.0 also allows Wine to run Vulkan applications by handing draw commands to the host OS, or in the case of macOS, by translating them into the Metal API by MoltenVK.[42]
Much of Wine's DirectX effort goes into building WineD3D, a translation layer from Direct3D and DirectDraw API calls into OpenGL. As of 2019, this component supports up to DirectX 11.[42] As of 12 December 2016, Wine is good enough to run Overwatch with D3D11.[45] Besides being used in Wine, WineD3D DLLs have also been used on Windows itself, allowing for older GPUs to run games using newer DirectX versions and for old DDraw-based games to render correctly.[46]
Some work is ongoing to move the Direct3D backend to Vulkan API. Direct3D 12 support in 4.0 is provided by a ""vkd3d"" subproject,[42] and WineD3D has in 2019 been experimentally ported to use the Vulkan API.[47]
Wine, when patched, can alternatively run Direct3D 9 API commands directly via a free and open-source Gallium3D State Tracker (aka Gallium3D GPU driver) without translation into OpenGL API calls. In this case, the Gallium3D layer allows a direct pass-through of DX9 drawing commands which results in performance improvements of up to a factor of 2.[48] As of 2020, the project is known as Gallium.Nine. It is now available as a separate standalone package and no longer requires a patched Wine version.[49]
Wine is usually invoked from the command-line interpreter: wine program.exe.[50]
There is the utility winecfg that starts a graphical user interface with controls for adjusting basic options.[51] It is a GUI configuration utility included with Wine. Winecfg makes configuring Wine easier by making it unnecessary to edit the registry directly, although, if needed, this can be done with the included registry editor (similar to Windows regedit).
Some applications require more tweaking than simply installing the application in order to work properly, such as manually configuring Wine to use certain Windows DLLs. The Wine project does not integrate such workarounds into the Wine codebase, instead preferring to focus solely on improving Wine's implementation of the Windows API. While this approach focuses Wine development on long-term compatibility, it makes it difficult for users to run applications that require workarounds. Consequently, many third-party applications have been created to ease the use of those applications that do not work out of the box within Wine itself. The Wine wiki maintains a page of current and obsolete third-party applications.[52]
The developers of the Direct3D portions of Wine have continued to implement new features such as pixel shaders to increase game support.[62] Wine can also use native DLLs directly, thus increasing functionality, but then a license for Windows is needed unless the DLLs were distributed with the application itself.
Wine also includes its own open-source implementations of several Windows programs, such as notepad, wordpad, control, iexplore, and explorer.[63]
The Wine Application Database (AppDB) is a community-maintained on-line database about which Windows programs works with Wine and how well they work.
Wine ensures good backward compatibility with legacy Windows applications, including those written for Windows 3.1x.[64] Wine can mimic different Windows versions required for some programs, going as far back as Windows version 2.0.[65] However, Windows 1.x and Windows 2.x support was removed from Wine development version 1.3.12. If DOSBox is installed on the system[citation needed] (see below on MS-DOS), Wine development version 1.3.12 and later nevertheless show the ""Windows 2.0"" option for the Windows version to mimic, but Wine still will not run most Windows 2.0 programs because MS-DOS and Windows functions are not currently integrated.
Backward compatibility in Wine is generally superior to that of Windows, as newer versions of Windows can force users to upgrade legacy Windows applications, and may break abandoned software forever as there is nobody adjusting the program for the changes in the operating system. In many cases, Wine can offer better legacy support than newer versions of Windows with ""Compatibility Mode"". Wine can run 16-bit Windows programs (Win16) on a 64-bit operating system, which uses an x86-64 (64-bit) CPU,[66] a functionality not found in 64-bit versions of Microsoft Windows.[67][68] WineVDM allows 16-bit Windows applications to run on 64-bit versions of Windows.[69]
Wine partially supports Windows console applications, and the user can choose which backend to use to manage the console (choices include raw streams, curses, and user32).[70] When using the raw streams or curses backends, Windows applications will run in a Unix terminal.
Preliminary support for 64-bit Windows applications was added to Wine 1.1.10, in December 2008.[71] As of April 2019[update], the support is considered stable. The two versions of wine are built separately, and as a result only building wine64 produces an environment only capable of running x86-64 applications.[72]
As of April 2019[update], Wine has stable support for a WoW64 build, which allows both 32-bit and 64-bit Windows applications to run inside the same Wine instance. To perform such a build, one must first build the 64-bit version, and then build the 32-bit version referencing the 64-bit version. Just like Microsoft's WoW64, the 32-bit build process will add parts necessary for handling 32-bit programs to the 64-bit build.[72] This functionality is seen from at least 2010.[73]
Early versions of Microsoft Windows run on top of MS-DOS, and Windows programs may depend on MS-DOS programs to be usable. Wine does not have good support for MS-DOS, but starting with development version 1.3.12, Wine tries running MS-DOS programs in DOSBox if DOSBox is available on the system.[74] However, due to a bug, current versions of Wine incorrectly identify Windows 1.x and Windows 2.x programs as MS-DOS programs, attempting to run them in DOSBox (which does not work).[75]
Wine provides Winelib, which allows its shared-object implementations of the Windows API to be used as actual libraries for a Unix program. This allows for Windows code to be built into native Unix executables. Since October 2010, Winelib also works on the ARM platform.[76]
Support for Solaris SPARC was dropped in version 1.5.26.
Wine provides some support for ARM (as well as ARM64/AArch64) processors and the Windows flavors that run on it. As of April 2019[update], Wine can run ARM/Win32 applications intended for unlocked Windows RT devices (but not Windows RT programs). Windows CE support (either x86 or ARM) is missing,[77] but an unofficial, pre-alpha proof-of-concept version called WineCE allows for some support.[78]
On 3 February 2013 at the FOSDEM talk in Brussels, Alexandre Julliard demonstrated an early demo of Wine running on Google's Android operating system.[79]
Experimental builds of WINE for Android (x86 and ARM) were released in late 2017. It has been routinely updated by the official developers ever since.[5] The default builds do not implement cross-architecture emulation via QEMU, and as a result ARM versions will only run ARM applications that use the Win32 API.[80]
Wine, by default, uses specialized Windows builds of Gecko and Mono to substitute for Microsoft's Internet Explorer and .NET Framework. Wine has built-in implementations of JScript and VBScript. It is possible to download and run Microsoft's installers for those programs through winetricks or manually.
Wine is not known to have good support for most versions of Internet Explorer (IE). Of all the reasonably recent versions, Internet Explorer 8 for Windows XP is the only version that reports a usable rating on Wine's AppDB, out-of-the-box.[81] However Google Chrome gets a gold rating (as of Wine 5.5-staging),[82] and Microsoft's IE replacement web browser Edge, is known to be based on that browser (after switching from Microsoft's own rendering engine[83]). Winetricks offer auto-installation for Internet Explorer 6 through 8, so these versions can be reasonably expected to work with its built-in workarounds.
An alternative for installing Internet Explorer directly is to use the now-defunct IEs4Linux. It is not compatible with the latest versions of Wine,[84] and the development of IEs4Linux is inactive.
The core Wine development aims at a correct implementation of the Windows API as a whole and has sometimes lagged in some areas of compatibility with certain applications. Direct3D, for example, remained unimplemented until 1998,[85] although newer releases have had an increasingly complete implementation.[86]
CodeWeavers markets CrossOver specifically for running Microsoft Office and other major Windows applications, including some games. CodeWeavers employs Alexandre Julliard to work on Wine and contributes most of its code to the Wine project under the LGPL. CodeWeavers also released a new version called CrossOver Mac for Intel-based Apple Macintosh computers on 10 January 2007.[87]
As of 2012, CrossOver includes the functionality of both the CrossOver Games and CrossOver Pro lines therefore CrossOver Games and CrossOver Pro are no longer available as single products.[88]
CrossOver Games was optimized for running Windows video games. Unlike CrossOver, it didn't focus on providing the most stable version of Wine. Instead, experimental features are provided to support newer games.[89]
TransGaming Inc. (now Findev Inc. since the sale of its software businesses) produced the proprietary Cedega software. Formerly known as WineX, Cedega represented a fork from the last MIT-licensed version of Wine in 2002. Much like CrossOver Games, TransGaming's Cedega was targeted towards running Windows video games. On 7 January 2011, TransGaming Inc. announced continued development of Cedega Technology under the GameTree Developer Program. TransGaming Inc. allowed members to keep using their Cedega ID and password until 28 February 2011.[90]
TransGaming also produced Cider, a library for Apple–Intel architecture Macintoshes. Instead of being an end-user product, Cider (like Winelib) is a wrapper allowing developers to adapt their games to run natively on Intel Mac without any changes in source code.
The Russian company Etersoft has been developing a proprietary version of Wine since 2006. WINE@Etersoft supports popular Russian applications (for example, 1C:Enterprise by 1C Company).[91]
Darwine is an outdated port of the Wine libraries to Darwin and to macOS for both the PowerPC and Intel x86 architectures. All patches for the x86 version were merged back into the main branch of Wine in 2009. Development on the PPC version was abandoned (and in 2020 Wine 5.11 dropped support for PowerPC.). Mike Kronenberg previously created the WineHelper for Darwine to add a GUI and macOS style app for interacting with Wine, which was later replaced by WineBottler.[92] Darwine now provides macOS compatible packages compiled from the Wine repository.[93]
The Pipelight Team has produced a custom version of Wine (wine-compholio) that acts as a wrapper for Windows NPAPI plugins within Linux browsers.[94] This tool permits Linux users to run Microsoft Silverlight, the Microsoft equivalent of Adobe Flash, and the Unity web plugin, along with a variety of other NPAPI plugins. The project provides an extensive set of patches against the upstream Wine project,[95] some of which were approved and added to upstream Wine. Pipelight is largely obsolete, as modern browsers no longer support NPAPI plugins and Silverlight has been deprecated by Microsoft.[96]
On 21 August 2018, Valve announced a new variation of Wine, named Proton, designed to integrate with the Linux version of the company's Steam software (including Steam installations built into their Linux-based SteamOS operating system and Steam Machine computers).[97] Valve's goal for Proton is to enable Steam users on Linux to play games which lack a native Linux port (particularly back-catalog games), and ultimately, through integration with Steam as well as improvements to game support relative to mainline Wine, to give users ""the same simple plug-and-play experience"" that they would get if they were playing the game natively on Linux.[97] Proton entered public beta immediately upon being announced.[97]
Valve had already been collaborating with CodeWeavers since 2016 to develop improvements to Wine's gaming performance, some of which have already been merged to the upstream Wine project.[97] Some of the specific improvements incorporated into Proton include Vulkan-based Direct3D 9, 10, 11, and 12 implementations via vkd3d,[98] DXVK,[99] and D9VK[100] multi-threaded performance improvements via esync,[101] improved handling of fullscreen games, and better automatic game controller hardware support.[97]
Proton is fully open-source and available via GitHub.[102]
Other projects using Wine source code include:
The Wine project has received a number of technical and philosophical complaints and concerns over the years.
Because of Wine's ability to run Windows binary code, concerns have been raised over native Windows viruses and malware affecting Unix-like operating systems[109] as Wine can run most malware. A 2018 security analysis found that 5 out of 30 malware samples were able to successfully run through Wine, a relatively low rate that nevertheless posed a security risk.[110] For this reason the developers of Wine recommend never running it as the superuser.[111] Malware research software such as ZeroWine[112] runs Wine on Linux in a virtual machine, to keep the malware completely isolated from the host system. An alternative to improve the security without the performance cost of using a virtual machine, is to run Wine in an LXC container, as Anbox software is doing by default with Android.
Another security concern is when the implemented specifications are ill-designed and allow for security compromise. Because Wine implements these specifications, it will likely also implement any security vulnerabilities they contain. One instance of this problem was the 2006 Windows Metafile vulnerability, which saw Wine implementing the vulnerable SETABORTPROC escape.[113][114]
A common concern about Wine is that its existence means that vendors are less likely to write native Linux, macOS, and BSD applications. As an example of this, it is worth considering IBM's 1994 operating system, OS/2 Warp.[original research?] An article describes the weaknesses of OS/2 which killed it, the first one being:
OS/2 offered excellent compatibility with DOS and Windows 3.1 applications. No, this is not an error. Many application vendors argued that by developing a DOS or Windows app, they would reach the OS/2 market in addition to DOS/Windows markets and they didn't develop native OS/2 applications.[115]However, OS/2 had many problems with end user acceptance. Perhaps the most serious was that most computers sold already came with DOS and Windows, and many people didn't bother to evaluate OS/2 on its merits due to already having an operating system. ""Bundling"" of DOS and Windows and the chilling effect this had on the operating system market frequently came up in United States v. Microsoft Corporation.
The Wine project itself responds to the specific complaint of ""encouraging"" the continued development for the Windows API on one of its wiki pages:
For most people there remain a handful of programs locking them in to Windows. It's obvious there will never be a Microsoft Office ported to Linux, however older versions of programs like TurboTax won't be ported either. Similarly, there are tens of thousands of games and internal corporate applications which will never be ported. If you want to use Linux and rely on any legacy Windows application, something like Wine is essential... Wine makes Linux more useful and allows for millions of users to switch who couldn't otherwise. This greatly raises Linux marketshare, drawing more commercial and community developers to Linux.[116]Also, the Wine Wiki page claims that Wine can help break the chicken-and-egg problem for Linux on the desktop:[117]
This brings us to the chicken and egg issue of Linux on the desktop. Until Linux can provide equivalents for the above applications, its market share on the desktop will stagnate. But until the market share of Linux on the desktop rises, no vendor will develop applications for Linux. How does one break this vicious circle?
Again, Wine can provide an answer. By letting users reuse the Windows applications they have invested time and money in, Wine dramatically lowers the barrier that prevents users from switching to Linux. This then makes it possible for Linux to take off on the desktop, which increases its market share in that segment. In turn, this makes it viable for companies to produce Linux versions of their applications, and for new products to come out just for the Linux market.
This reasoning could be dismissed easily if Wine was only capable of running Solitaire. However, now it can run Microsoft Office, multimedia applications such as QuickTime and Windows Media Player, and even games such as Max Payne or Unreal Tournament 3. Almost any other complex application can be made to run well given a bit of time. And each time that work is done to add one application to this list, many other applications benefit from this work and become usable too.

Have a look at our Application Database to get an idea on what can be run under Wine.The use of Wine for gaming has proved specifically controversial in the Linux community, as some feel it is preventing, or at least hindering, the further growth of native Linux gaming on the platform.[118][119]
Microsoft has not made public statements about Wine. However, the Windows Update software will block updates to Microsoft applications running in Wine. On 16 February 2005, Ivan Leo Puoti discovered that Microsoft had started checking the Windows Registry for the Wine configuration key and would block the Windows Update for any component.[120] As Puoti noted: ""It's also the first time Microsoft acknowledges the existence of Wine.""
In January 2020, Microsoft cited Wine as a positive consequence of being able to reimplement APIs, in its amicus curiae brief for Google LLC v. Oracle America, Inc.[121]
"
Darwin (operating system) - Wikipedia," 
Darwin is an open-source Unix-like operating system first released by Apple Inc. in 2000. It is composed of code developed by Apple, as well as code derived from NeXTSTEP, BSD, Mach, and other free software projects. 
Darwin forms the core set of components upon which macOS (previously OS X and Mac OS X), iOS, watchOS, tvOS, and iPadOS are based. It is mostly POSIX-compatible, but has never, by itself, been certified as compatible with any version of POSIX. Starting with Leopard, macOS has been certified as compatible with the Single UNIX Specification version 3 (SUSv3).[4][5][6]
The heritage of Darwin began with NeXT's NeXTSTEP operating system (later, since version 4.0, known as OPENSTEP), first released in 1989. After Apple bought NeXT in 1997, it announced it would base its next operating system on OPENSTEP. This was developed into Rhapsody in 1997, Mac OS X Server 1.0 in 1999, Mac OS X Public Beta in 2000, and Mac OS X 10.0 in 2001. 
In 1999, Apple announced it would release the Mach 2.5 microkernel, BSD Unix 4.4 OS, and the Apache Web server components of Mac OS X Server.[7] At the time interim CEO Steve Jobs alluded to British naturalist Charles Darwin by announcing ""because it's about evolution"".[8] In 2000, the core operating system components of Mac OS X were released as open-source software under the Apple Public Source License (APSL) as Darwin; the higher-level components, such as the Cocoa and Carbon frameworks, remained closed-source. 
Up to Darwin 8.0.1, Apple released a binary installer (as an ISO image) after each major Mac OS X release that allowed one to install Darwin on PowerPC and Intel x86 systems as a standalone operating system.[9] Minor updates were released as packages that were installed separately. Darwin is now only available as source code.
The kernel of Darwin is XNU, a hybrid kernel which uses OSFMK 7.3[10] (Open Software Foundation Mach Kernel) from the OSF, various elements of FreeBSD (including the process model, network stack, and virtual file system),[11] and an object-oriented device driver API called I/O Kit.[12] The hybrid kernel design provides the flexibility of a microkernel[13][failed verification – see discussion] and the performance of a monolithic kernel.[14]
Darwin currently includes support for the 64-bit x86-64 variant of the Intel x86 processors used in Intel-based Macs and the 64-bit ARM processors used in the iPhone 5S and later, the 6th generation iPod Touch, the 7th generation iPad and later, the iPad Air family, the iPad Mini 2 and later, the iPad Pro family, the fourth generation and later Apple TVs, the HomePod family, and Macs with Apple Silicon such as the 2020 Apple M1 Macs. An open-source port of the XNU kernel exists that supports Darwin on Intel and AMD x86 platforms not officially supported by Apple, though it does not appear to have been updated since 2009.[15] An open-source port of the XNU kernel also exists for ARM platforms.[16] Older versions supported some or all of 32-bit PowerPC, 64-bit PowerPC, 32-bit x86, and 32-bit ARM.
It supports the POSIX API by way of its BSD lineage (largely FreeBSD userland) and a large number of programs written for various other UNIX-like systems can be compiled on Darwin with no changes to the source code.
Darwin does not include many of the defining elements of macOS, such as the Carbon and Cocoa APIs or the Quartz Compositor and Aqua user interface, and thus cannot run Mac applications. It does, however, support a number of lesser-known features of macOS, such as mDNSResponder, which is the multicast DNS responder and a core component of the Bonjour networking technology, and launchd, an advanced service management framework.
In July 2003, Apple released Darwin under version 2.0 of the Apple Public Source License (APSL), which the Free Software Foundation (FSF) classifies as a free software license incompatible with the GNU General Public License.[17] Previous versions were released under an earlier version of the APSL license, which did not meet the FSF definition of free software, although it did meet the requirements of the Open Source Definition.[18]
The following is a table of major Darwin releases with their dates of release and their corresponding macOS releases.[19] Note that the corresponding macOS release may have been released on a different date; refer to the macOS pages for those dates.
The jump in version numbers from Darwin 1.4.1 to 5.1 with the release of Mac OS X v10.1.1 was designed to tie Darwin to the Mac OS X version and build numbering system, which in turn is inherited from NeXTSTEP. In the build numbering system of macOS, every version has a unique beginning build number, which identifies what whole version of macOS it is part of. Mac OS X v10.0 had build numbers starting with 4, 10.1 had build numbers starting with 5, and so forth (earlier build numbers represented developer releases).[31]
The command .mw-parser-output .monospaced{font-family:monospace,monospace}uname -r in Terminal will show the Darwin version number, and the command uname -v will show the XNU build version string, which includes the Darwin version number.
Due to the free software nature of Darwin, there have been projects that aim to modify or enhance the operating system.
OpenDarwin was a community-led operating system based on the Darwin system. It was founded in April 2002 by Apple Inc. and Internet Systems Consortium. Its goal was to increase collaboration between Apple developers and the free software community. Apple benefited from the project because improvements to OpenDarwin would be incorporated into Darwin releases; and the free/open source community benefited from being given complete control over its own operating system, which could then be used in free software distributions such as GNU-Darwin.[32]
On July 25, 2006, the OpenDarwin team announced that the project was shutting down, as they felt OpenDarwin had ""become a mere hosting facility for Mac OS X related projects"", and that the efforts to create a standalone Darwin operating system had failed. They also state: ""Availability of sources, interaction with Apple representatives, difficulty building and tracking sources, and a lack of interest from the community have all contributed to this.""[33] The last stable release was version 7.2.1, released on July 16, 2004.[34]
PureDarwin is a project to create a bootable operating system image from Apple's released source code for Darwin.[35] Since the cessation of OpenDarwin and the release of bootable images since Darwin 8.x, it has been increasingly difficult to create a full operating system as many components become closed source. The project has managed to create an Xmas release based on Darwin 9 with an X11 GUI[36] and a command-line only 17.4 Beta based on Darwin 17.[37]
"
TextEdit - Wikipedia," 
TextEdit is a simple, open-source word processor and text editor, first featured in NeXT's NeXTSTEP and OpenStep. It is now distributed with macOS since Apple Inc.'s acquisition of NeXT, and available as a GNUstep application for other Unix-like operating systems such as Linux.[2] It is powered by Apple Advanced Typography and has many advanced typographic features.
TextEdit replaced the text editor of previous Macintosh operating systems, SimpleText. TextEdit uses the Cocoa text system to read and write documents in Rich Text Format (RTF), Rich Text Format Directory, plain text, and HTML formats, and can open (but not save) old SimpleText files. It also has access to the operating system's built-in spell-checking service. The version included in Mac OS X v10.3 added the ability to read and write documents in Word format, and the version in Mac OS X v10.4 added the ability to read and write Word XML documents. The version included in Mac OS X v10.5 added read and write support for Office Open XML and OpenDocument Text. The version included in Mac OS X v10.6 added automatic spelling correction, support for data detectors, and text transformations. The version included in Mac OS X v10.7 added versioning of files, and Autosave similar to iOS.
Formatted text, justification, and even the inclusion of graphics and other multimedia elements are supported by TextEdit, as well as the ability to read and write to different character encodings, including Unicode (UTF-8 and UTF-16). TextEdit automatically adjusts letter spacing in addition to word spacing while justifying text. TextEdit does not support multiple columns of text.
The high-resolution TextEdit 1.5 icon found in Mac OS X versions starting with 10.5 (Leopard) features an extract from Apple's ""Think different"" ad campaign. This was replaced by a blank sheet of notebook paper in 10.10 (Yosemite).

Apple formerly distributed TextEdit's source code as part of the documentation of its integrated development environment (IDE) Xcode. On the Internet, the source code of TextEdit can be found in Apple's Mac Developer Library.[3] The following quote is from the characteristic part of the New BSD-compliant license text included in the source code: .mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}[…] In consideration of your agreement to abide by the following terms, and subject to these terms, Apple grants you a personal, non-exclusive license, under Apple's copyrights in this original Apple software (the ""Apple Software""), to use, reproduce, modify and redistribute the Apple Software, with or without modifications, in source and/or binary forms; provided that if you redistribute the Apple Software in its entirety and without modifications, you must retain this notice and the following text and disclaimers in all such redistributions of the Apple Software. Neither the name, trademarks, service marks or logos of Apple Computer, Inc. may be used to endorse or promote products derived from the Apple Software without specific prior written permission from Apple. Except as expressly stated in this notice, no other rights or licenses, express or implied, are granted by Apple herein, including but not limited to any patent rights that may be infringed by your derivative works or by other works in which the Apple Software may be incorporated.[…]"
iDVD - Wikipedia," 
iDVD is a discontinued[1] DVD-creation application for Mac OS X produced by Apple Inc. iDVD allows the user to burn QuickTime movies, MP3 music, and digital photos to a DVD that can then be played on a commercial DVD player. It was often considered the last step of Apple's iLife suite, bringing together the results of all of the other iLife apps onto a removable medium.
While initially available only for Macs with a SuperDrive, it was included until 2011 with all new Macs; from iDVD 6 onwards, Apple supported the ability to burn projects with third-party optical drives. iDVD was no longer preinstalled on Macs shipping with Mac OS X 10.7 Lion, and was not available on the Mac App Store with all of the other iLife apps. It was, however, still available in the boxed copy of iLife '11, until the release of iLife '13.[2]
It is no longer functional on macOS 10.15+ due to lack of 32-bit support.
iDVD included over 100 Apple-designed themes for DVD menus and submenus, which allowed for the easy creation of DVD menu systems. Each theme included ""drop zones,"" onto which movies or photographs could be placed, some of which could be animated automatically. Any theme could be applied to each of the menus in an iDVD project.[3]
iDVD integrated tightly with the rest of the iLife suite, as well as with Final Cut Express and Final Cut Pro. iMovie projects and iPhoto slideshows could be exported directly to iDVD. In the case of iMovie projects, scene selection menus were automatically created in accordance with chapter markers that were set within iMovie. The application also had a Media panel that provided access to the user's iTunes library, iPhoto library and Movies folder at any time. It also provided a map view, which showed a flow chart of the project's menu system.[4] Another feature was the ability to hide or show an approximation of the 'TV-safe area' (as old televisions often cut off some of a video's outer areas). iDVD also incorporated a 'One-Step DVD' function, which would automatically rewind the currently connected DV camcorder and burn a DVD of the video footage stored on the tape.
iDVD shipped with fonts (located at /Applications/iDVD.app/Contents/Resources/Fonts) that were not installed, to prevent them from being available to other applications by default.
Incompatible with later versions of Mac OS X 10.5
First Universal binary version. Refined look based on iTunes 5 and 6.
"
MacTerminal - Wikipedia," 
MacTerminal was the first telecommunications and terminal emulation application software program available for the classic Mac OS. MacTerminal enabled users to connect via modem or serial port to bulletin board systems and online services (e.g., The Source, CompuServe), and to other computers. MacTerminal was capable of emulating the DEC VT100 and other computer terminals.
Apple Computer began retailing MacTerminal in July 1984 following the launch of the Macintosh 128K (the first Apple Macintosh) in January. Although MacTerminal was compatible with the original 128K model using Apple's optional 300 or 1200 bit/s external modem designed for the Apple II, MacTerminal was not available for general release at the 128K's launch date. Apple began bundling MacTerminal with later Macintosh models.
When Apple Computer, Inc. spawned Claris in 1987 as its application software division, Claris continued development of most of Apple's major applications, but development of MacTerminal ceased. However, similar functionality was rolled into ClarisWorks' terminal program.
"
macOS Catalina - Wikipedia," 
macOS Catalina (version 10.15) is the sixteenth major release of macOS, Apple Inc.'s desktop operating system for Macintosh computers. It is the successor to macOS Mojave and was announced at WWDC 2019 on June 3, 2019 and released to the public on October 7, 2019. Catalina is the first version of macOS to support only 64-bit applications and the first to include Activation Lock.[3][4] It is also the last version of macOS to have the version number prefix of 10. Its successor, Big Sur, is version 11.[5] macOS Big Sur succeeded macOS Catalina on November 12, 2020.[6]
The operating system is named after Santa Catalina Island, which is located off the coast of southern California.
macOS Catalina officially runs on all standard configuration Macs that supported Mojave. 2010–2012 Mac Pros, which could run Mojave only with a GPU upgrade, are no longer supported.[4] Catalina requires 4 GB of memory, an increase over the 2 GB required by Lion through Mojave.[7][8]
It is possible to install Catalina on many older Macintosh computers that are not officially supported by Apple. This requires using a patch to modify the install image.[9]
Catalyst is a new software-development tool that allows developers to write apps that can run on both macOS and iPadOS. Apple demonstrated several ported apps, including Jira and Twitter (after the latter discontinued its macOS app in February 2018).[10][11]
An upgrade from Kexts. System extensions avoid the problems of Kexts. There are 3 kinds of System extensions: Network Extensions, Endpoint Security Extensions, and Driver Extensions. System extensions run in userspace, outside of the kernel.[12][13] Catalina will be the last version of macOS to support legacy system extensions.[14][15]
A replacement for IOKit device drivers, driver extensions are built using DriverKit. DriverKit is a new SDK with all-new frameworks based on IOKit, but updated and modernized. It is designed for building device drivers in userspace, outside of the kernel.[16][13]
Mac apps, installer packages, and kernel extensions that are signed with a Developer ID must be notarized by Apple to run on macOS Catalina.[17]
Activation Lock helps prevent the unauthorized use and drive erasure of devices with an Apple T2 security chip (2018, 2019, and 2020 MacBook Pro; 2020 5K iMac; 2018 MacBook Air, iMac Pro; 2018 Mac Mini; 2019 Mac Pro).[4][18]
The system runs on its own read-only volume, separate from all other data on the Mac.[4]
Users can give detailed voice commands to applications.[19] On-device machine processing is used to offer better navigation.[4]
Sidecar allows a Mac to use an iPad (running iPadOS) as a wireless external display. With Apple Pencil, the device can also be used as a graphics tablet for software running on the computer.[11][20] Sidecar requires a Mac with Intel Skylake CPUs and newer (such as the fourth-generation MacBook Pro), and an iPad that supports Apple Pencil.[21][22]
The Game Controller framework adds support for two major console game controllers: the PlayStation 4's DualShock 4 and the Xbox One controller.[23][24][25][26]
iTunes is replaced by separate Books, Music, Podcasts, and TV apps, in line with iOS. iOS device management is now conducted via Finder.[27][28] The TV app on Mac supports Dolby Atmos, Dolby Vision, and HDR10 on MacBooks released in 2018 or later, while 4K HDR playback is supported on Macs released in 2018 or later when connected to a compatible display.[11]
Find My Mac and Find My Friends are merged into an application called Find My.
Among other visual and functional overhauls, attachments can be added to reminders and Siri can intelligently estimate when to remind the user about an event.[4]
macOS Catalina exclusively supports 64-bit applications. 32-bit applications no longer run (including all software that utilizes the Carbon API as well as QuickTime 7 applications, image, audio and video codecs). Apple has also removed all 32-bit-only apps from the Mac App Store.[29]
Zsh is the default login shell and interactive shell in macOS Catalina,[30] replacing Bash, the default shell since Mac OS X Panther in 2003.[31] Bash continues to be available in macOS Catalina, along with other shells such as csh/tcsh and ksh.
Dashboard has been removed in macOS Catalina.[32]
The ability to add Backgrounds in Photo Booth was removed in macOS Catalina.
The command-line interface GNU Emacs application was removed in macOS Catalina.
Built-in support for Perl, Python 2.7 and Ruby are included in macOS for compatibility with legacy software.[33] Future versions of macOS will not include scripting language runtimes by default, possibly requiring users to install additional packages.[34]
Legacy AirDrop for connecting with Macs running Mac OS X Lion, Mountain Lion and Mavericks, or 2011 and older Macs has been removed.[35]
Catalina received favourable reviews on release for some of its features.[36] However, some writers and bloggers said that the OS was unreliable.[37][38][39][40][41] Similar to the addition of User Account Control dialog boxes on Windows Vista the previous decade, prompts for allowing software access to sensitive data were criticized by some writers as annoying.[39][42]
Security content
Security content
Security content
macOS 10.15.2 Combo Update
Security content
macOS 10.15.3 Combo Update
Security content
macOS 10.15.4 Combo Update
Security content
macOS 10.15.5 Combo Update
Security content
Security content
macOS 10.15.6 Combo Update
Security content
macOS 10.15.7 Combo Update
Security content
macOS 10.15.7 Supplemental Update (Combo)
"
NeXTSTEP - Wikipedia," NeXTSTEP is a discontinued object-oriented, multitasking operating system based on the Mach kernel and the UNIX-derived BSD. It was developed by NeXT Computer in the late 1980s and early 1990s and was initially used for its range of proprietary workstation computers such as the NeXTcube. It was later ported to several other computer architectures.
Although relatively unsuccessful at the time, it attracted interest from computer scientists and researchers. It was used as the original platform for the development of the Electronic AppWrapper,[1] the first commercial electronic software distribution catalog to collectively manage encryption and provide digital rights for application software and digital media, a forerunner of the modern ""app store"" concept. It was also the platform on which Tim Berners-Lee created the first web browser, and on which id Software developed the video games Doom and Quake.[2][3]
After the purchase of NeXT by Apple, it became the source of the popular operating systems macOS, iOS, iPadOS, watchOS, and tvOS. Many bundled macOS applications, such as TextEdit, Mail, and Chess, are descendants of NeXTSTEP applications.
NeXTSTEP (also stylized as NeXTstep, NeXTStep, and NEXTSTEP[4]) is a combination of several parts:
NeXTSTEP is notable for having been a preeminent implementation of the latter three items. The toolkits offer considerable power, and are the canonical development system for all of the software on the machine.
It introduced the idea of the Dock (carried through OpenStep and into today's macOS) and the Shelf. NeXTSTEP also originated or innovated a large number of other GUI concepts which became common in other operating systems: 3D ""chiseled"" widgets, large full-color icons, system-wide drag and drop of a wide range of objects beyond file icons, system-wide piped services, real-time scrolling and window dragging, properties dialog boxes called ""inspectors"", and window modification notices (such as the saved status of a file). The system is among the first general-purpose user interfaces to handle publishing color standards, transparency, sophisticated sound and music processing (through a Motorola 56000 DSP), advanced graphics primitives, internationalization, and modern typography, in a consistent manner across all applications.
Additional kits were added to the product line to make the system more attractive. These include Portable Distributed Objects (PDO), which allow easy remote invocation, and Enterprise Objects Framework, a powerful object-relational database system. The kits made the system particularly interesting to custom application programmers, and NeXTSTEP had a long history in the financial programming community.[citation needed]
A preview release of NeXTSTEP (version 0.8) was shown with the launch of the NeXT Computer on October 12, 1988. The first full release, NeXTSTEP 1.0, shipped on September 18, 1989.[5] The last version, 3.3, was released in early 1995, by which time it ran on not only the Motorola 68000 family processors used in NeXT computers, but also on Intel x86, Sun SPARC, and HP PA-RISC-based systems.
NeXTSTEP was later modified to separate the underlying operating system from the higher-level object libraries. The result was the OpenStep API, which ran on multiple underlying operating systems, including NeXT's own OPENSTEP, Windows NT[6] and Solaris. NeXTSTEP's legacy stands today in the form of its direct descendants, Apple's macOS, iOS, watchOS, and tvOS operating systems.
From day one, the operating system of NeXTSTEP was built upon Mach/BSD.
The first web browser, WorldWideWeb, and the first-ever app store[7] were all invented on the NeXTSTEP platform.
1990 CERN: A Joint proposal for a hypertext system is presented to the management. Mike Sendall buys a NeXT cube for evaluation, and gives it to Tim Berners-Lee. Tim's prototype implementation on NeXTStep is made in the space of a few months, thanks to the qualities of the NeXTStep software development system. This prototype offers WYSIWYG browsing/authoring! Current Web browsers used in ""surfing the Internet"" are mere passive windows, depriving the user of the possibility to contribute. During some sessions in the CERN cafeteria, Tim and I try to find a catching name for the system. I was determined that the name should not yet again be taken from Greek mythology. Tim proposes ""World-Wide Web"". I like this very much, except that it is difficult to pronounce in French...Some features and keyboard shortcuts now commonly found in web browsers can be traced back to NeXTSTEP conventions. The basic layout options of HTML 1.0 and 2.0 are attributable to those features available in NeXT's Text class.[9]
Features seen first on NeXTSTEP:
In the 1990s, the pioneering PC games Doom (with its WAD level editor), Doom II, and Quake (with its respective level editor) were developed by id Software on NeXT machines. Other games based on the Doom engine such as Heretic and its sequel Hexen by Raven Software as well as Strife by Rogue Entertainment were also developed on NeXT hardware using id's tools.[10]
Altsys made a NeXTSTEP application called Virtuoso, version 2 of which was ported to Mac OS and Windows to become Macromedia FreeHand version 4. The modern ""Notebook"" interface for Mathematica, and the advanced spreadsheet Lotus Improv, were developed using NeXTSTEP. The software that controlled MCI's Friends and Family calling plan program was developed using NeXTSTEP.[11][12]
About the time of the release of NeXTSTEP 3.2, NeXT partnered with Sun Microsystems to develop OpenStep. It is the product of an effort to separate the underlying operating system from the higher-level object libraries to create a cross-platform object-oriented API standard derived from NeXTSTEP. The OpenStep API targets multiple underlying operating systems, including NeXT's own OPENSTEP. Implementations of that standard were released for Sun's Solaris, Windows NT, and NeXT's version of the Mach kernel. NeXT's implementation is called ""OPENSTEP for Mach"" and its first release (4.0) superseded NeXTSTEP 3.3 on NeXT, Sun, and Intel IA-32 systems.
Following an announcement on December 20, 1996,[13] Apple Computer acquired NeXT on February 4, 1997, for $429 million. Based upon the ""OPENSTEP for Mach"" operating system, and developing the OPENSTEP API to become Cocoa, Apple created the basis of Mac OS X,[14] and eventually, in turn, of iOS, watchOS, and tvOS.
A free software implementation of the OpenStep standard, GNUstep, also exists.[15]
Delivered on 2 CDs: NeXTSTEP CISC and NeXTSTEP RISC. The Developer CD includes libraries for all architectures, so that programs can be cross-compiled on any architecture for all architectures.
Allegedly dropped due to complaints of having to re-teach users but not for technical reasons (the new UI worked well in the beta).
Versions up to 4.1 are general releases. OPENSTEP 4.2 pre-release 2 is a bug-fix release published by Apple and supported for five years after its September 1997 release.

"
Doom engine - Wikipedia," id Tech 1, also known as Doom engine, is the game engine that powers the id Software games Doom and Doom II: Hell on Earth. It is also used in Heretic, Hexen: Beyond Heretic, Strife: Quest for the Sigil, Hacx: Twitch 'n Kill, Freedoom, and other games produced by licensees. It was created by John Carmack, with auxiliary functions written by Mike Abrash, John Romero, Dave Taylor, and Paul Radek. Originally developed on NeXT computers, it was ported to DOS for Doom's initial release and was later ported to several game consoles and operating systems.
The source code to the Linux version of Doom was released to the public under a license that granted rights to non-commercial use on December 23, 1997, followed by the Linux version of Doom II about a week later on December 29, 1997.[2] The source code was later re-released under the GNU General Public License on October 3, 1999.[3][4]
The dozens of unofficial Doom source ports that have been created since then allow Doom to run on previously unsupported operating systems and sometimes radically expand the engine's functionality with new features.
Although the engine renders a 3D space, that space is projected from a two-dimensional floor plan. The line of sight is always parallel to the floor, walls must be perpendicular to the floors, and it is not possible to create multi-level structures or sloped areas (floors and ceilings with different angles). Despite these limitations, the engine represented a technological leap from id's previous Wolfenstein 3D engine. The Doom engine was later renamed to ""id Tech 1"" in order to categorize it in a list of id Software's long line of game engines.[5]
The Doom engine separates rendering from the rest of the game. The graphics engine runs as fast as possible, but the game world runs at 35 frames per second regardless of the hardware, so multiple players can play against each other using computers of varying performance.[6]
A simple setup demonstrating how Doom represents levels internally
Viewed from the top down, all Doom levels are actually two-dimensional, demonstrating one of the key limitations of the Doom engine: room-over-room is not possible. This limitation, however, has a silver lining: a ""map mode"" can be easily displayed, which represents the walls and the player's position, much like the first image to the right.
The base unit is the vertex, which represents a single 2D point. Vertices (or ""vertexes"" as they are referred to internally) are then joined to form lines, known as ""linedefs"". Each linedef can have either one or two sides, which are known as ""sidedefs"". Sidedefs are then grouped together to form polygons; these are called ""sectors"". Sectors represent particular areas of the level.
Each sector contains a number of properties: a floor height, ceiling height, light level, a floor texture and a ceiling texture. To have a different light level in a particular area, for example, a new sector must be created for that area with a different light level. One-sided linedefs therefore represent solid walls, while two-sided linedefs represent bridge lines between sectors.
Sidedefs are used to store wall textures; these are completely separate from the floor and ceiling textures. Each sidedef can have three textures; these are called the middle, upper and lower textures. In one-sided linedefs, only the middle texture is used for the texture on the wall. In two-sided linedefs, the situation is more complex. The lower and upper textures are used to fill the gaps where adjacent sectors have different floor and ceiling heights: lower textures are used for steps, for example. The sidedefs can have a middle texture as well, although most do not; this is used to make textures hang in mid air. For example, when a transparent bar texture is seen forming a cage, this is an example of a middle texture on a two-sided linedef.
Doom makes use of a system known as binary space partitioning (BSP).[7]  A tool is used to generate the BSP data for a level beforehand. This process can take quite some time for a large level. It is because of this that it is not possible to move the walls in Doom; while doors and lifts move up and down, none of them ever move sideways.
The level is divided up into a binary tree: each location in the tree is a ""node"" which represents a particular area of the level (with the root node representing the entire level). At each branch of the tree there is a dividing line which divides the area of the node into two subnodes. At the same time, the dividing line divides linedefs into line segments called ""segs"".[8]
At the leaves of the tree are convex polygons, where further division of the level is not needed. These convex polygons are referred to as subsectors (or ""SSECTORS""), and are bound to a particular sector. Each subsector has a list of segs associated with it.[7]
The BSP system sorts the subsectors into the right order for rendering. The algorithm is fairly simple:
The process is complete when the whole column of pixels is filled (i.e., there are no more gaps left). This ordering ensures that no time is used drawing objects that are not visible and as a result maps can become very large without any speed penalty.
All of the walls in Doom are drawn vertically; it is because of this that it is not possible to properly look up and down. It is possible to perform a form of look up/down via ""y-shearing"", and many modern Doom source ports do this, as well as later games that use the engine, such as Heretic. Essentially this works by moving the horizon line up and down within the screen, in effect providing a ""window"" on a taller viewable area. By moving the window up and down, it is possible to give the illusion of looking up and down. However, this will distort the view the further up and down the player looks.
The Doom engine renders the walls as it traverses the BSP tree, drawing subsectors by order of distance from the camera so that the closest segs are drawn first. As the segs are drawn, they are stored in a linked list. This is used to clip other segs rendered later on, reducing overdraw. This is also used later to clip the edges of sprites.
Once the engine reaches a solid (1-sided) wall at a particular x ordinate, no more lines need to be drawn at that area. For clipping the engine stores a ""map"" of areas of the screen where solid walls have been reached. This allows far away parts of the level which are invisible to the player to be clipped completely.
The Doom graphic format stores the wall textures as sets of vertical columns; this is useful to the renderer, which essentially renders the walls by drawing many vertical columns of textures.
The system for drawing floors and ceilings (""flats"") is less elegant than that used for the walls. Flats are drawn with a flood fill-like algorithm. Because of this, it is sometimes possible if a bad BSP builder is used to get ""holes"" where the floor or ceiling bleeds down to the edges of the screen. This is also the reason that if the player travels outside of the level using the noclip cheat the floors and ceilings will appear to stretch out from the level over the empty space.
The floor and ceiling are drawn as ""visplanes"". These represent horizontal runs of texture, from a floor or ceiling at a particular height, light level and texture (if two adjacent sectors have exactly the same floor, these can get merged into one visplane). Each x position in the visplane has a particular vertical line of texture which is to be drawn.
Because of this limit of drawing one vertical line at each x position, it is sometimes necessary to split visplanes into multiple visplanes. For example, consider viewing a floor with two concentric squares. The inner square will vertically divide the surrounding floor. In that horizontal range where the inner square is drawn, two visplanes are needed for the surrounding floor.
This leads to one of Doom's classic limitations which frustrated many mappers for a long time. Doom contained a static limit on the number of visplanes; if exceeded, a ""visplane overflow"" would occur, causing the game to exit to DOS with one of two messages, ""No more visplanes!"" or ""visplane overflow (128 or higher)"". The easiest way to invoke the visplane limit is a large checkerboard floor pattern; this creates a large number of visplanes.
As the segs are rendered, visplanes are also added, extending from the edges of the segs towards the vertical edges of the screen. These extend until they reach existing visplanes. Because of the way this works, the system is dependent on the fact that segs are rendered in order by the overall engine; it is necessary to draw nearer visplanes first, so that they can ""cut off"" by others further away. If unstopped, the floor or ceiling will ""bleed out"" to the edges of the screen, as previously described. Eventually, the visplanes form a ""map"" of particular areas of the screen in which to draw particular textures.
While visplanes are constructed essentially from vertical ""strips"", the actual low level rendering is performed in the form of horizontal ""spans"" of texture. After all the visplanes have been constructed, they are converted into spans which are then rendered to the screen. This appears to be a trade off: it is easier to construct visplanes as vertical strips, but because of the nature of how the floor and ceiling textures appear it is easier to draw them as horizontal strips.
Each sector within the level has a linked list of things stored in that sector. As each sector is drawn the sprites are placed into a list of sprites to be drawn. If not within the field of view these are ignored.
The edges of sprites are clipped by checking the list of segs previously drawn. Sprites in Doom are stored in the same column based format as the walls are, which again is useful for the renderer. The same functions which are used to draw walls are used to draw sprites as well.
While subsectors are guaranteed to be in order, the sprites within them are not. Doom stores a list of sprites to be drawn (""vissprites"") and sorts the list before rendering. Far away sprites are drawn before close ones. This causes some overdraw but usually this is negligible.
There is a final issue of middle textures on 2-sided lines, used in transparent bars for example. These are mixed in and drawn with the sprites at the end of the rendering process, rather than with the other walls.
The Doom engine achieved most of its fame as a result of powering the classic first person shooter Doom, and it was used in several other games. It is usually considered that the ""Big Four"" Doom engine games are Doom, Heretic, Hexen: Beyond Heretic, and Strife: Quest for the Sigil.
"
id Tech - Wikipedia," 
id Tech is a series of separate game engines designed and developed by id Software. Prior to the presentation of the id Tech 5-based game Rage in 2011, the engines lacked official designation and as such were simply referred to as the Doom and Quake engines, from the name of the main game series the engines had been developed for. ""id Tech"" numbers 1, 2, 3, and 4 have been released as free software under the GNU General Public License, along with the source code to Wolfenstein 3D, Doom (id Tech 1), and Quake (id Tech 2). id Tech 7 is currently the latest utilized engine.
According to Eurogamer.net, ""id Software has been synonymous with PC game engines since the concept of a detached game engine was first popularised."" However id Tech 4 had far fewer licensees than the Unreal Engine from Epic Games, and id planned to regain the momentum with id Tech 5,[1] until they were bought by ZeniMax Media which intends to keep the id Tech engines exclusively for id's sister studios.
id Software had developed 3D engines for several games before Doom. Each engine had progressively more advanced 3D technology.
Originally known as the ""Doom engine"", this engine powers the id Software games Doom (1993) and Doom II: Hell on Earth (1994). It was created by John Carmack, with auxiliary functions written by John Romero, Dave Taylor, and Paul Radek. Initially developed on NeXT computers, it was ported to MS-DOS for Doom's release and was later ported to several game consoles and operating systems.
The code was also reused for other titles, such as Heretic and Hexen: Beyond Heretic (both by Raven Software), and Strife: Quest for the Sigil (Rogue Entertainment).
Previously known as ""Quake engine"" with its successor ""Quake II engine"", it was originally written to power 1996's Quake. It featured true 3D real-time rendering and is the first id Tech engine to use the client–server model.
The Quake engine was updated with a new executable titled QuakeWorld that contained code to enhance the networking capabilities of Quake in response to the demand for across-internet network games that arose as a result of Quake's usage of UDP for networking.
It was later updated again for the release of Quake II in 1997, with enhancements such as colored lighting and a new MD2 model format.[3] By way of Half-Life, it was also adapted into the GoldSrc engine and its successors, the Source engine and Source 2.
Previously known as the ""Quake III Arena engine"", it was used to power id Software's Quake III Arena in 1999.
The Quake III Arena engine was updated to patch 1.26 and later versions are called ""Quake III Team Arena engine"" with a new MD4 skeletal model format and huge outdoor areas. It was updated again with the 2001 release of Return To Castle Wolfenstein which included a single-player scripting system, and was eventually used to power the first Call of Duty title in 2003, ultimately spawning the IW engine.
Commonly known as the ""Doom 3 engine"" which was used to power Doom 3 as it released in 2004, id Tech 4 began as an enhancement to id Tech 3. During development, it was initially just a complete rewrite of the engine's renderer, while still retaining other subsystems, such as file access, and memory management.  The decision to switch from C to the C++ programming language necessitated a restructuring and rewrite of the rest of the engine; today, while id Tech 4 contains code from id Tech 3, much of it has been rewritten.[4]
Other games using this engine were Raven Software's Quake 4 (2005) and Wolfenstein (2009), Human Head Studios' Prey (2006), Splash Damage's Enemy Territory: Quake Wars (2007) and Brink (2011).
Used for id Software's Rage, the engine is based on the file system frameworks. Some technologies included are the GUI system from id Tech 4, including a new renderer, MegaTexture 2.0 technology, soft shadows and more.
id is requiring companies that use the engine to publish their games through id's sister company, Bethesda Softworks.[5]
The engine has since been used to power MachineGames' first two Wolfenstein titles; The New Order in 2014 with its standalone expansion The Old Blood, which released in 2015. It was also used for Tango Gameworks' The Evil Within (2014).
Used for Doom released on May 13, 2016.  While the engine uses some of the features from id Tech 5, id has also added support for Vulkan rendering. Development of the renderer is led by Tiago Sousa, who had previously worked on CryEngine, following previous technical director John Carmack's resignation in 2013. id Tech 6 was also used in Wolfenstein: Youngblood (2019) and Wolfenstein II: The New Colossus (2017), again by MachineGames. It was not used for Quake Champions however, which combined id Tech features with the Saber3D Engine.
The latest release of id tech released alongside Doom Eternal on March 20, 2020. At QuakeCon 2018 id Software announced a new game in the Doom franchise called Doom Eternal which is powered by the id Tech 7 engine.[6] The new engine is capable of delivering increase in geometric detail without drops in frame-rate vs. id Tech 6.[7] On PC, id Tech 7 supports Vulkan rendering only.[8]
"
Nvidia GameWorks - Wikipedia," Nvidia GameWorks is a middleware software suite developed by Nvidia.[1] The Visual FX, PhysX and Optix SDKs provide a wide range of enhancements pre-optimized for Nvidia GPUs. [2] GameWorks is partially open-source.[3] The competing solution being in development by AMD is GPUOpen, which was announced to be free and open-source software under the MIT License.
Nvidia Gameworks consists of several main components:
In addition, the suite contains sample code for DirectX and OpenGL developers, as well as tools for debugging, profiling, optimization and Android development.
"
GameSpy - Wikipedia," GameSpy was an American  provider of online multiplayer and matchmaking middleware for video games founded in 1995 by Mark Surfas.[2] After the release of a multiplayer server browser for the game, QSpy, Surfas licensed the software under the GameSpy brand to other video game publishers through a newly established company, GameSpy Industries, which also incorporated his Planet Network of video game news and information websites, and GameSpy.com.
GameSpy merged with IGN in 2004;[3][4] by 2014, its services had been used by over 800 video game publishers and developers since its launch.[5] In August 2012, the GameSpy Industries division (which remained responsible for the GameSpy service) was acquired by mobile video game developer Glu Mobile. IGN (then owned by News Corporation) retained ownership of the GameSpy.com website. In February 2013, IGN's new owner, Ziff Davis, shut down IGN's ""secondary"" sites, including GameSpy's network. This was followed by the announcement in April 2014 that GameSpy's service platform would be shut down on May 31, 2014.
The 1996 release of id Software's video game Quake, one of the first 3D multiplayer action games to allow play over the Internet, furthered the concept of players creating and releasing ""mods"" or modifications of games. Mark Surfas saw the need for hosting and distribution of these mods and created PlanetQuake, a Quake-related hosting and news site.[6][7] The massive success of mods catapulted PlanetQuake to huge traffic and a central position in the burgeoning game website scene.
Quake also marked the beginning of the Internet multiplayer real-time action game scene. However, finding a Quake server on the Internet proved difficult, as players could only share IP addresses of known servers between themselves or post them on websites. To solve this problem, a team of three programmers (consisting of Joe ""QSpy"" Powell, Tim Cook, and Jack ""morbid"" Matthews) formed Spy Software and created QSpy (or QuakeSpy). This allowed the listing and searching of Quake servers available across the Internet. Surfas licensed QSpy and became the official distributor and marketer while retaining the original programming team. QSpy became QuakeSpy and went on to be bundled with its QuakeWorld update - an unprecedented move by a top tier developer and huge validation for QuakeSpy. With the release of the Quake Engine-based game Hexen II, QuakeSpy added this game to its capabilities and was renamed GameSpy3D. In 1997 Mark Surfas licensed GameSpy 3D from Spy Software, and created GameSpy Industries.
In 1999, GameSpy received angel investment funding from entrepreneur David Berkus. The company released MP3Spy.com (later renamed RadioSpy.com), a software browser allowing people to browse and connect to online radio feeds, such as those using Nullsoft's ShoutCast. GameSpy received $3 million in additional funding from the Yucaipa Companies, an investment group headed by Hollywood agent Michael Ovitz and Southern California supermarket billionaire Ronald Burkle.
The expanding of the company's websites included the games portal, GameSpy.com, created in October 1999;[8] the Planet Network (also known as the GameSpy Network), a collection of ""Planet"" websites devoted to popular video games (such as Planet Quake, Planet Half-Life and Planet Unreal) as well as the genre-related websites, 3DActionPlanet, RPGPlanet, SportPlanet and StrategyPlanet; ForumPlanet, the network's extensive message board system; and FilePlanet, which was one of the largest video game file download sites. It also included platform-specific sites (e.g., Planet PS2, Planet Xbox, Planet Nintendo and Planet Dreamcast), but these were consolidated into GameSpy.com; only Classic Gaming remains separate. ForumPlanet and FilePlanet were services offered by GameSpy, and were not part of the Planet Network.
In 2000, GameSpy received additional investment funding from the Ziff Davis publishing division ZDNet.com and from Guillemot Corporation. GameSpy shut down its RadioSpy division, backing away from the online music market which was dominated by peer-to-peer applications such as Napster and Gnutella. In 2001, GameSpy's corporate technology business grew to include software development kits and middleware for video game consoles, such as Sony's PlayStation 2, Sega's Dreamcast and Microsoft's Xbox. In March 2007, IGN and GameSpy Industries merged, and was briefly known as IGN/GameSpy before formalizing their corporate name as IGN Entertainment.[9]
Also in 2000, GameSpy turned GameSpy3D into GameSpy Arcade and purchased RogerWilco, MPlayer.com and various assets from HearMe; the MPlayer service was shut down and the RogerWilco technology is improved and incorporated into GameSpy Arcade. GameSpy Arcade was the company's flagship matchmaking software, allowing users to find servers for different online video games (whether they be free or purchased) and connect the user to game servers of that game. GameSpy also published the Roger Wilco voice chat software, primarily meant for communication and co-ordination in team-oriented games, where users join a server to chat with other users on the server using voice communication. This software rivaled the other major voice chat software Ventrilo and Teamspeak. The company's ""Powered by GameSpy"" technology enabled online functionality in over 300 PC and console games.[10] In 2005, GameSpy added the PlayStation Portable, and Nintendo DS[10] to its stable supported platforms. In March 2007, GameSpy added the Wii as another supported platform.[7]
GameSpy Industries (the entity responsible for GameSpy multiplayer services) was bought from IGN Entertainment by Glu Mobile in August 2012,[11] and proceeded in December to raise integration costs and shut down servers for many older games, including Star Wars: Battlefront, Sniper Elite, Microsoft Flight Simulator X, Saints Row 2, and Neverwinter Nights, with no warning to developers or players, much to the outrage of communities of those games.[12]  GameSpy Technologies remained operational as a separate entity since.[13] In February 2013, following the acquisition of IGN Entertainment by Ziff Davis, IGN's ""secondary"" sites were shut down, ending GameSpy's editorial operations.[1][9]
In April 2014, Glu announced that it would shut down the GameSpy servers on May 31, 2014, so its developers could focus on work for Glu's own services. Games that still used GameSpy are no longer able to offer online functionality or multiplayer services through GameSpy. While some publishers announced plans to migrate GameSpy-equipped games to other platforms (such as Steam or in-house servers), some publishers, such as Nintendo (who used the GameSpy servers as the basis of its Nintendo Wi-Fi Connection platform for DS and Wii games) did not, particularly due to the age of the affected games.[5][14][15] Electronic Arts, in particular, announced 24 PC games, including titles such as Battlefield 2, the Crysis series, Saints Row 2 and the Star Wars: Battlefront series, that would be affected by the end of GameSpy service.[16]
Fan-created Game mods restored online functionality with alternative servers. One such mod for the PC version of Halo was officially incorporated into a patch for the game released by Bungie in May 2014, and Disney helped developers create a similar mod for Battlefront II (2005) in 2017. By contrast, in 2017, Electronic Arts demanded the takedown of modified versions of Battlefield 2 and Battlefield 2142 on alternate servers, distributed by a group known as ""Revive Network"", as infringement of their copyrights.[17][18][19]
The GameSpy Debriefings was a party-style discussion between editors of GameSpy and IGN Entertainment on (purportedly) that week's gaming news.[20] The GameSpy Debriefings was the 25th most popular podcast under the category “Games and Hobbies” on iTunes (as of May 1, 2011). It was however infamous for the crew's frequent propensity to de-rail the conversation from video games into explicit content or in-depth discussions about nerd culture.
The main crew at the show's conclusion of The GameSpy Debriefings consisted of:
Frequent guests  included:
On July 30, 2011, The GameSpy Debriefings ended with an episode consisting of only the main crew. Following its conclusion, they launched a fundraising drive on Kickstarter which resulted in the release of their own popular podcast, The Comedy Button.[21] The Comedy Button is similar in content to the later GameSpy Debriefings, with a renewed focus on humorous discussions and listener e-mails rather than the in-depth discussion of recent video games like the early Debriefings.
As of September 4, 2019, The Comedy Button has produced 400 episodes.
"
Eurogamer - Wikipedia," 
Eurogamer is a British  video game journalism website owned by Gamer Network, both formed alongside each other in 1999. Its editor is Oli Welsh.
Eurogamer (initially stylised as EuroGamer) was launched on 4 September 1999.[1] The founding team included John ""Gestalt"" Bye, the webmaster for the PlanetQuake website and a writer for British magazine PC Gaming World; Patrick ""Ghandi"" Stokes, a contributor for the website Warzone; and Rupert ""rauper"" Loman, who had organised the EuroQuake esports event for the game Quake.[1]
In January 2008, Tom Bramwell overtook the role of editor-in-chief from Kristan Reed, remaining in that role until he resigned in November 2014.[2][3] Since then, Oli Welsh served as editor for Eurogamer.[4]
It is known for the EGX, formerly Eurogamer Expo, trade fair organised by its parent company since 2008.[5]
In February 2015, Eurogamer dropped its ten-point scale for review scores in favour of a ""recommendation system"", in which a game would be labelled as ""Essential"", ""Recommended"" or ""Avoid"".[6]
Eurogamer is the principal site of the Gamer Network family of video game-related websites. It has several regional sub-outlets:
Digital Foundry, founded in 2004, has been hosted on Eurogamer since 2007 and is led by Richard Leadbetter. It performs technical analyses of games.[19]
"
Estates Gazette - Wikipedia," 
Estates Gazette is a weekly business magazine published for the UK commercial property market. It was first published in 1858 and celebrated its 150th anniversary in 2008. Damian Wild has been its editor since August 2009.[1]
In March 2008, Estates Gazette was announced as one of the top 500 ""Business Superbrands"" in the UK.[2]
In 1996, Estates Gazette launched its own online property news and research arm, EGi. In 1997, the group launched Propertylink, the UK's largest free-access commercial property availability search website. The group also publishes EuroProperty, a fortnightly publication for the pan-European real estate investment market.[citation needed]
The group's services are published by Reed Business Information. The publication hosts its own ""EG Awards"" annually, the show being held in London each year.[3]

"
Safestore - Wikipedia," Safestore is the UK’s largest and Europe’s second largest provider of self-storage.[4][5] It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index.
Safestore was founded in the UK in 1998,[6] and floated on the Alternative Investment Market (AIM) that year. In 2003, the company delisted from the AIM after a Bridgepoint-backed £39.8 million Management Buyout led by Steve Williams (Chief Executive at the time).[7]
In 2004, Safestore acquired Mentmore plc[8] for £209 million who were trading under the ‘Spaces’ brand in the UK and as the ‘Une Pièce en Plus’ (UPEP) brand in France.[9] The company has been listed in the London Stock Exchange since 2007.[10]  In April 2013, Safestore was converted into a real estate investment trust (REIT).[11]
Safestore acquired Space Maker in July 2016 (adding 12 stores to the UK operation)[12][13][14] and it completed the acquisition of Alligator Self Storage in November 2017.[15]
Safestore acquired the Ready Steady store in Heathrow from Rockpool Investments in August 2019[16] and two stores joined the Safestore group in London through the acquisition of Fort Box Self Storage in November 2019.[17]
Safestore also established a JV with Carlyle which acquired M3 Self Storage in the Netherlands (six stores in Amsterdam and Haarlem) in August 2019.[18] Safestore also acquired Oh My Box Self storage (four stores in central Barcelona) in January 2020.[19]
"
BB Healthcare Trust - Wikipedia," BB Healthcare Trust (LSE: BBH) is a large British investment trust dedicated to investments in listed or quoted healthcare companies on a worldwide basis. Established in 2016,[1] the company is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index.[2] The chairman is Randeep Grewal.[3]
"
Bellway - Wikipedia," 
Bellway plc is a residential property developer based in Newcastle upon Tyne, England.[2]  It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index.
The company was founded in 1946 by John Thomas Bell and his sons John and Russell as a housebuilder operating in Newcastle upon Tyne under the name John T. Bell & Sons.[3] In 1951 Kenneth Bell, the youngest of the brothers, joined the business.[4]
The three brothers also developed commercial property in the 1950s and their company, North British Properties, was floated on the London Stock Exchange in 1961. In 1963, North British acquired John T Bell in a reverse takeover.[5]
The Bell family managed to tap into the huge demand for private housing that followed World War II promoting developments such as Cramlington New Town, built in partnership with William Leech in the early 1960s.[6] Bellway developed a substantial housebuilding operation in the north of England and sales reached 1,500 units in 1972 with a further 500 in the newly formed Australian and French subsidiaries. In 1973, Bellway moved into the south-east with the purchase of A & R A Searle. The group continued to expand through England in the 1970s but its overseas operations were less successful and were eventually closed.[7]
In 1979 the ""Bellway"" private housebuilding business was demerged from the commercial side of the business under the leadership of Kenneth Bell. In 1981 Bellway and fellow Newcastle housebuilder William Leech announced a merger but it was called off within days: ""the lifestyle of the two firms looked pretty incompatible"".[8]
Diversification had not been wholly satisfactory; Ken Bell became largely non-executive and the day-to-day running of the business was assumed by Howard Dawe. Dawe reorganised the business, resumed the regional expansion on a more profitable basis and increased the company's focus on regeneration sites.[7]
Family involvement with the company ended with the death of Kenneth Bell in 1997.[9]
In 2018, the company was reported to be on track to build 10,000 homes for the first time in its history. This activity was attributed to low interest rates and good mortgage finance providing buoyancy to the housing market.[10]
In common with other housebuilders, Bellway was adversely affected by the COVID-19 pandemic in the United Kingdom during 2020; in June, it reported sales had fallen by more than two-thirds since the introduction of lockdown, and expected ""year-on-year sales activity to be severely constrained until a time when 'lockdown' restrictions are further lifted.""[11] A month later, it announced plans to cut up to 175 jobs, around 6% of its 3,100-strong workforce.[12]
A major fire broke out in July 2015 at a housing development constructed by Bellway in Canterbury, which destroyed and damaged 45 homes. An investigation launched in 2016 discovered problems in the fire separation constructed between the properties. Repairs to the development began in November 2018.[13]
In May 2019 a Watchdog investigation was screened on BBC One regarding the fire safety of Bellway and Persimmon plc homes. In the programme a surveyor visited an estate developed by Bellway after concerns about fire safety had been raised by a resident. The investigation found safety breaches in every property that was looked at due to poorly fitted fire barriers.[14]
On 9 June 2019 a fire took place at the newly built Samuel Garside House located in De Pass Garden, which was constructed by Bellway. Peter Mason, chair of the Barking Reach residents’ association had contacted Bellway prior to the fire expressing concern about the potential fire risk of the development, but was told not to worry.[15] London mayor Sadiq Khan described the fire as ""shocking"" and stated that it could have ""easily resulted in fatalities"".[16]
The company does not have national house types and prefers to develop local designs with the help of local people.[17] It achieved a 5 star rating in the 2015/16 Home Builders Federation new home customer satisfaction survey.[18]
Official site
"
LXi REIT - Wikipedia," LXi REIT is a real estate investment trust based in London, England. It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index.
The company was established by Osprey Equity Partners with support from Ram Bhavnani, a wealthy Spanish investor, in 2016.[2] Bhavnani's previous ventures included a significant investment in Bankinter, a Spanish-based bank.[2] LXi REIT was the subject of an initial public offering in 2018.[2]
The company has a portfolio of commercial properties rented out to corporate customers on 20 to 30-year inflation-linked leases.[3] The net book value of the portfolio as at 31 March 2019 was £0.5 billion.[1]
"
Genesis Emerging Markets Fund - Wikipedia," Genesis Emerging Markets Fund Ltd (LSE: GSS) is a large Guernsey-incorporated, London-based closed-end investment fund focused predominantly on holdings in the stock markets of emerging economies.[1] Established in July 1989, the company is a constituent of the FTSE 250 Index. The fund is managed under the auspices of Genesis Investment Management and its chairman is Coen Teulings.[2]
"
Clarkson plc - Wikipedia," 
Clarkson PLC, often referred to simply as Clarksons, is a provider of shipping services, and is headquartered in London.[3] In 2011, Lloyd's List described the company's shipbroking service as the ""undisputed heavyweight of the shipbroking market"".[4] It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index.
The company was founded by Horace Anderton Clarkson in London in 1852.[5] The son of a prosperous lawyer, he invited Leon Benham, a former colleague, to join him in partnership.[6] Benham's son Henry soon joined the business.[7] In the 1850s the business involved sailing ships, but by the 1860s the company was chartering steam ships.[7] In 1872 Clarksons became shipowners with the acquisition of three schooners.[7] The company became the world's largest tanker broker in 1929 when Esso appointed Clarksons as its exclusive shipbroker.[6]
The first overseas office opened in New York City in 1954. Offices soon followed in France, Australia and Germany, and in the 1960s, South Africa and Greece.[6] Expansion in the Far East began in the 1990s when companies were established in Hong Kong, Shanghai and Singapore.[6]
Carron Greig became joint managing director in 1962, served as chairman of the shipbroking division from 1972 to 1985, and as chairman of the whole group, then known as Horace Clarkson, until his retirement in 1993.[8] He was credited with transforming the group into a world leader in its field.[8] The company became a global force and Greig increased its business, especially with Esso, bringing in Brazil, Norway and the Far East to its portfolio.[8] He also fought off Russian and English corporate raiders in the 1970s.[8]
The company spun off various diversified assets as Shipping Industrial Holdings in 1974.[6] Michael Wade, a prominent insurance executive, sat on the board from 1984 to 1993[9] and Peter Parker, a leading industrial executive, sat on the board from 1971 until 1993.[9]
Richard Fulford-Smith became chief executive of the company in 2004.[10] According to Robert Wright of the Financial Times, he was central to Clarkson's emergence as the world's largest sale and purchase shipbroker.[10] Fulford-Smith was ousted from the board in 2008 after Russian litigation claims emerged against the company.[11]
Until March 2012, Martin Stopford, a prominent economist, sat on the board and was head of the research arm of the company.[12] In 2013, Clarkson acquired Gibb Tools, a North Sea oil engineering tools supplier, for £12.7 million.[3] In 2015, Clarkson acquired RS Platou AS, a Norwegian headed shipbroking and investment banking group.[13]
Clarkson operations are divided into four areas: broking, financial, support and research. The company brokers vessels for some of the world's largest producers and traders of natural resources.[14] Clarkson Research Services focuses primarily on the collection, validation, analysis and management of data about the merchant shipping and offshore markets.[15] The chief executive since 2008 has been Andi Case.[16] William (Bill) Thomas was appointed as Chair on 13 February 2019.[17] The company has its headquarters at Commodity Quay, St Katharine Docks, London.[18]
"
Dunelm Group - Wikipedia," 
Dunelm (Soft Furnishings) Limited (formerly Dunelm Mill (Soft Furnishings) Limited) is a British home furnishings retailer with 169 superstores, 3 high street stores and over 100 in-store Pausa coffee shops, throughout the United Kingdom. One of the largest homewares retailers in the United Kingdom, Dunelm's headquarters are in Watermead Business Park, Syston in Leicestershire, England. It also has its own factory for curtains, blinds and accessories, based in Leicester.
It is listed on the London Stock Exchange and is a constituent of the FTSE 250 Index.
Dunelm was founded in 1979 by Bill Adderley and Jeany Adderley, trading in home textiles from a market stall in Leicester.[2] The first Dunelm store opened in Churchgate Leicester in 1984[2] with the first superstore opening in Rotherham in 1991.[2] In 1996 Will Adderley took over responsibility for the day-to-day running of the company from his father, Bill Adderley. The expansion of Dunelm continued with a new head office and warehouse being established in 1999 in Syston, Leicestershire.[2]
In 2001 the company ventured into manufacturing, acquiring Bellbird producing custom-made curtains, blinds, and accessories, with the facility now being known as Dunelm's Manufacturing Centre. On reaching their 50th store (Walsall) Dunelm opened a new warehouse in Burton.[2] Key appointments were made in 2003 with David Stead being brought in as Finance Director; this also coinciding with Dunelm's 60th store (Ilkeston) and the roll-out of EPOS.[2] 2004 saw the company appointing Geoff Cooper as Non-Executive chairman and Marion Sears as a non-executive director. It also saw the opening of their 70th store (Trafford). Two years later Dunelm opened its 80th store (Bradford), a new distribution centre in Stoke, and launched their online shopping facility, offering 13,000 homewares products and floated on the London Stock Exchange with it now being a constituent of the FTSE 250 Index.[3]
2007 saw the appointment of Simon Emney as non-executive director followed in 2008 with their 90th store (Plymouth) and the acquisition of the worldwide rights to the 'Dorma' bed linen brand, for £5 million in July.[4] In 2009 Dunelm appointed Nick Wharton as a non-executive director and re-launched their online shopping website.[5] In September 2009, the company announced that Nick Wharton would be taking over from Will Adderley as Chief Executive in March 2011 with Adderley remaining at Dunelm as Executive Deputy chairman.[6]
In September 2014 Dunelm Group plc announced that Nick Wharton had resigned his position as Chief Executive and was stepping down from the Board. Will Adderley, previously Executive Deputy chairman, resumed the role of Chief Executive with immediate effect.[7] On 28 November 2016, the company purchased WorldStores and its subsidiary Kiddicare for £8.5 million.[8]
On 30 August 2017 Dunelm Group plc announced that John Browett was stepping down with immediate effect as Chief Executive after two years in the role.[9]
In September 2020, the company reported a massive surge in sales for the months of July and August. The increased sales were a result of people working from home as a result of the COVID-19 pandemic and investing in their living spaces.[10]
In November 2020, the company was criticised by shareholders for renominating Paula Vennells to its board, despite her responsibility for the Post Office subpostmasters' scandal, during which her leadership was accused of having been ""both cruel and incompetent"" by a Conservative peer and various MPs.[11]
As of 30 June 2018 Dunelm operated 169 stores, spread across the UK, and a webstore.[1]
"
Category:Companies based in Leicestershire - Wikipedia," This category has the following 7 subcategories, out of 7 total.
The following 43 pages are in this category, out of  43 total. This list may not reflect recent changes (learn more).
"
Joules (clothing) - Wikipedia," Joules is a British clothing company which sells clothing and homeware products inspired by British country lifestyles.[1][2] Its founder Tom Joule described its business model in 2011 as creating clothing with ""colour and fun and entertainment"".[3]
Established to sell clothing at country shows, the company established its own clothing line in 1999 and began to open shops in the 2000s.[4][5] As of 2018, the company had 123 stores and a turnover of £185.9m.[6]
Originally established as Joule & Sons in 1977 by Ian Joule, his son Tom took over the business in 1989. Joule & Son originally sold branded clothing and accessories at equestrian and country shows. Seeing a gap in the market for colourful country clothing as an alternative to traditional styles, Tom got 100 pairs of pink wellington boots manufactured.  These sold out almost immediately.[3]
In 1994 Joule and Son rebranded as Joules. Tom was selecting and selling branded goods at minor fairs and by 1997 had done even better selling known outdoor clothing brands at the bigger shows.[4] It was around this time that Tom Joule was inspired to create his own clothing collection and in March 1999 he sold the first shirt under the Joules brand on his own stall.[3][7] In September 2000, Tom opened the first Joules store next door to the café owned by his father in Market Harborough, Leicestershire.[3] The company remains based in Market Harborough and manufacture is in China.
With the Foot-and-Mouth outbreak in 2001 almost all the shows that Tom Joule intended selling at were cancelled. He decided to take Joules clothing directly to retailers helping the brand to become more widely available.[7] The mail order catalogue business was launched in 2002,[4] with the website following in 2003. Joules launched Little Joule in 2008, designed for 2-12 year olds. Following the popularity of Little Joule, Baby Joule was established in 2009.[8] The sitting hare seen branded on most Joules clothing was first used as a logo in 2009 and later expanded to include most ranges.
Over seven years from 2003 the company saw turnover increase from £3m to £50m with mail order and internet sales accounting for a fifth of turnover by 2010.[9] By early 2011 Joules saw its high street network rise to a total of 52 stores.[10] During May 2016, Joules completed a successful stock exchange listing on the Alternative Investment Market.[11]
The Joules collection includes contemporary men's, women's and children's clothing, footwear and accessories. It began with wellies, country-inspired clothing and polo shirts and although these products still form a key part of the range, much of the collection is now fashion-focused. The Little Joule brand is aimed at the children's clothing market and was launched in 2008, followed by Baby Joule in 2009.[12] In 2019, Joules announced a partnership with Peter Rabbit for its kids range.[13]
Joules started selling home and garden products in 2013 and in 2014 launched an optical line in partnership with Vision Express.[14] In 2015 Joules launched a range of home fragrances and toiletries.
There are currently over 123 Joules stores in the UK (2018),[15] with flagship stores in Cheltenham, Norwich and York.[16]
Additional stores are in market towns and coastal holiday destinations such as St. Ives, Southwold, Burnham Market and Salcombe.
Recent years have seen Joules open travel stores in Gatwick Airport, Waterloo and St. Pancras train stations in London.[17][18]
In March 2020, Joules clothing has asked the Government to do more to support UK retail staff as the coronavirus' economic effect persists. The company – which over the last month saw its share price plunge from £1.70 to 40p – claimed it had cash in the bank to hold it over while it tries to retain the brand's ""long-term interest."" Joules has seen a decline in revenue at its 124 UK and Ireland outlets following the outbreak of COVID-19 in the UK, which has increased sharply over the last few days.[19]
Online purchases were also affected, but to a lesser degree, with shoppers becoming more vigilant about their buying habits.
The company said it will notice the downturn with its concession and wholesale trade partners too.
Joules clothing and products can also be found in various stockists in the UK and internationally. This includes:
"
Category:Clothing retailers of the United Kingdom - Wikipedia," This category has the following 4 subcategories, out of 4 total.
The following 136 pages are in this category, out of  136 total. This list may not reflect recent changes (learn more).
"
Internacionale - Wikipedia," Internacionale was a British clothing retailer with stores throughout the United Kingdom, including stores which were acquired from the former clothing chain MK One in 2008. It sold women's wear including dresses, tops, jeans, trousers, jackets and jewellery.
The company was created by Ken Cairnduff, a Scottish businessman in 1980 and grew to become known as Internacionale.[1] The business was sold in 2006 to a management buyout, which at the time had 150 stores.[2]
The company entered administration for the third time since 2008 on 28 February 2014. In September 2014, Edinburgh Woollen Mill agreed to purchase the intellectual property of Internacionale.[3]
"
Template talk:UK-retail-company-stub - Wikipedia, 
Category:NA-importance Retailing articles - Wikipedia," The following 200 pages are in this category, out of approximately 609 total. This list may not reflect recent changes (learn more).
"
Talk:List of assets owned by Walmart - Wikipedia," The official spelling/capitalization of the name of Sam's Club is in all caps. In all corporate literature it is all uppercase.
I am marking this article as {{cleanup-list}} because the bullet hierarchy is very confusing and some of the information contradicts with what is on Wal-Mart.  I was expecting this to be a list of subsidiaries and its assets by subsidiary, but I see no indicator here saying that the highest level is subsidiary.  Also, what is the name of the subsidiary that owns Walmart.com?  Is it really called Walmart.com, or is it Wal-Mart Stores Division U.S. as it says in the main article?  Tuxide 19:52, 15 August 2006 (UTC)
walmart.com is part of the ""Wal-Mart Stores USA"" division, known internally as Division 1 (all divisions have a number). It is the same division as the US Discount, Supercenter, and Neighborhood Market stores.
jblake 02:15, 19 November 2006 (UTC)
In March of 2006, WalMart increased their ownership share in the Central American Holding Co. (CARHCO) to 51%.  I've revised that statistic.  Corroboration is readily available, but see, e.g., http://www.nwaonline.com/articles/2006/03/16/business/01wmcarhco.txt.
The image Image:New Walmart Logo.svg is used in this article under a claim of fair use, but it does not have an adequate explanation for why it meets the requirements for such images when used here.  In particular, for each page the image is used on, it must have an explanation linking to that page which explains why it needs to be used on that page.  Please check
This is an automated notice by FairuseBot. For assistance on the image use policy, see Wikipedia:Media copyright questions. --03:31, 23 September 2008 (UTC)
What are total units? I assume that these are the number of stores/sites that Walmart have.
In which case the figure of 1,300,900,000 against ""Wal-Mart Stores Division U.S."" would
roughly give us one store per 5 people on the planet. So what is this number then?  —Preceding unsigned comment added by 94.193.93.109 (talk) 21:27, 15 February 2009 (UTC)
Hello fellow Wikipedians,
I have just modified 7 external links on List of assets owned by Walmart. Please take a moment to review my edit. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit this simple FaQ for additional information. I made the following changes:
When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.
As of February 2018, ""External links modified"" talk page sections are no longer generated or monitored by InternetArchiveBot. No special action is required regarding these talk page notices, other than regular verification using the archive tool instructions below. Editors have permission to delete these ""External links modified"" talk page sections if they want to de-clutter talk pages, but see the RfC before doing mass systematic removals. This message is updated dynamically through the template {{sourcecheck}} (last update: 15 July 2018).
Cheers.—InternetArchiveBot (Report bug) 09:05, 20 May 2017 (UTC)
Hello fellow Wikipedians,
I have just modified 2 external links on List of assets owned by Walmart. Please take a moment to review my edit. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit this simple FaQ for additional information. I made the following changes:
When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.
As of February 2018, ""External links modified"" talk page sections are no longer generated or monitored by InternetArchiveBot. No special action is required regarding these talk page notices, other than regular verification using the archive tool instructions below. Editors have permission to delete these ""External links modified"" talk page sections if they want to de-clutter talk pages, but see the RfC before doing mass systematic removals. This message is updated dynamically through the template {{sourcecheck}} (last update: 15 July 2018).
Cheers.—InternetArchiveBot (Report bug) 20:47, 24 May 2017 (UTC)
Hello fellow Wikipedians,
I have just modified one external link on List of assets owned by Walmart. Please take a moment to review my edit. If you have any questions, or need the bot to ignore the links, or the page altogether, please visit this simple FaQ for additional information. I made the following changes:
When you have finished reviewing my changes, you may follow the instructions on the template below to fix any issues with the URLs.
As of February 2018, ""External links modified"" talk page sections are no longer generated or monitored by InternetArchiveBot. No special action is required regarding these talk page notices, other than regular verification using the archive tool instructions below. Editors have permission to delete these ""External links modified"" talk page sections if they want to de-clutter talk pages, but see the RfC before doing mass systematic removals. This message is updated dynamically through the template {{sourcecheck}} (last update: 15 July 2018).
Cheers.—InternetArchiveBot (Report bug) 10:04, 9 September 2017 (UTC)
"
Walmart - Wikipedia," 
Walmart Inc. ( /ˈwɔːlmɑːrt/; formerly Wal-Mart Stores, Inc.) is an American multinational retail corporation that operates a chain of hypermarkets, discount department stores, and grocery stores, headquartered in Bentonville, Arkansas.[10] The company was founded by Sam Walton in 1962 and incorporated on October 31, 1969. It also owns and operates Sam's Club retail warehouses.[11][12] As of October 31, 2020,[update] Walmart has 11,510 stores and clubs in 27 countries, operating under 56 different names.[2][3][13] The company operates under the name Walmart in the United States and Canada, as Walmart de México y Centroamérica in Mexico and Central America, as Asda in the United Kingdom, as the Seiyu Group in Japan, and as Flipkart Wholesale in India. It has wholly owned operations in Argentina, Chile, Canada, and South Africa. Since August 2018, Walmart holds only a minority stake in Walmart Brasil, which was renamed Grupo Big in August 2019, with 20 percent of the company's shares, and private equity firm Advent International holding 80 percent ownership of the company.
Walmart is the world's largest company by revenue, with US$514.405 billion, according to the Fortune Global 500 list in 2019. It is also the largest private employer in the world with 2.2 million employees. It is a publicly traded family-owned business, as the company is controlled by the Walton family. Sam Walton's heirs own over 50 percent of Walmart through both their holding company Walton Enterprises and their individual holdings.[14] Walmart was the largest United States grocery retailer in 2019, and 65 percent of Walmart's US$510.329 billion sales came from U.S. operations.[4][15]
Walmart was listed on the New York Stock Exchange in 1972. By 1988, it was the most profitable retailer in the U.S.,[16] and it had become the largest in terms of revenue by October 1989.[17] The company was originally geographically limited to the South and lower Midwest, but it had stores from coast to coast by the early 1990s. Sam's Club opened in New Jersey in November 1989, and the first California outlet opened in Lancaster, in July 1990. A Walmart in York, Pennsylvania, opened in October 1990, the first main store in the Northeast.[18]
Walmart's investments outside the U.S. have seen mixed results. Its operations and subsidiaries in Canada, the United Kingdom, Central America, South America, and China are highly successful, but its ventures failed in Germany and South Korea.
In 1945, businessman and former J. C. Penney employee Sam Walton bought a branch of the Ben Franklin stores from the Butler Brothers.[19] His primary focus was selling products at low prices to get higher-volume sales at a lower profit margin, portraying it as a crusade for the consumer. He experienced setbacks because the lease price and branch purchase were unusually high, but he was able to find lower-cost suppliers than those used by other stores and was consequently able to undercut his competitors on pricing.[20] Sales increased 45 percent in his first year of ownership to US$105,000 in revenue, which increased to $140,000 the next year and $175,000 the year after that. Within the fifth year, the store was generating $250,000 in revenue. The lease then expired for the location and Walton was unable to reach an agreement for renewal, so he opened up a new store at 105 N. Main Street in Bentonville, naming it ""Walton's Five and Dime"".[20][21] That store is now the Walmart Museum.[22]
On July 2, 1962, Walton opened the first Walmart Discount City store at 719 W. Walnut Street in Rogers, Arkansas. Its design was inspired by Ann & Hope, which Walton visited in 1961, as did Kmart founder Harry B. Cunningham.[23][24] The building is now occupied by a hardware store and an antiques mall, while the company's ""Store #1"" has since expanded to a Supercenter several blocks west at 2110 W. Walnut Street. Within its first five years, the company expanded to 18 stores in Arkansas and reached $9 million in sales.[25] In 1968, it opened its first stores outside Arkansas in Sikeston, Missouri and Claremore, Oklahoma.[26]
The company was incorporated as Wal-Mart, Inc. on October 31, 1969, and changed its name to Wal-Mart Stores, Inc. in 1970. The same year, the company opened a home office and first distribution center in Bentonville, Arkansas. It had 38 stores operating with 1,500 employees and sales of $44.2 million. It began trading stock as a publicly held company on October 1, 1970, and was soon listed on the New York Stock Exchange. The first stock split occurred in May 1971 at a price of $47 per share. By this time, Walmart was operating in five states: Arkansas, Kansas, Louisiana, Missouri, and Oklahoma; it entered Tennessee in 1973 and Kentucky and Mississippi in 1974. As the company moved into Texas in 1975, there were 125 stores with 7,500 employees and total sales of $340.3 million.[26]
In the 1980s, Walmart continued to grow rapidly, and by the company's 25th anniversary in 1987, there were 1,198 stores with sales of $15.9 billion and 200,000 associates.[26]
This year also marked the completion of the company's satellite network, a $24 million investment linking all stores with two-way voice and data transmissions and one-way video communications with the Bentonville office. At the time, the company was the largest private satellite network, allowing the corporate office to track inventory and sales and to instantly communicate to stores.[27] By 1984, Sam Walton had begun to source between 6% and 40% of his company's products from China.[28] In 1988, Walton stepped down as CEO and was replaced by David Glass.[29] Walton remained as Chairman of the Board. During this year, the first Walmart Supercenter opened in Washington, MO.[30]
With the contribution of its superstores, the company surpassed Toys ""R"" Us in toy sales in 1998.[31][32]
While it was the third-largest retailer in the United States, Walmart was more profitable than rivals Kmart and Sears by the late 1980s. By 1990, it became the largest U.S. retailer by revenue.[33]
Prior to the summer of 1990, Walmart had no presence on the West Coast or in the Northeast (except for a single Sam's Club in New Jersey which opened in November 1989), but in July and October that year, it opened its first stores in California and Pennsylvania, respectively. By the mid-1990s, it was far and away the most powerful retailer in the U.S. and expanded into Mexico in 1991 and Canada in 1994.[34] Walmart stores opened throughout the rest of the U.S., with Vermont being the last state to get a store in 1995.[35]
The company also opened stores outside North America, entering South America in 1995 with stores in Argentina and Brazil; and Europe in July 1999, buying Asda in the United Kingdom for US$10 billion.[36]
In 1997, Walmart was added to the Dow Jones Industrial Average.[37]
In 1998, Walmart introduced the Neighborhood Market concept with three stores in Arkansas.[38] By 2005, estimates indicate that the company controlled about 20 percent of the retail grocery and consumables business.[39]
In 2000, H. Lee Scott became Walmart's president and CEO as the company's sales increased to $165 billion.[40] In 2002, it was listed for the first time as America's largest corporation on the Fortune 500 list, with revenues of $219.8 billion and profits of $6.7 billion. It has remained there every year except 2006, 2009, and 2012.[41][42][43][44][45][46][47][48][49][50][51]
In 2005, Walmart reported US$312.4 billion in sales, more than 6,200 facilities around the world—including 3,800 stores in the United States and 2,800 elsewhere, employing more than 1.6 million associates. Its U.S. presence grew so rapidly that only small pockets of the country remained more than 60 miles (97 kilometers) from the nearest store.[52]
As Walmart rapidly expanded into the world's largest corporation, many critics worried about its effect on local communities, particularly small towns with many ""mom and pop"" stores. There have been several studies on the economic impact of Walmart on small towns and local businesses, jobs, and taxpayers. In one, Kenneth Stone, a professor of economics at Iowa State University, found that some small towns can lose almost half of their retail trade within ten years of a Walmart store opening.[53] However, in another study, he compared the changes to what small town shops had faced in the past—including the development of the railroads, the advent of the Sears Roebuck catalog, and the arrival of shopping malls—and concluded that shop owners who adapt to changes in the retail market can thrive after Walmart arrives.[53] A later study in collaboration with Mississippi State University showed that there are ""both positive and negative impacts on existing stores in the area where the new supercenter locates.""[54]
In the aftermath of Hurricane Katrina in September 2005, Walmart used its logistics network to organize a rapid response to the disaster, donating $20 million, 1,500 truckloads of merchandise, food for 100,000 meals, and the promise of a job for every one of its displaced workers.[55] An independent study by Steven Horwitz of St. Lawrence University found that Walmart, The Home Depot, and Lowe's made use of their local knowledge about supply chains, infrastructure, decision makers and other resources to provide emergency supplies and reopen stores well before the Federal Emergency Management Agency (FEMA) began its response.[56] While the company was overall lauded for its quick response amidst criticism of FEMA, several critics were quick to point out that there still remained issues with the company's labor relations.[57]
In November 2005, Walmart announced several environmental measures to increase energy efficiency and improve its overall environmental record, which had previously been lacking.[58] The company's primary goals included spending $500 million a year to increase fuel efficiency in Walmart's truck fleet by 25 percent over three years and double it within ten; reduce greenhouse gas emissions by 20 percent in seven years; reduce energy use at stores by 30 percent; and cut solid waste from U.S. stores and Sam's Clubs by 25 percent in three years. CEO Lee Scott said that Walmart's goal was to be a ""good steward of the environment"" and ultimately use only renewable energy sources and produce zero waste.[59] The company also designed three new experimental stores with wind turbines, photovoltaic solar panels, biofuel-capable boilers, water-cooled refrigerators, and xeriscape gardens.[60] In this time, Walmart also became the biggest seller of organic milk and the biggest buyer of organic cotton in the world, while reducing packaging and energy costs.[58] In 2007, the company worked with outside consultants to discover its total environmental impact and find areas for improvement. Walmart created its own electric company in Texas, Texas Retail Energy, planned to supply its stores with cheap power purchased at wholesale prices. Through this new venture, the company expected to save $15 million annually and also to lay the groundwork and infrastructure to sell electricity to Texas consumers in the future.[61]
In 2006, Walmart announced that it would remodel its U.S. stores to help it appeal to a wider variety of demographics, including more affluent shoppers. As part of the initiative, the company launched a new store in Plano, Texas, that included high-end electronics, jewelry, expensive wines and a sushi bar.[62]
On September 12, 2007, Walmart introduced new advertising with the slogan, ""Save money. Live better."", replacing ""Always Low Prices, Always"", which it had used since 1988. Global Insight, which conducted the research that supported the ads, found that Walmart's price level reduction resulted in savings for consumers of $287 billion in 2006, which equated to $957 per person or $2,500 per household (up 7.3 percent from the 2004 savings estimate of $2,329).[63]
On June 30, 2008, Walmart removed the hyphen from its logo and replaced the star with a Spark symbol that resembles a sunburst, flower, or star. The new logo received mixed reviews from design critics who questioned whether the new logo was as bold as those of competitors, such as the Target bullseye, or as instantly recognizable as the previous company logo, which was used for 18 years.[64] The new logo made its debut on the company's website on July 1, 2008, and its U.S. locations updated store logos in the fall of 2008.[65] Walmart Canada started to adopt the logo for its stores in early 2009.[66]
On March 20, 2009, Walmart announced that it was paying a combined US$933.6 million in bonuses to every full and part-time hourly worker.[67] This was in addition to $788.8 million in profit sharing, 401(k) pension contributions, hundreds of millions of dollars in merchandise discounts, and contributions to the employees' stock purchase plan.[68] While the economy at large was in an ongoing recession, Walmart reported solid financial figures for the fiscal year ending January 31, 2009, with $401.2 billion in net sales, a gain of 7.2 percent from the prior year. Income from continuing operations increased 3 percent to $13.3 billion, and earnings per share rose 6 percent to $3.35.[citation needed]
On February 22, 2010, the company confirmed it was acquiring video streaming company Vudu, Inc. for an estimated $100 million.[69]
Walmart's truck fleet logs millions of miles each year, and the company planned to double the fleet's efficiency between 2005 and 2015.[70] The truck pictured is one of 15 based at Walmart's Buckeye, Arizona, distribution center that was converted to run on biofuel from reclaimed cooking grease made during food preparation at Walmart stores.[71]
In January 2011, Walmart announced a program to improve the nutritional value of its store brands over five years, gradually reducing the amount of salt and sugar and completely eliminating trans fat. Walmart also promised to negotiate with suppliers with respect to nutritional issues, reduce prices for whole foods and vegetables, and open stores in low-income areas, so-called ""food deserts"", where there are no supermarkets.[72] On April 23, 2011, the company announced that it was testing its new ""Walmart To Go"" home delivery system where customers will be able to order specific items offered on their website. The initial test was in San Jose, California, and the company has not yet said whether the delivery system will be rolled out nationwide.[73]
On November 14, 2012, Walmart launched its first mail subscription service called Goodies. Customers pay a $7 monthly subscription for five to eight delivered food samples each month, so they can try new foods.[74] The service shut down in late 2013.[75]
In August 2013, the firm announced it was in talks to acquire a majority stake in the Kenya-based supermarket chain, Naivas.[76]
In June 2014, some Walmart employees went on strike in major U.S. cities demanding higher wages.[77] In July 2014, American actor and comedian Tracy Morgan launched a lawsuit against Walmart seeking punitive damages over a multi-car pile-up which the suit alleges was caused by the driver of one of the firm's tractor-trailers who had not slept for 24 hours. Morgan's limousine was apparently hit by the trailer, injuring him and two fellow passengers and killing a fourth, fellow comedian James McNair.[78] Walmart settled with the McNair family for $10 million, while admitting no liability.[79] Morgan and Walmart reached a settlement in 2015 for an undisclosed amount,[80] though Walmart later accused its insurers of ""bad faith"" in refusing to pay the settlement.[81]
In 2015, the company closed five stores on short notice for plumbing repairs.[82] However, employees and the United Food and Commercial Workers International Union (UFCW) alleged some stores were closed in retaliation for strikes aimed at increasing wages and improving working conditions.[83] The UFCW filed a complaint with the National Labor Relations Board. All five stores have since reopened.[84]
In 2015, Walmart was the biggest US commercial producer of solar power with 142 MW capacity, and had 17 energy storage projects.[85][86] This solar was primarily on rooftops, whereas there is an additional 20,000 m2 for solar canopies over parking lots.[87]
On January 15, 2016, Walmart announced it would close 269 stores in 2016, affecting 16,000 workers.[91] One hundred and fifty-four of these stores earmarked for closure were in the U.S. (150 Walmart U.S. stores, 115 Walmart International stores, and 4 Sam's Clubs). Ninety-five percent of these U.S. stores were located, on average, 10 miles from another Walmart store.[92] The 269 stores represented less than 1 percent of global square footage and revenue for the company. All 102 locations of Walmart Express, which had been in a pilot program since 2011, were included in the closures. Walmart planned to focus on ""strengthening Supercenters, optimizing Neighborhood Markets, growing the e-commerce business and expanding pickup services for customers"". In fiscal 2017, the company plans to open between 50 and 60 Supercenters, 85 to 95 Neighborhood Markets, 7 to 10 Sam's Clubs, and 200 to 240 international locations.[92] At the end of fiscal 2017, Walmart opened 38 Supercenters and relocated, expanded or converted 21 discount stores into Supercenters, for a total of 59 Supercenters, and opened 69 Neighborhood Markets, 8 Sam's Clubs, and 173 international locations, and relocated, expanded or converted 4 locations for a total of 177 international locations. On August 8, 2016, Walmart announced a deal to acquire e-commerce website Jet.com for US$3.3 billion.[93][94] Jet.com co-founder and CEO Marc Lore stayed on to run Jet.com in addition to Walmart's existing U.S. e-commerce operation. The acquisition was structured as a payout of $3 billion in cash, and an additional $300 million in Walmart stock vested over time as part of an incentive bonus plan for Jet.com executives.[95] On October 19, 2016, Walmart announced it would partner with IBM and Tsinghua University to track the pork supply chain in China using blockchain.[96]
On February 15, 2017, Walmart announced the acquisition of Moosejaw, a leading online active outdoor retailer, for approximately $51 million. The acquisition closed on February 13, 2017.[97] On June 16, 2017, Walmart agreed to acquire the men's apparel company Bonobos for $310 million in an effort to expand its fashion holdings.[98] On September 29, 2017, Walmart acquired Parcel, a technology-based, same-day and last-mile delivery company in Brooklyn.[99] The acquisition announcement saw Walmart shares rise more than 1%.[100] On December 6, 2017, Walmart announced that it will change its corporate name to Walmart Inc. from Wal-Mart Stores, Inc. effective February 1, 2018.[101][102]
On January 11, 2018, Walmart announced that 63 Sam's Club locations in cities including Memphis, Houston, Seattle, and others would be closing. Some of the stores had already liquidated, without notifying employees; some employees learned by a company-wide email delivered January 11. All of the 63 stores were gone from the Sam's Club website as of the morning of January 11. Walmart said that ten of the stores will become e-commerce distribution centers and employees can reapply to work at those locations. Business Insider magazine calculated that over 11,000 workers will be affected.[103][104] On the same day, Walmart announced that as a result of the new tax law, it would be raising Walmart starting wages, distributing bonuses, expanding its leave policies and contributing toward the cost of employees' adoptions. Doug McMillon, Walmart's CEO, said, ""We are early in the stages of assessing the opportunities tax reform creates for us to invest in our customers and associates and to further strengthen our business, all of which should benefit our shareholders.""[105]
In March 2018, Walmart announced that it is producing its own brand of meal kits in all of its stores that is priced under Blue Apron designed to serve two people.[106]
It was reported that Walmart is now looking at entering the subscription-video space, hoping to compete with Netflix and Amazon. They have enlisted the help of former Epix CEO, Mark Greenberg, to help develop a low-cost subscription video-streaming service.[107]
In September 2018, Walmart partnered with comedian and talk show host Ellen DeGeneres to launch a new brand of women's apparel and accessories called EV1.[108]
On February 26, 2019, Walmart announced that it had acquired Tel Aviv-based product review start-up Aspectiva for an undisclosed sum.[109]
In May 2019, Walmart announced the launch of free one-day shipping on more than 220,000 items with minimum purchase amount of $35.[110] The initiative first launched in Las Vegas and the Phoenix area.[111]
In September 2019, Walmart made the announcement that it would cease the sale of all e-cigarettes due to ""regulatory complexity and uncertainty"" over the products. Earlier in 2019, Walmart stopped selling fruit-flavored e-cigarette and had raised the minimum age to 21 for the purchase of products containing tobacco.[112] That same month, Walmart opened its first Health Center, a ""medical mall"" where customers can purchase primary care services, such as: vision tests, dental exams and root canals, lab work, X-rays and EKGs, counseling, and fitness and diet classes. Prices without insurance were listed, for instance, at $30 for an annual physical and $45 for a counseling session.[113] Continuing with its health care initiative, they opened a 2,600 sq. ft. health and wellness clinic prototype in Springdale, Arkansas just to expand services.[114]
As of October 2019, Walmart stopped selling all live fish and aquatic plants.[115]
This decade, as with many other companies, started off very unorthodox and unusual, due to the large part to the coronavirus (COVID-19) pandemic, including store closures, limited store occupancy and employment, along with social distancing protocols.
In March 2020, due to the pandemic, Walmart changed some of its employee benefits. Employees can now decide to stay home and take unpaid leave if they feel unable to work or uncomfortable coming to work. Additionally, Walmart employees who contract the virus will receive ""up to two weeks of pay"". After two weeks, hourly associates who are unable to return to work are eligible for up to 26 weeks in pay.[116] During this pandemic, people who work temporary receive $150 but for those who work full-time get a bonus of $300 issuing all of the employees more than $390M starting on June 5.[117] Previously during the pandemic on April 2, the bonus cash totaling was more than $365. In July 2020, Walmart announced that all customers would be required to wear masks in all stores nationwide, including Sam's Club.[118] In the third quarter of 2020, ending October 31, Walmart reported revenue of $134.7 billion, representing a year-on-year increase of 5.2 percent.[119]
In December 2020, Walmart launched a new service, Carrier Pickup, that allows the customers to schedule a return for a product bought online, in-store or from a third-party vendor. This services can be initiated on the Walmart App or on the website.[120]
In January 2021, Walmart announced that the company is launching a fintech startup, with venture partner Ribbit Capital, to provide financial products for consumers and employees.[121]
Walmart's operations are organized into four divisions: Walmart U.S., Walmart International, Sam's Club and Global eCommerce.[122] The company offers various retail formats throughout these divisions, including supercenters, supermarkets, hypermarkets, warehouse clubs, cash-and-carry stores, home improvement, specialty electronics, restaurants, apparel stores, drugstores, convenience stores, and digital retail.[123]
Walmart U.S. is the company's largest division, accounting for US$331.666 billion, or 65 percent of total sales, for fiscal 2019.[4][15] It consists of three retail formats that have become commonplace in the United States: Supercenters, Discount Stores, Neighborhood Markets, and other small formats. The discount stores sell a variety of mostly non-grocery products, though emphasis has now shifted towards supercenters, which include more groceries. As of October 31, 2019,[update] there are a total of 4,748 Walmart U.S. stores.[2][3] In the United States, 90 percent of the population resides within 10 miles of a Walmart store.[124] The total number of Walmart U.S. and Sam's Club combined is 5,347.[2][3]
The president and CEO of Walmart U.S. is John Furner.[125]
Walmart Supercenters, branded simply as ""Walmart"", are hypermarkets with sizes varying from 69,000 to 260,000 square feet (6,400 to 24,200 square meters), but averaging about 178,000 square feet (16,500 square meters).[13] These stock general merchandise and a full-service supermarket, including meat and poultry, baked goods, delicatessen, frozen foods, dairy products, garden produce, and fresh seafood. Many Walmart Supercenters also have a garden center, pet shop, pharmacy, Tire & Lube Express, optical center, one-hour photo processing lab, portrait studio, and numerous alcove shops, such as cellular phone stores, hair and nail salons, video rental stores, local bank branches (such as Woodforest National Bank branches in newer locations), and fast food outlets.
Many Walmart Supercenters have featured McDonald's restaurants, but in 2007, Walmart announced it would stop opening McDonald's restaurants at most of their newer stores, most likely due to nutritional concerns. Most locations that opened up after the announcement had Subway as their restaurants, and some McDonald's inside the stores were replaced with Subways.[126] In some Canadian locations, Tim Hortons were opened. Recently, in several Supercenters, like the Tallahassee, Florida, location, Walmart added Burger King to their locations, and the location in Glen Burnie, Maryland, due to its past as a hypermarket called Leedmark, which operated from May 1991 to January 1994, boasts an Auntie Anne's and an Italian restaurant
Some locations also have fuel stations which sell gasoline distributed by Murphy USA (which spun off from Murphy Oil in 2013), Sunoco, Inc. (""Optima""), the Tesoro Corporation (""Mirastar""), USA Gasoline, and even now Walmart-branded gas stations.[127]
The first Supercenter opened in Washington, Missouri, in 1988. A similar concept, Hypermart USA, had opened a year earlier in Garland, Texas. All Hypermart USA stores were later closed or converted into Supercenters.
As of October 31, 2020,[update] there were 3,570 Walmart Supercenters in 49 of the 50 U.S. states, the District of Columbia, and Puerto Rico.[2][3] Hawaii is the only state to not have a Supercenter location. The largest Supercenter in the world, covering 260,000 square feet (24,000 square meters) on two floors, is located in Crossgates Commons in Albany, New York.[128]
A typical supercenter sells approximately 120,000 items, compared to the 35 million products sold in Walmart's online store.[129]
The ""Supercenter"" name has since been phased out, with these stores now simply referred to as ""Walmart"", since the company introduced the new Walmart logo in 2008. However, the branding is still used in Walmart's Canadian stores (spelled as ""Supercentre"" in Canadian English).[130]
Walmart Discount Stores, also branded as simply ""Walmart"", are discount department stores with sizes varying from 30,000 to 206,000 square feet (2,800 to 19,100 square meters), with the average store covering 105,000 square feet (9,800 square meters).[13] They carry general merchandise and limited groceries. Some newer and remodeled discount stores have an expanded grocery department, similar to Target's PFresh department. Many of these stores also feature a garden center, pharmacy, Tire & Lube Express, optical center, one-hour photo processing lab, portrait studio, a bank branch, a cell phone store, and a fast food outlet. Some also have gasoline stations.[127] Discount Stores were Walmart's original concept, though they have since been surpassed by Supercenters.
In 1990, Walmart opened its first Bud's Discount City location in Bentonville. Bud's operated as a closeout store, much like Big Lots. Many locations were opened to fulfill leases in shopping centers as Walmart stores left and moved into newly built Supercenters. All of the Bud's Discount City stores had closed or converted into Walmart Discount Stores by 1997.[131]
At its peak in 1996, there were 1,995 Walmart Discount Stores,[132] but as of October 31, 2020, that number was dropped to 374.[2][3]
Walmart Neighborhood Market, sometimes branded as ""Neighborhood Market by Walmart"" or informally known as ""Neighborhood Walmart"", is Walmart's chain of smaller grocery stores ranging from 28,000 to 65,000 square feet (2,600 to 6,000 square meters) and averaging about 42,000 square feet (3,900 square metres), about a fifth of the size of a Walmart Supercenter.[13][133] The first Walmart Neighborhood Market opened ten years after the first Supercenter opened, yet Walmart renewed its focus on the smaller grocery store format in the 2010s.[134]
The stores focus on three of Walmart's major sales categories: groceries, which account for about 55 percent of the company's revenue,[135][136] pharmacy, and, at some stores, fuel.[137] For groceries and consumables, the stores sell fresh produce, deli and bakery items, prepared foods, meat, dairy, organic, general grocery and frozen foods, in addition to cleaning products and pet supplies.[133][138] Some stores offer wine and beer sales[133] and drive-through pharmacies.[139] Some stores, such as one at Midtown Center in Bentonville, Arkansas, offer made-to-order pizza with a seating area for eating.[139] Customers can also use Walmart's site-to-store operation and pick up online orders at Walmart Neighborhood Market stores.[140]
Products at Walmart Neighborhood Market stores carry the same prices as those at Walmart's larger supercenters. A Moody's analyst said the wider company's pricing structure gives the chain of grocery stores a ""competitive advantage"" over competitors Whole Foods, Kroger and Trader Joe's.[137] Walmart contributed around 14.5% of the entire quantity of food purchased in the USA during 2016. This figure was more than double the contribution from their closest competitor Kroger.[141]
Neighborhood Market stores expanded slowly at first as a way to fill gaps between Walmart Supercenters and Discount Stores in existing markets.[142] In its first 12 years, the company opened about 180 Walmart Neighborhood Markets.[142] By 2010, Walmart said it was ready to accelerate its expansion plans for the grocery stores.[142] As of October 31, 2020,[update] there were 686 Walmart Neighborhood Markets,[2][3] each employing between 90 and 95 full-time and part-time workers.[143] There are also currently 12 Amigo supermarkets in Puerto Rico. The total number of Neighborhood Markets and Amigo combined is 698, while the total number of the former two and other small formats combined is 804.
Walmart opened Supermercado de Walmart locations to appeal to Hispanic communities in the United States.[144] The first one, a 39,000-square-foot (3,600-square-meter) store in the Spring Branch area of Houston, opened on April 29, 2009.[145] The store was a conversion of an existing Walmart Neighborhood Market.[146] In 2009, another Supermercado de Walmart opened in Phoenix, Arizona.[147] Both locations closed in 2014.[148] In 2009, Walmart opened ""Mas Club"", a warehouse retail operation patterned after Sam's Club. Its lone store closed in 2014.[145]
Walmart Express was a chain of smaller discount stores with a range of services from groceries to check cashing and gasoline service. The concept was focused on small towns deemed unable to support a larger store, and large cities where space was at a premium. Walmart planned to build 15 to 20 Walmart Express stores, focusing on Arkansas, North Carolina and Chicago, by the end of its fiscal year in January 2012. As of September 2014,[update] Walmart re-branded all of its Express format stores to Neighborhood Markets in an effort to streamline its retail offer. It continued to open new Express stores under the Neighborhood Market name. As of October 31, 2020,[update] there were 106 small-format stores in the United States. These include 95 E-Commerce Acquisition / C-stores, 8 convenience stores and 3 other store formats.[2][3] On January 15, 2016, Walmart announced that it would be closing 269 stores globally, including all 102 U.S. Walmart Express stores, including those branded as Neighborhood Markets.[149]
In September 2006, Walmart announced a pilot program to sell generic drugs at $4 per prescription. The program was launched at stores in the Tampa, Florida, area, and by January 2007 had been expanded to all stores in Florida. While the average price of generics is $29 per prescription, compared to $102 for name-brand drugs, Walmart maintains that it is not selling at a loss, or providing them as an act of charity—instead, they are using the same mechanisms of mass distribution that it uses to bring lower prices to other products.[150] Many of Walmart's low cost generics are imported from India, where they are made by drug makers that include Ranbaxy and Cipla.[151]
On February 6, 2007, the company launched a ""beta"" version of a movie download service, which sold about 3,000 films and television episodes from all major studios and television networks.[152] The service was discontinued on December 21, 2007, due to low sales.[153]
In 2008, Walmart started a pilot program in the small grocery store concept called Marketside in the metropolitan Phoenix, Arizona, area. The four stores closed in 2011.[154]
In 2015, Walmart began testing a free grocery pickup service, allowing customers to select products online and choose their pickup time. At the store, a Walmart employee loads the groceries into the customer's car. As of December 17, 2017,[update] the service is available in 39 U.S. states.[155]
In May 2016, Walmart announced a change to ShippingPass, its three-day shipping service, and that it will move from a three-day delivery to two-day delivery to remain competitive with Amazon.[156] Walmart priced it at 49 dollars per year, compared to Amazon Prime's 99-dollar-per-year price.[157][158]
In June 2016, Walmart and Sam's Club announced that they would begin testing a last-mile grocery delivery that used services including Uber, Lyft, and Deliv, to bring customers' orders to their homes. Walmart customers would be able to shop using the company's online grocery service at grocery.walmart.com, then request delivery at checkout for a small fee. The first tests were planned to go live in Denver and Phoenix.[159] Walmart announced on March 14, 2018, that it would expand online delivery to 100 metropolitan regions in the United States, the equivalent of 40 percent of households, by the end of the year of 2018.[160]
Walmart's Winemakers Selection private label wine was introduced in June 2018 in about 1,100 stores. The wine, from domestic and international sources, was described by Washington Post food and wine columnist Dave McIntyre as notably good for the inexpensive ($11 to $16 per bottle) price level.[161]
In October 2019, Walmart announced that customers in 2,000 locations in 29 states can use the grocery pickup service for their adult beverage purchases. Walmart will also deliver adult beverages from nearly 200 stores across California and Florida.[162]
In February 2020, Walmart announced a new membership program called, ""Walmart +"". The news came shortly after Walmart cut ties with its personal shopping service, Jetblack.[163][164]
Locations as of November 2, 2020
As of October 31, 2020,[update] Walmart's international operations comprised 6,163 stores[2][3] and 800,000 workers in 26 countries outside the United States.[217] There are wholly owned operations in Argentina, Brazil, Canada, and the UK. With 2.2 million employees worldwide, the company is the largest private employer in the U.S. and Mexico, and one of the largest in Canada.[8] In fiscal 2019 Walmart's international division sales were US$120.824 billion, or 23.7 percent of total sales.[4][15] International retail units range from 1,400 to 186,000 square feet (130 to 17,280 square metres), while wholesale units range from 25,000 to 156,000 square feet (2,300 to 14,500 square metres).[13] Judith McKenna is the president and CEO.[218]
Walmart Argentina was founded in 1995 and, as of October 31, 2020,[update] operates 92 stores under the banners Walmart Supercenter (29 locations), Changomas (53 locations), Mi Changomas (8 locations), and Punto Mayorista (2 locations).[2][3] On November 6, 2020, it was announced that Walmart has sold its Argentine operations to Grupo de Narváez.[219]
Walmart also owns 51 percent of the Central American Retail Holding Company (CARHCO), which, as of July 31, 2020,[update] consists of 853 stores, including 263 stores in Guatemala (under the Paiz [27 locations], Walmart Supercenter [10 locations], Despensa Familiar [181 locations], and Maxi Dispensa [45 locations] banners),[2][3] 101 stores in El Salvador (under the Despensa Familiar [63 locations], La Despensa de Don Juan [17 locations], Walmart Supercenter [6 locations], and Maxi Despensa [15 locations] banners),[2][3] 110 stores in Honduras (including the Paiz [8 locations], Walmart Supercenter [3 locations], Dispensa Familiar [71 locations], and Maxi Despensa [28 locations] banners),[2][3] 102 stores in Nicaragua (including the Pali [71 locations], La Unión [9 locations], Maxi Pali [20 locations], and Walmart Supercenter [2 locations] banners),[2][3] and 277 stores in Costa Rica (including the Maxi Pali [48 locations], Mas X Menos [39 locations], Walmart Supercenter [14 locations], and Pali [176 locations] banners[2][3]).[220]
In January 2009, the company acquired a controlling interest in the largest grocer in Chile, Distribución y Servicio D&S SA.[221][222] In 2010, the company was renamed Walmart Chile.[223] As of October 31, 2020,[update] Walmart Chile operates 364 stores under the banners Lider Hiper (89 locations), Lider Express (153 locations), Superbodega Acuenta (112 locations), Ekono (4 locations), and Central Mayorista (6 locations).[2][3]
As of October 31, 2020,[update] Walmart's Mexico division, the largest outside the U.S., consisted of 2,599 stores.[2][3] Walmart in Mexico operates Walmart Supercenter (282 locations), Sam's Club (164 locations), Bodega Aurrera (548 locations), Mi Bodega Aurrera (399 locations), Bodega Aurrera Express (1,113 locations), and Superama (93 locations).[3]
Walmart has operated in Canada since it acquired 122 stores comprising the Woolco division of Woolworth Canada, Inc on January 14, 1994.[224] As of October 31, 2020,[update] it operates 408 locations (including 343 supercentres and 65 discount stores)[2][3] and, as of June 2015,[update] it employs 89,358 people, with a local home office in Mississauga, Ontario.[225] Walmart Canada's first three Supercentres (spelled in Canadian English) opened in November 2006 in Ancaster, London, and Stouffville, Ontario.[226] The 100th Canadian Supercentre opened in July 2010, in Victoria, British Columbia.
In 2010, approximately one year after its incorporation of Schedule 2 (foreign-owned, deposit-taking) of Canada's Bank Act,[227] Walmart Canada Bank was introduced with the launch of the Walmart (Canada) Rewards MasterCard.[228] Less than ten years later, however, on May 17, 2018, Wal-Mart Canada announced it had reached a definitive agreement to sell Wal-Mart Canada Bank to First National co-founder Stephen Smith and private equity firm Centerbridge Partners, L.P., on undisclosed financial terms, though it added that it would still be issuer of the Walmart (Canada) Rewards MasterCard.[229]
On April 1, 2019, Centerbridge Partners, L.P. and Stephen Smith jointly announced the closing of the previously announced acquisition of Wal-Mart Canada Bank and that it was to be renamed Duo Bank of Canada, to be styled simply as Duo Bank.[230][231] Though exact ownership percentages were never revealed in either company announcement, it has also since been revealed that Duo Bank was reclassified as a Schedule 1 (domestic, deposit-taking)[232][233] federally chartered bank of the Bank Act in Canada from the Schedule 2 (foreign-owned or -controlled, deposit-taking)[233] that it had been, which indicates that Stephen Smith, as a noted Canadian businessman, is in a controlling position.
Walmart's UK subsidiary Asda (which retained its name after being acquired by Walmart) is based in Leeds and accounted for 42.7 percent of 2006 sales of Walmart's international division. In contrast to the U.S. operations, Asda was originally and still remains primarily a grocery chain, but with a stronger focus on non-food items than most UK supermarket chains other than Tesco. As of October 31, 2020,[update] Asda had 631 stores,[2][3] including 147 from the 2010 acquisition of Netto UK. In addition to small suburban Asda Supermarkets at 207 locations,[3] larger stores are branded Supercentres; there are 32 of these.[3] Other banners include Asda Superstores (341 locations), Asda Living (33 locations), and Asda Petrol Fueling Station (18 locations).[2][3][234] In July 2015, Asda updated its logo featuring the Walmart Asterisks behind the first 'A' in the Logo. In May 2018, Walmart announced plans to sell Asda to rival Sainsbury's for $10.1 billion. Under the terms of the deal, Walmart would have received a 42% stake in the combined company and about £3 billion in cash.[235] However, in April 2019, the United Kingdom's Competition and Markets Authority blocked the proposed sale of Asda to Sainsburys.[236]
On October 2, 2020, it was announced Walmart sold the majority of Asda to a consortium of Zuber and Mohsin Issa (the owners of EG Group) and private equity firm TDR Capital for £6.8bn.[237]
On September 28, 2010, Walmart announced it would buy Massmart Holdings Ltd. of Johannesburg, South Africa in a deal worth over US$4 billion giving the company its first footprint in Africa.[238] As of October 31, 2020,[update] it has 422 stores, including 372 stores in South Africa (under the banners Game Foodco [79 locations], CBW [43 locations], Game [43 locations], Builders Express [49 locations], Builders Warehouse [33 locations], Cambridge [45 locations], Rhino [18 locations], Makro [22 locations], Builders Trade Depot [9 locations], Jumbo [13 locations], and Builders Superstore [18 locations]),[2][3] 11 stores in Botswana (under the banners CBW [7 locations], Game Foodco [2 locations], and Builders Warehouse [2 locations]),[2][3] 4 stores in Ghana (under the Game Foodco banner),[2][3] 4 stores in Kenya (under the banners Game Foodco [3 locations] and Builders Warehouse [1 location]),[2][3] 3 stores in Lesotho (under the banners CBW [2 locations] and Game Foodco [1 location]),[2] 2 stores in Malawi (under the Game banner),[2][3] 6 stores in Mozambique (under the banners Builders Warehouse [2 locations], Game Foodco [2 locations], CBW [1 location], and Builders Express [1 location]),[2][3] 5 stores in Namibia (under the banners Game Foodco [4 locations] and Game [1 location]),[2][3] 5 stores in Nigeria (under the banners Game [3 locations] and Game Foodco [2 location]),[2][3] 1 store in Swaziland (under the CBW banner),[2][3] 1 store in Tanzania (under the Game banner),[2][3] 1 store in Uganda (under the Game banner),[2][3] and 7 stores in Zambia (under the banners CBW [1 location], Game [3 locations], Builders Warehouse [2 locations], and Builders Express [1 location]).[2][3]

Walmart has joint ventures in China and several majority-owned subsidiaries. As of October 31, 2020,[update] Walmart China (沃尔玛 Wò'ērmǎ)[239] operates 435 stores under the Walmart Supercenter (398 locations), Sam's Club (28 locations) and Neighborhood Market (9 locations) banners.[2][3]
In February 2012, Walmart announced that the company raised its stake to 51 percent in Chinese online supermarket Yihaodian to tap rising consumer wealth and help the company offer more products. Walmart took full ownership in July 2015.[240]
In Japan, Walmart owns 100 percent of Seiyu (西友 Seiyū) as of 2008.[update][241][242] As of October 31, 2020, there are 331 stores under the Seiyu (Hypermarket) (87 locations), Seiyu (Supermarket) (174 locations), Seiyu (General Merchandise) (1 location), Livin (6 locations), and Sunny (63 locations) banners.[2][3] 
On November 16, 2020, Walmart announced they would be selling 65% of their shares in the company to private-equity firm KKR in a deal valuing 329 stores and 34,600 employees at $1.6 billion. Walmart is supposed to retain 15% and a seat on the board, while Japanese company Rakuten Inc. will receive 20%.[243]
In November 2006, the company announced a joint venture with Bharti Enterprises to operate in India. As foreign corporations were not allowed to enter the retail sector directly, Walmart operated through franchises and handled the wholesale end of the business.[244] The partnership involved two joint ventures—Bharti manages the front end, involving opening of retail outlets while Walmart takes care of the back end, such as cold chains and logistics. Walmart operates stores in India under the name Best Price Modern Wholesale.[245] The first store opened in Amritsar on May 30, 2009. On September 14, 2012, the Government of India approved 51 percent FDI in multi-brand retails, subject to approval by individual states, effective September 20, 2012.[246][247] Scott Price, Walmart's president and CEO for Asia, told The Wall Street Journal that the company would be able to start opening Walmart stores in India within two years.[248] Expansion into India faced some significant problems. In November 2012, Walmart admitted to spending US$25 million lobbying the Indian National Congress;[249] lobbying is conventionally considered bribery in India.[250] Walmart is conducting an internal investigation into potential violations of the Foreign Corrupt Practices Act.[251] Bharti Walmart suspended a number of employees, rumored to include its CFO and legal team, to ensure ""a complete and thorough investigation"".[252] As of October 31, 2020,[update] there are 28 Best Price locations.[2][3] In October 2013, Bharti and Walmart separated to pursue business independently.[253]
On May 9, 2018, Walmart announced its intent to acquire a 77% majority stake in the Indian e-commerce company Flipkart for $16 billion, in a deal that was completed on August 18, 2018.[254][255][256]
In the mid-1990s, Walmart tried with a large financial investment to get a foothold in the German retail market. In 1997, Walmart took over the supermarket chain Wertkauf with its 21 stores for DM 750 million[257] and the following year Walmart acquired 74 Interspar stores for DM 1.3 billion.[258][259] The German market at this point was an oligopoly with high competition among companies which used a similar low price strategy as Walmart. As a result, Walmart's low price strategy yielded no competitive advantage. Walmart's corporate culture was not viewed positively among employees and customers, particularly Walmart's ""statement of ethics"", which attempted to restrict relationships between employees, a possible violation of German labor law, and led to a public discussion in the media, resulting in a bad reputation among customers.[260][261] In July 2006, Walmart announced its withdrawal from Germany due to sustained losses. The stores were sold to the German company Metro during Walmart's fiscal third quarter.[241][262] Walmart did not disclose its losses from its German investment, but they were estimated to be around €3 billion.[263]
In 2004, Walmart bought the 118 stores in the Bompreço supermarket chain in northeastern Brazil. In late 2005, it took control of the Brazilian operations of Sonae Distribution Group through its new subsidiary, WMS Supermercados do Brasil, thus acquiring control of the Nacional and Mercadorama supermarket chains, the leaders in the Rio Grande do Sul and Paraná states, respectively. None of these stores were rebranded. As of January 2014,[update] Walmart operated 61 Bompreço supermarkets, 39 Hiper Bompreço stores. It also ran 57 Walmart Supercenters, 27 Sam's Clubs, and 174 Todo Dia stores. With the acquisition of Bompreço and Sonae, by 2010, Walmart was the third-largest supermarket chain in Brazil, behind Carrefour and Pão de Açúcar.[264]
Walmart Brasil, the operating company, has its head office in Barueri, São Paulo State, and regional offices in Curitiba, Paraná; Porto Alegre, Rio Grande do Sul; Recife, Pernambuco; and Salvador, Bahia.[265] Walmart Brasil operates under the banners Todo Dia, Nacional, Bompreço, Walmart Supercenter, Maxxi Atacado, Hipermercado Big, Hiper Bompreço, Sam's Club, Mercadorama, Walmart Posto (Gas Station), Supermercado Todo Dia, and Hiper Todo Dia.[2][3] Recently, the company started the conversion process of all Hiper Bompreço and Big stores into Walmart Supercenters and Bompreço, Nacional and Mercadorama stores into the Walmart Supermercado brand.
Since August 2018, Walmart Inc. only holds a minority stake in Walmart Brasil, which was renamed Grupo Big on August 12, 2019,[266] with 20% of the company's shares, and private equity firm Advent International holding 80% ownership of the company.[267]
An April 2012 investigation by The New York Times reported the allegations of a former executive of Walmart de Mexico that, in September 2005, the company had paid bribes via local fixers to officials throughout Mexico in exchange for construction permits, information, and other favors, which gave Walmart a substantial advantage over competitors.[268] Walmart investigators found credible evidence that Mexican and American laws had been broken. Concerns were also raised that Walmart executives in the United States had ""hushed up"" the allegations. A follow-up investigation by The New York Times, published December 17, 2012, revealed evidence that regulatory permission for siting, construction, and operation of nineteen stores had been obtained through bribery. There was evidence that a bribe of US$52,000 was paid to change a zoning map, which enabled the opening of a Walmart store a mile from a historical site in San Juan Teotihuacán in 2004.[269] After the initial article was released, Walmart released a statement denying the allegations and describing its anti-corruption policy. While an official Walmart report states that it had found no evidence of corruption, the article alleges that previous internal reports had indeed turned up such evidence before the story became public.[270] Forbes magazine contributor Adam Hartung also commented that the bribery scandal was a reflection of Walmart's ""serious management and strategy troubles"", stating, ""[s]candals are now commonplace ... [e]ach scandal points out that Walmart's strategy is harder to navigate and is running into big problems"".[271]
In 2012, there was an incident with CJ's Seafood, a crawfish processing firm in Louisiana that was partnered with Walmart, that eventually gained media attention for the mistreatment of its 40 H-2B visa workers from Mexico. These workers experienced harsh living conditions in tightly packed trailers outside of the work facility, physical threats, verbal abuse and were forced to work day-long shifts. Many of the workers were afraid to take action about the abuse due to the fact that the manager threatened the lives of their family members in the U.S. and Mexico if the abuse were to be reported. Eight of the workers confronted management at CJ's Seafood about the mistreatment; however, the management denied the abuse allegations and the workers went on strike. The workers then took their stories to Walmart due to their partnership with CJ's. While Walmart was investigating the situation, the workers collected 150,000 signatures of supporters who agreed that Walmart should stand by the workers and take action. In June 2012, the visa workers held a protest and day-long hunger strike outside of the apartment building where a Walmart board member resided. Following this protest, Walmart announced its final decision to no longer work with CJ's Seafood. Less than a month later, the Department of Labor fined CJ's Seafood ""approximately $460,000 in back-pay, safety violations, wage and hour violations, civil damages and fines for abuses to the H-2B program. The company has since shut down.""[272]
As of December 2012,[update] internal investigations were ongoing into possible violations of the Foreign Corrupt Practices Act.[273] Walmart has invested US$99 million on internal investigations, which expanded beyond Mexico to implicate operations in China, Brazil, and India.[274][275] The case has added fuel to the debate as to whether foreign investment will result in increased prosperity, or if it merely allows local retail trade and economic policy to be taken over by ""foreign financial and corporate interests"".[276][277]
Sam's Club is a chain of warehouse clubs that sell groceries and general merchandise, often in bulk. Locations generally range in size from 32,000–168,000 sq ft (3,000–15,600 m2), with an average club size of approximately 134,000 sq ft (12,400 m2).[13] The first Sam's Club was opened by Walmart, Inc. in 1983 in Midwest City, Oklahoma[278] under the name ""Sam's Wholesale Club"". The chain was named after its founder Sam Walton. As of October 31, 2020, Sam's Club operated 599 membership warehouse clubs and accounted for 11.3% of Walmart's revenue at $57.839 billion in fiscal year 2019.[4][279] Kathryn McLay is the president and CEO.[218][280]
Based in San Bruno, California, Walmart's Global eCommerce division provides online retailing for Walmart, Sam's Club, Asda, and all other international brands. There are several locations in the United States in California and Oregon: San Bruno, Sunnyvale, Brisbane, and Portland. Locations outside of the United States include Shanghai (China), Leeds (United Kingdom), and Bangalore (India). Marc Lore is the president and CEO.[218]
About 40 percent of products sold in Walmart are private label store brands, which are produced for the company through contracts with manufacturers. Walmart began offering private label brands in 1991, with the launch of Sam's Choice, a line of drinks produced by Cott Beverages for Walmart. Sam's Choice quickly became popular and by 1993, was the third-most-popular beverage brand in the United States.[281] Other Walmart brands include Great Value and Equate in the U.S. and Canada and Smart Price in Britain. A 2006 study talked of ""the magnitude of mind-share Walmart appears to hold in the shoppers' minds when it comes to the awareness of private label brands and retailers.""[282]
In 2010, the company teamed with Procter & Gamble to produce Secrets of the Mountain and The Jensen Project, two-hour family movies which featured the characters using Walmart and Procter & Gamble-branded products. The Jensen Project also featured a preview of a product to be released in several months in Walmart stores.[283][284] A third movie, A Walk in My Shoes, also aired in 2010 and a fourth is in production.[when?][285] Walmart's director of brand marketing also serves as co-chair of the Association of National Advertisers's Alliance for Family Entertainment.[286]
In September 2016, Walmart purchased e-commerce company Jet.com, founded in 2014 by Marc Lore, to start competing with Amazon.com. Jet.com has acquired its own share of online retailers such as Hayneedle in March 2016, Shoebuy.com in December 2016, and ModCloth in March 2017. Walmart also acquired Parcel, a delivery service in New York, on September 29, 2017.[287][288]
On February 15, 2017, Walmart acquired Moosejaw, an online active outdoor retailer, for approximately $51 million. Moosejaw brought with it partnerships with more than 400 brands, including Patagonia, The North Face, Marmot, and Arc'teryx.[289]
Marc Lore, Walmart's U.S. e-commerce CEO, said that Walmart's existing physical infrastructure of almost 5,000 stores around the U.S. will enhance their digital expansion by doubling as warehouses for e-commerce without increasing overhead.[290] As of 2017,[update] Walmart offers in-store pickup for online orders at 1,000 stores with plans to eventually expand the service to all of its stores.[291]
On May 9, 2018, Walmart announced its intent to acquire a 77% controlling stake in the Indian e-commerce website Flipkart for $16 billion[292] (beating bids by Amazon.com), subject to regulatory approval. Following its completion, the website's management will report to Marc Lore.[293][294][295] Completion of the deal was announced on August 18, 2018.[296]
The company's partnership with subscription service Kidbox was announced on April 16, 2019.[297]
Walmart is headquartered in the Walmart Home Office complex in Bentonville, Arkansas. The company's business model is based on selling a wide variety of general merchandise at low prices.[11] Doug McMillon became Walmart's CEO on February 1, 2014. He has also worked as the head of Sam's Club and Walmart International.[298] The company refers to its employees as ""associates"". All Walmart stores in the U.S. and Canada also have designated ""greeters"" at the entrance, a practice pioneered by Sam Walton and later imitated by other retailers. Greeters are trained to help shoppers find what they want and answer their questions.[299]
For many years, associates were identified in the store by their signature blue vest, but this practice was discontinued in June 2007 and replaced with khaki pants and polo shirts. The wardrobe change was part of a larger corporate overhaul to increase sales and rejuvenate the company's stock price.[300] In September 2014, the uniform was again updated to bring back a vest (paid for by the company) for store employees over the same polos and khaki or black pants paid for by the employee. The vest is navy blue for Walmart employees at Supercenters and discount stores, lime green for Walmart Neighborhood Market employees and yellow for self check out associates; door greeters and customer service managers. Both state ""Proud Walmart Associate"" on the left breast and the ""Spark"" logo covering the back.[301] Reportedly one of the main reasons the vest was reintroduced was that some customers had trouble identifying employees.[302] In 2016, self-checkout associates, door greeters and customer service managers began wearing a yellow vest to be better seen by customers. By requiring employees to wear uniforms that are made up of standard ""street wear"", Walmart is not required to purchase the uniforms or reimburse employees which is required in some states, as long as that clothing can be worn elsewhere. Businesses are only legally required to pay for branded shirts and pants or clothes that would be difficult to wear outside of work.[303]
Unlike many other retailers, Walmart does not charge slotting fees to suppliers for their products to appear in the store.[304] Instead, it focuses on selling more-popular products and provides incentives for store managers to drop unpopular products.[304]
From 2006 to 2010, the company eliminated its layaway program. In 2011, the company revived its layaway program.[305][306]
Walmart introduced its Site-To-Store program in 2007, after testing the program since 2004 on a limited basis. The program allows walmart.com customers to buy goods online with a free shipping option, and have goods shipped to the nearest store for pickup.[307]
On September 15, 2017, Walmart announced that it would build a new headquarters in Bentonville to replace its current 1971 building and consolidate operations that have spread out to 20 different buildings throughout Bentonville.[308]
According to watchdog group Documented, in 2020 Walmart contributed $140,000 to the Rule of Law Defense Fund, a fund-raising arm of the Republican Attorneys General Association.[309]
For the fiscal year ending January 31, 2019, Walmart reported net income of US$6.67 billion on $514.405 billion of revenue. The company's international operations accounted for $120.824 billion, or 23.7 percent, of its $510.329 billion of sales.[4][7] Walmart is the world's 29th-largest public corporation, according to the Forbes Global 2000 list, and the largest public corporation when ranked by revenue.[310]
Walmart is governed by an eleven-member board of directors elected annually by shareholders. Gregory B. Penner, son-in-law of S. Robson Walton and the grandson-in-law of Sam Walton, serves as chairman of the board. Doug McMillon serves as president and chief executive officer. Current members of the board are:[311][7][312]
Notable former members of the board include Hillary Clinton (1985–1992)[313] and Tom Coughlin (2003–2004), the latter having served as vice chairman. Clinton left the board before the 1992 U.S. presidential election, and Coughlin left in December 2005 after pleading guilty to wire fraud and tax evasion for stealing hundreds of thousands of dollars from Walmart.[314]
After Sam Walton's death in 1992, Don Soderquist, Chief Operating Officer and Senior Vice Chairman, became known as the ""Keeper of the Culture"".[315]
Walmart Inc. is a Delaware-domiciled joint-stock company registered with the U.S. Securities and Exchange Commission, with its registered office located in Wolters Kluwer's Corporation Trust Center in Wilmington. As of March 2017,[update][362] it has 3,292,377,090 outstanding shares. These are held mainly by the Walton family, a number of institutions and funds.[5][363]
In North America, Walmart's primary competitors include grocery stores and department stores like Aldi, Lidl, Kmart, Kroger, Ingles, Publix, Target, Harris Teeter, Meijer, and Winn Dixie in the United States; Hudson's Bay, Loblaw retail stores, Sobeys, Metro, and Giant Tiger in Canada; and Comercial Mexicana and Soriana in Mexico. Competitors of Walmart's Sam's Club division are Costco and the smaller BJ's Wholesale Club chain. Walmart's move into the grocery business in the late 1990s set it against major supermarket chains in both the United States and Canada. Several smaller retailers, primarily dollar stores, such as Family Dollar and Dollar General, have been able to find a small niche market and compete successfully against Walmart.[365] In 2004, Walmart responded by testing its own dollar store concept, a subsection of some stores called ""Pennies-n-Cents.""[366]
Walmart also had to face fierce competition in some foreign markets. For example, in Germany it had captured just 2 percent of the German food market following its entry into the market in 1997 and remained ""a secondary player"" behind Aldi with 19 percent.[367] Walmart continues to do well in the UK, where its Asda subsidiary is the second-largest retailer.[368]
In May 2006, after entering the South Korean market in 1998, Walmart sold all 16 of its South Korean outlets to Shinsegae, a local retailer, for US$882 million. Shinsegae re-branded the Walmarts as E-mart stores.[369]
Walmart struggled to export its brand elsewhere as it rigidly tried to reproduce its model overseas. In China, Walmart hopes to succeed by adapting and doing things preferable to Chinese citizens. For example, it found that Chinese consumers preferred to select their own live fish and seafood; stores began displaying the meat uncovered and installed fish tanks, leading to higher sales.[370]
Walmart customers cite low prices as the most important reason for shopping there.[371] The average U.S. Walmart customer's income is below the national average.[372] A 2006 Walmart report also indicated that Walmart customers are sensitive to higher utility costs and gas prices.[372] A poll indicated that after the 2004 US presidential election, 76 percent of voters who shopped at Walmart once a week voted for George W. Bush while only 23 percent supported senator John Kerry.[373] When measured against similar retailers in the U.S., frequent Walmart shoppers were rated the most politically conservative.[374] Thus, as of 2014,[update] the ""majority (54 percent) [of] Americans who prefer shopping at Walmart report that they oppose same-sex marriage, while 40 percent are in favor of it.""[375]
Due to its prominence in the Bible Belt, Walmart is known for its ""tradition of tailoring its service to churchgoing customers"".[376][377] Walmart only carries clean versions of hip-hop audio CDs and in cooperation with The Timothy Plan, places ""plastic sheathes over suggestive women's periodicals and banned 'lad mags' such as Maxim"" magazine.[376][377] In addition, Walmart also caters to its Christian customer base by selling Christian books and media,[376][378] ""such as VeggieTales videos and The Purpose-Driven Life"", which earns the company over US$1 billion annually.[378][379]
In 2006, Walmart took steps to expand its U.S. customer base, announcing a modification in its U.S. stores from a ""one-size-fits-all"" merchandising strategy to one designed to ""reflect each of six demographic groups—African-Americans, the affluent, empty-nesters, Hispanics, suburbanites, and rural residents.""[380] Around six months later, it unveiled a new slogan: ""Saving people money so they can live better lives"". This reflects the three main groups into which Walmart categorizes its 200 million customers: ""brand aspirationals"" (people with low incomes who are obsessed with big name brands), ""price-sensitive affluents"" (wealthier shoppers who love deals), and ""value-price shoppers"" (people who like low prices and cannot afford much more).[371] Walmart has also made steps to appeal to more liberal customers, for example, by rejecting the American Family Association's recommendations and carrying the DVD Brokeback Mountain, a love story between two gay cowboys in Wyoming.[381]
Walmart stopped selling handguns in all U.S. states, except for Alaska, in 1993.[382]
In 2018, Walmart stopped selling guns and ammunition to persons younger than 21, following a similar move by Dick's Sporting Goods on the same day.[383] In the same year, Walmart stopped selling military-style rifles that were commonly used in mass shootings.[382]
As of 2019, Walmart was a major retailer of firearms and ammunition.[384] In 2019, after 23 people[385] were killed in a mass shooting at a Walmart store in El Paso, Texas, Walmart announced that it would stop selling all handgun ammunition and certain short-barreled rifle ammunition.[384] The company also announced that it would stop selling handguns in Alaska, the only state where the company still sold handguns.[383] The move was expected to reduce Walmart's U.S. market share in ammunition from around 20% to around 6–9%.[383] Walmart also stated that it was ""respectfully requesting"" that customers not openly carry weapons in Walmart stores, except for authorized law enforcement officers.[384][383]
Following the fatal police shooting of Walter Wallace Jr. in October 2020, Walmart temporarily removed gun and ammunition displays in thousands of stores across the U.S. from sales floors, grounding their reason in concerns of civil unrest. Company spokesman Kory Lundberg said in a statement that ""We have seen some isolated civil unrest and as we have done on several occasions over the last few years, we have moved our firearms and ammunition off the sales floor as a precaution for the safety of our associates and customers."" Firearms and ammunition will still be available for purchase on request, but the duration of the removal of both from the sales floor remains undetermined.[386]
Many Walmart technology projects are coded in the open and available through the Walmart Labs GitHub repository[387] as open-source software under the OSI approved Apache V2.0 license. As of November 2016,[update] 141 public Github projects are listed.
During a migration of the walmart.com retail platform to Facebook React and Node.js, the Electrode[388] project was created to power the e-commerce platform which serves 80 million visitors per month and 15 million items.
Electrode provides various developer enhancements and tools for the developer including Node.js configuration and feature management.
Alex Grigoryan[389] of Walmart Labs released a statement[390] on Medium.com on October 3, 2016, explaining the details of the applications and the scale that they operate at Walmart.
As the largest retailer in the U.S., Walmart collects and analyzes a large amount of consumer data. The big data sets are mined for use in predictive analytics, which allow the company to optimize operations by predicting customer's habits. Walmart's datacenter is unofficially referred to as Area 71.[391]
In April 2011, Walmart acquired Kosmix to develop software for analyzing real-time data streams.[392] In August 2012, Walmart announced its Polaris search engine.[393]
The amount of data gathered by Walmart has raised privacy concerns.[394][395][396]
in 2016, Walmart began a drive to automate much of the cash handling process. Walmart began replacing employees who count currency by hand with machines that count 8 bills per second and 3,000 coins a minute. The processing machines, located in the back of stores, allow cashiers to process the money for electronic depositing.[397][398]
Sam Walton believed that the company's contribution to society was the fact that it operated efficiently, thereby lowering the cost of living for customers, and, therefore, in that sense was a ""powerful force for good"", despite his refusal to contribute cash to philanthropic causes.[399] Having begun to feel that his wealth attracted people who wanted nothing more than a ""handout"", he explained that while he believed his family had been fortunate and wished to use his wealth to aid worthy causes like education, they could not be expected to ""solve every personal problem that comes to [their] attention"". He explained later in his autobiography, ""We feel very strongly that Wal-Mart really is not, and should not be, in the charity business,"" stating ""any debit has to be passed along to somebody—either shareholders or our customers.""[400] Since Sam Walton's death in 1992, however, Walmart and the Walmart Foundation dramatically increased charitable giving. For example, in 2005, Walmart donated US$20 million in cash and merchandise for Hurricane Katrina relief and in 2020 they committed $25 million to organizations on the frontlines of the COVID-19 pandemic response.[401] Today, Walmart's charitable donations approach US$1 billion each year.[402]
Kenneth Stone, Professor of Economics at Iowa State University, in a paper published in Farm Foundation in 1997, found that some small towns can lose almost half of their retail trade within ten years of a Walmart store opening. He compared the changes to previous competitors small town shops have faced in the past—from the development of the railroads and the Sears Roebuck catalog to shopping malls. He concludes that small towns are more affected by ""discount mass merchandiser stores"" than larger towns and that shop owners who adapt to the ever-changing retail market can ""co-exist and even thrive in this type of environment.""[53]
One study found Walmart's entry into a new market has a profound impact on its competition. When a Walmart opens in a new market, median sales drop 40 percent at similar high-volume stores, 17 percent at supermarkets and 6 percent at drugstores, according to a June 2009 study by researchers at several universities and led by the Tuck School of Business at Dartmouth College.[403] A Loyola University Chicago study suggested that the impact a Walmart store has on a local business is correlated to its distance from that store. The leader of that study admits that this factor is stronger in smaller towns and doesn't apply to more urban areas saying ""It'd be so tough to nail down what's up with Wal-Mart"".[404] These findings are underscored by another study conducted in 2009 by the National Bureau of Economics that showed ""large, negative effects"" for competing businesses within 5 to 10 miles (8 to 16 km) of the newly opening big-box retailer. This same study also found that the local retailers experience virtually no benefit.[405] Walmart's negative effects on local retailers may be partially explained by studies that find that local firms re-invest nearly 63 percent more of profits in other local businesses compared to chain retailers, as found by the Maine Center of Economic Policy in 2011.[406]
David Merriman, Joseph Persky, Julie Davis and Ron Baiman did a study in Economic Development Quarterly outlining the impacts of Walmart in Chicago. The study draws from three annual surveys of enterprises within a four-mile radius of a new Chicago Walmart and it ""shows that the probability of going out of business was significantly higher for establishments close to that store"". The study illustrated how approximately 300 jobs were lost due to the opening of the store, which is about equivalent to Walmart's employment in the area. The overall findings of this study reinforce the ""contention that large-city Walmarts, like those in small towns, absorb retail sales from nearby stores without significantly expanding the market"" as this is one of the first studies of Walmarts economic impacts on local economies.[407]
With over 2.3 million employees worldwide, Walmart has faced a torrent of lawsuits and issues with regards to its workforce. These issues involve low wages, poor working conditions, inadequate health care, and issues involving the company's strong anti-union policies. In November 2013, the National Labor Relations Board (NLRB) announced that it had found that in 13 U.S. states, Wal-Mart had pressured employees not to engage in strikes on Black Friday, and had illegally disciplined workers who had engaged in strikes.[419] Critics point to Walmart's high turnover rate as evidence of an unhappy workforce, although other factors may be involved. Approximately 70 percent of its employees leave within the first year.[420] Despite this turnover rate, the company is still able to affect unemployment rates. This was found in a study by Oklahoma State University which states, ""Walmart is found to have substantially lowered the relative unemployment rates of blacks in those counties where it is present, but to have had only a limited impact on relative incomes after the influences of other socio-economic variables were taken into account.""[421]
Walmart is the largest private employer in the United States, employing almost five times as many people as IBM, the second-largest employer.[422] Walmart employs more African Americans than any other private employer in the United States.[423]
Walmart rebranded their Associate Education Benefits to Live Better U in March 2019.[424] Live Better U supports associate education at every level and includes $1 a day college program, cost-free high school education, and discounts on higher education programs through partnership with Guild Education.[424]
In April 2019 Walmart Inc. announced plans to extend the use of robots in stores in order to improve and monitor inventory, clean floors and unload trucks, part of the company's effort to lower its labor costs.[425]
In June 2019, Walmart Inc. announced the expansion of education benefits to recruit high school students. The incentives include flexible work schedules, free SAT and ACT preparation courses, up to seven hours of free college credit, and a debt-free college degree in three fields from six nonprofit universities.[426]
In 2007, a gender discrimination lawsuit, Dukes v. Wal-Mart Stores, Inc., was filed against Walmart, alleging that female employees were discriminated against in matters regarding pay and promotions. A class action suit was sought, which would have been the nation's largest in history, covering 1.5 million past and current employees.[427] On June 20, 2011, the United States Supreme Court ruled in Wal-Mart's favor, stating that the plaintiffs did not have enough in common to constitute a class.[428] The court ruled unanimously that because of the variability of the plaintiffs' circumstances, the class action could not proceed as presented, and furthermore, in a 5–4 decision that it could not proceed as any kind of class action suit.[429] Several plaintiffs, including the lead plaintiff, Betty Dukes, expressed their intent to file individual discrimination lawsuits separately.[430]
According to a consultant hired by plaintiffs in a sex discrimination lawsuit, in 2001, Wal-Mart's Equal Employment Opportunity Commission filings showed that female employees made up 65 percent of Wal-Mart's hourly paid workforce, but only 33 percent of its management.[431][432] Just 35 percent of its store managers were women, compared to 57 percent at similar retailers.[432] Wal-Mart says comparisons with other retailers are unfair, because it classifies employees differently; if department managers were included in the totals, women would make up 60 percent of the managerial ranks.[432] Others have criticized the lawsuit as without basis in the law and as an abuse of the class action mechanism.[433][434][435] In 2007, Wal-Mart was named by the National Association for Female Executives as one of the top 35 companies for executive women.[436]
In the Human Rights Campaign's (HRC) 2002 Corporate Equality Index, a measure of how companies treat LGBT employees and customers, gave Wal-Mart Stores Inc. a score of 14%.[437] By 2017, however, HRC's 2017 Corporate Equality Index gave Wal-Mart Stores Inc. a score of a 100%.[438] In 2003, Walmart added sexual orientation to their anti-discrimination policy.[439] In 2005, Walmart's definition of family began including same-sex partners.[440][441][442] In 2006, Walmart announced that ""diversity efforts include new groups of minority, female and gay employees that meet at Walmart headquarters in Bentonville to advise the company on marketing and internal promotion. There are seven business resource groups: women, African Americans, Hispanics, Asians, Native Americans, gays and lesbians, and a disabled group.""[443] From 2006 to 2008, Walmart was a member of the National Gay & Lesbian Chamber of Commerce.[444] In 2011, Walmart added gender identity to their anti-discrimination policy.[445] Walmart's anti-discrimination policies allow associates to use restroom facilities that corresponds with their gender identity and gender expression.[446] In 2013, Walmart began offering health insurance benefits to domestic partners.[444] In 2015, Doug McMillon, CEO of Walmart, issued a statement opposing House Bill 1228 and asked Governor Asa Hutchinson to veto the bill.[447] In 2016, Walmart added full healthcare benefits to its transgender employees.[448]
Walmart has been subject to criticism from various groups and individuals, including labor unions, community groups, grassroots organizations, religious organizations, environmental groups, firearm groups, and the company's own customers and employees. They have protested against the company's policies and business practices, including charges of racial and gender discrimination.[449][450][451] Other areas of criticism include the company's foreign product sourcing, treatment of suppliers, employee compensation and working conditions, environmental practices, the use of public subsidies, the company's security policies, and slavery.[452][453] Walmart denies doing anything wrong and maintains that low prices are the result of efficiency.[454][455][456]
In April 2016, Walmart announced that it plans to eliminate eggs from battery cages from its supply chain by 2025.[457] The decision was particularly important because of Walmart's large market share and influence on the rest of the industry.[458][459] The move was praised by major animal welfare groups[460] but a poultry trade group representative expressed skepticism about the decision's impact.[460] Walmart's cage-free eggs will not come from free range producers, but rather industrial-scale farms where the birds will be allotted between 1 and 1.5 square feet each, a stressful arrangement which can cause cannibalism.[458][460] Unlike battery cages, the systems Walmart's suppliers will allow the hens to move around, but relative to battery cages they have higher hen mortality rates and present distinct environmental and worker health problems.[461]
In March 2018, Walmart was sued by former Director of Business Development Tri Huynh for claims of reporting misleading e-commerce performance results in favor of the company. Huynh stated the company's move was an attempt to regain lost ground to competitor Amazon.[462]
In September 2018, Walmart was sued by Equal Employment Opportunity Commission alleging that Walmart denied requests from pregnant employees to limit heavy lifting.[463]
In May 2019, the Center for Inquiry filed a lawsuit in the District of Columbia alleging consumer fraud and the endangering of its customers' health due to Walmart's practice of ""selling homeopathic [products] alongside real medicine, in the same sections in its stores, under the same signs,"" according to Nicholas Little, CFI's vice president and general counsel.[464][465]
In July 2019, the Walmart subreddit was flooded with pro-union memes in a protest to the firing of an employee who posted confidential material to the subreddit.[466][467] Many of these posts were angry with Walmart surveying its staff on the Internet. The posting of the union content is in response to the aforementioned alleged anti-union position Walmart has taken in the past.[468]
According to an August 2016 report by Bloomberg Businessweek, aggressive cost-cutting decisions that began in 2000 when Lee Scott took over as CEO of the company led to a significant increase in crime in stores across the United States. These included the removal of the store's famed greeters, who are in part seen as a theft deterrent at exits, the replacement of many cashiers with self-checkout stations, and the addition of stores at a rate that exceeded the hiring of new employees, which led to a 19% increase in space per employee from a decade previous. While these decisions succeeded in increasing profits 23% in the decade that followed, they also led to an increase in both theft and violent crime.[469]
In 2015, under CEO Doug McMillon, Walmart began a company-wide campaign to reduce crime that included spot-checking receipts at exits, stationing employees at self-checkout areas, eye-level security cameras in high-theft areas, use of data analytics to detect credit fraud, hiring off-duty police and private security officers, and reducing calls to police with a program by which first-time offenders caught stealing merchandise below a certain value can avoid arrest if they agree to go through a theft-prevention program.[469]
However, some law enforcement professionals have said the improvements are coming too slowly. A longtime police veteran in Tulsa, Oklahoma, stated, ""It's ridiculous—we are talking about the biggest retailer in the world. I may have half my squad there for hours."" A police captain in Port Richey, Florida, stated, ""They recognize the problem and refuse to do anything about it.""[469]
Law enforcement agencies across the United States have noted a burden on resources created by a disproportionate number of calls from Walmart. Experts have criticized the retailer for shifting its security burden onto the taxpayers. Across three Florida counties, approximately 9,000 police calls were logged to 53 Walmart stores but resulted in only a few hundred arrests.[470] In Granite Falls, North Carolina, 92% of larceny calls to local police were from the Walmart store there.[471] The trend is similar in rural, suburban, and urban areas. Police are called to Walmart stores 3 to 4 times as much as similar retailers such as Target.[472] Experts say the chain and its razor-thin profit margins rely heavily on police to protect its bottom line. Walmart Supercenters top the list of those most visited by police.[470]
The police captain in Port Richey, Florida, said that Target stores more often have uniformed security, as well as more visible staff in any case. Another comparison might be shopping malls which often have security patrols and off-duty police officers. J.R. Roberts, a former director for risk management at Valor Security Services (which provides mall security) says: ""Shopping centers all have security; they know it's an expense, but one they know pays dividends because people feel safer going to their stores.""[469]
In addition to hundreds of thousands of petty crimes, more than 200 violent crimes, including attempted kidnappings, stabbings, shootings, and murders occurred at the 4,500 Walmarts in the U.S. in 2016.[469] In 2019, 23 people were killed in a mass shooting at a Walmart store in El Paso, Texas.[383][385]
On June 27, 2020, a shooting occurred at a Walmart distribution center in Red Bluff, California, United States. One employee was killed, four other employees were wounded, and the shooter was killed by officers.[473][474][475][476]
Coordinates: .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}36°21′56″N 94°13′03″W﻿ / ﻿36.36556°N 94.21750°W﻿ / 36.36556; -94.21750
"
Mississippi - Wikipedia," 

Mississippi (/ˌmɪsɪˈsɪpi/ (listen)) is a state in the Deep South region of the Southern United States, bordered to the north by Tennessee; to the east by Alabama; to the south by the Gulf of Mexico; to the southwest by Louisiana; and to the northwest by Arkansas. Mississippi's western boundary is largely defined by the Mississippi River. Mississippi is the 32nd largest and 34th-most populous of the 50 U.S. states. Jackson is both the state's capital and largest city. Greater Jackson is the state's most populous metropolitan area, with an estimated population of 580,166 in 2018.
On December 10, 1817, Mississippi became the 20th state admitted to the Union. By 1860, Mississippi was the nation's top cotton-producing state and enslaved persons accounted for 55% of the state population.[5] Mississippi declared its secession from the Union on March 23, 1861, and was one of the seven original Confederate States, which constituted the largest slaveholding states in the nation. Following the Civil War, it was restored to the Union on February 23, 1870.[6]
Until the Great Migration of the 1930s, African Americans were a majority of Mississippi's population. Mississippi was the site of many prominent events during the civil rights movement, including the Ole Miss riot of 1962 by white students objecting to desegregation, the 1963 assassination of Medgar Evers, and the 1964 Freedom Summer murders of three activists working on voting rights. With large areas of agriculture and rural towns, Mississippi frequently ranks low among states in measures of health, education, and development, and high in measures of poverty.[7][8][9][10] In 2010, 37.3% of Mississippi's population was African American, the highest percentage of any state.
Mississippi is almost entirely within the Gulf coastal plain, and generally consists of lowland plains and low hills. The northwest remainder of the state consists of the Mississippi Delta, a section of the Mississippi Alluvial Plain. Mississippi's highest point is Woodall Mountain at 807 feet (246 m) above sea level adjacent to the Cumberland Plateau; the lowest is the Gulf of Mexico. Mississippi has a humid subtropical climate classification.
The state's name is derived from the Mississippi River, which flows along and defines its western boundary. European-American settlers named it after the Ojibwe word misi-ziibi (English: Great river).
Mississippi is bordered to the north by Tennessee, to the east by Alabama, to the south by Louisiana and a narrow coast on the Gulf of Mexico; and to the west, across the Mississippi River, by Louisiana and Arkansas.
In addition to its namesake, major rivers in Mississippi include the Big Black River, the Pearl River, the Yazoo River, the Pascagoula River, and the Tombigbee River. Major lakes include Ross Barnett Reservoir, Arkabutla, Sardis, and Grenada, with the largest being Sardis Lake.
Mississippi is entirely composed of lowlands, the highest point being Woodall Mountain, in the foothills of the Cumberland Mountains, 807 ft (246 m) above sea level. The lowest point is sea level at the Gulf Coast. The state's mean elevation is 300 ft (91 m) above sea level.
Most of Mississippi is part of the East Gulf Coastal Plain. The coastal plain is generally composed of low hills, such as the Pine Hills in the south and the North Central Hills. The Pontotoc Ridge and the Fall Line Hills in the northeast have somewhat higher elevations. Yellow-brown loess soil is found in the western parts of the state. The northeast is a region of fertile black earth uplands, a geology that extend into the Alabama Black Belt.
The coastline includes large bays at Bay St. Louis, Biloxi, and Pascagoula. It is separated from the Gulf of Mexico proper by the shallow Mississippi Sound, which is partially sheltered by Petit Bois Island, Horn Island, East and West Ship Islands, Deer Island, Round Island, and Cat Island.
The northwest remainder of the state consists of the Mississippi Delta, a section of the Mississippi Alluvial Plain. The plain is narrow in the south and widens north of Vicksburg. The region has rich soil, partly made up of silt which had been regularly deposited by the flood waters of the Mississippi River.
Areas under the management of the National Park Service include:[11]
Mississippi City Population Rankings of at least 50,000 (United States Census Bureau as of 2017):[12]
Mississippi City Population Rankings of at least 20,000 but fewer than 50,000 (United States Census Bureau as of 2017):[12]
Mississippi City Population Rankings of at least 10,000 but fewer than 20,000 (United States Census Bureau as of 2017):[12]
(See: Lists of cities, towns and villages, census-designated places, metropolitan areas, micropolitan areas, and counties in Mississippi)
Mississippi has a humid subtropical climate with long, hot and humid summers, and short, mild winters. Temperatures average about 81 °F (27 °C) in July and about 42 °F (6 °C) in January. The temperature varies little statewide in the summer; however, in winter, the region near Mississippi Sound is significantly warmer than the inland portion of the state. The recorded temperature in Mississippi has ranged from −19 °F (−28 °C), in 1966, at Corinth in the northeast, to 115 °F (46 °C), in 1930, at Holly Springs in the north. Heavy snowfall rarely occurs, but isn't unheard of, such as during the New Year's Eve 1963 snowstorm. Yearly precipitation generally increases from north to south, with the regions closer to the Gulf being the most humid. Thus, Clarksdale, in the northwest, gets about 50 in (1,300 mm) of precipitation annually and Biloxi, in the south, about 61 in (1,500 mm). Small amounts of snow fall in northern and central Mississippi; snow is occasional in the southern part of the state.
The late summer and fall is the seasonal period of risk for hurricanes moving inland from the Gulf of Mexico, especially in the southern part of the state. Hurricane Camille in 1969 and Hurricane Katrina in 2005, which killed 238 people in the state, were the most devastating hurricanes to hit the state. Both caused nearly total storm surge destruction of structures in and around Gulfport, Biloxi, and Pascagoula.
As in the rest of the Deep South, thunderstorms are common in Mississippi, especially in the southern part of the state. On average, Mississippi has around 27 tornadoes annually; the northern part of the state has more tornadoes earlier in the year and the southern part a higher frequency later in the year. Two of the five deadliest tornadoes in United States history have occurred in the state. These storms struck Natchez, in southwest Mississippi (see The Great Natchez Tornado) and Tupelo, in the northeast corner of the state. About seven F5 tornadoes have been recorded in the state.

Climate change in Mississippi encompasses the effects of climate change, attributed to man-made increases in atmospheric carbon dioxide, in the U.S. state of Mississippi.
Studies show that Mississippi is among a string of ""Deep South"" states that will experience the worst effects of climate change in the United States.[15] The United States Environmental Protection Agency reports:
Mississippi is heavily forested, with over half of the state's area covered by wild or cultivated trees. The southeastern part of the state is dominated by longleaf pine, in both uplands and lowland flatwoods and Sarracenia bogs. The Mississippi Alluvial Plain, or Delta, is primarily farmland and aquaculture ponds but also has sizeable tracts of cottonwood, willows, bald cypress, and oaks. A belt of loess extends north to south in the western part of the state, where the Mississippi Alluvial Plain reaches the first hills; this region is characterized by rich, mesic mixed hardwood forests, with some species disjunct from Appalachian forests.[17] Two bands of historical prairie, the Jackson Prairie and the Black Belt, run northwest to southeast in the middle and northeastern part of the state. Although these areas have been highly degraded by conversion to agriculture, a few areas remain, consisting of grassland with interspersed woodland of eastern redcedar, oaks, hickories, osage-orange, and sugarberry. The rest of the state, primarily north of Interstate 20 not including the prairie regions, consists of mixed pine-hardwood forest, common species being loblolly pine, oaks (e.g., water oak), hickories, sweetgum, and elm. Areas along large rivers are commonly inhabited by bald cypress, water tupelo, water elm, and bitter pecan. Commonly cultivated trees include loblolly pine, longleaf pine, cherrybark oak, and cottonwood.
There are approximately 3000 species of vascular plants known from Mississippi.[18] As of 2018, a project funded by the U.S. National Science Foundation aims to update that checklist of plants with museum (herbarium) vouchers and create an online atlas of each species's distribution.[19]
About 420 species of birds are known to inhabit Mississippi.
Mississippi has one of the richest fish faunas in the United States, with 204 native fish species.[20]
Mississippi also has a rich freshwater mussel fauna, with about 90 species in the primary family of native mussels (Unionidae).[21] Several of these species were extirpated during the construction of the Tennessee-Tombigbee Waterway.
Mississippi is home to 63 crayfish species, including at least 17 endemic species.[22]
Mississippi is home to eight winter stonefly species.[23]
Due to seasonal flooding, possible from December to June, the Mississippi and Yazoo rivers and their tributaries created a fertile floodplain in the Mississippi Delta. The river's flooding created natural levees, which planters had built higher to try to prevent flooding of land cultivated for cotton crops. Temporary workers built levees along the Mississippi River on top of the natural levees that formed from dirt deposited after the river flooded.
From 1858 to 1861, the state took over levee building, accomplishing it through contractors and hired labor. In those years, planters considered their slaves too valuable to hire out for such dangerous work. Contractors hired gangs of Irish immigrant laborers to build levees and sometimes clear land. Many of the Irish were relatively recent immigrants from the famine years who were struggling to get established.[24] Before the American Civil War, the earthwork levees averaged six feet in height, although in some areas they reached twenty feet.
Flooding has been an integral part of Mississippi history, but clearing of the land for cultivation and to supply wood fuel for steamboats took away the absorption of trees and undergrowth. The banks of the river were denuded, becoming unstable and changing the character of the river. After the Civil War, major floods swept down the valley in 1865, 1867, 1874 and 1882. Such floods regularly overwhelmed levees damaged by Confederate and Union fighting during the war, as well as those constructed after the war.[25] In 1877, the state created the Mississippi Levee District for southern counties.
In 1879, the United States Congress created the Mississippi River Commission, whose responsibilities included aiding state levee boards in the construction of levees. Both white and black transient workers were hired to build the levees in the late 19th century. By 1882, levees averaged seven feet in height, but many in the southern Delta were severely tested by the flood that year.[25] After the 1882 flood, the levee system was expanded. In 1884, the Yazoo-Mississippi Delta Levee District was established to oversee levee construction and maintenance in the northern Delta counties; also included were some counties in Arkansas which were part of the Delta.[26]
Flooding overwhelmed northwestern Mississippi in 1912–1913, causing heavy damage to the levee districts. Regional losses and the Mississippi River Levee Association's lobbying for a flood control bill helped gain passage of national bills in 1917 and 1923 to provide federal matching funds for local levee districts, on a scale of 2:1. Although U.S. participation in World War I interrupted funding of levees, the second round of funding helped raise the average height of levees in the Mississippi-Yazoo Delta to 22 feet (6.7 m) in the 1920s.[27] Scientists now understand the levees have increased the severity of flooding by increasing the flow speed of the river and reducing the area of the floodplains. The region was severely damaged due to the Great Mississippi Flood of 1927, which broke through the levees. There were losses of millions of dollars in property, stock and crops. The most damage occurred in the lower Delta, including Washington and Bolivar counties.[28]
Even as scientific knowledge about the Mississippi River has grown, upstream development and the consequences of the levees have caused more severe flooding in some years. Scientists now understand that the widespread clearing of land and building of the levees have changed the nature of the river. Such work removed the natural protection and absorption of wetlands and forest cover, strengthening the river's current. The state and federal governments have been struggling for the best approaches to restore some natural habitats in order to best interact with the original riverine ecology.
Near 10,000 BC Native Americans or Paleo-Indians arrived in what today is referred to as the American South.[29] Paleo-Indians in the South were hunter-gatherers who pursued the megafauna that became extinct following the end of the Pleistocene age. In the Mississippi Delta, Native American settlements and agricultural fields were developed on the natural levees, higher ground in the proximity of rivers. The Native Americans developed extensive fields near their permanent villages. Together with other practices, they created some localized deforestation but did not alter the ecology of the Mississippi Delta as a whole.[30]
After thousands of years, succeeding cultures of the Woodland and Mississippian culture eras developed rich and complex agricultural societies, in which surplus supported the development of specialized trades. Both were mound builder cultures. Those of the Mississippian culture were the largest and most complex, constructed beginning about 950 AD. The peoples had a trading network spanning the continent from the Great Lakes to the Gulf Coast. Their large earthworks, which expressed their cosmology of political and religious concepts, still stand throughout the Mississippi and Ohio River valleys.
Descendant Native American tribes of the Mississippian culture in the Southeast include the Chickasaw and Choctaw. Other tribes who inhabited the territory of Mississippi (and whose names were honored by colonists in local towns) include the Natchez, the Yazoo, and the Biloxi.
The first major European expedition into the territory that became Mississippi was that of the Spanish explorer, Hernando de Soto, who passed through the northeast part of the state in 1540, in his second expedition to the New World.
In April 1699, French colonists established the first European settlement at Fort Maurepas (also known as Old Biloxi), built in the vicinity of present-day Ocean Springs on the Gulf Coast. It was settled by Pierre Le Moyne d'Iberville. In 1716, the French founded Natchez on the Mississippi River (as Fort Rosalie); it became the dominant town and trading post of the area. The French called the greater territory ""New France""; the Spanish continued to claim part of the Gulf coast area (east of Mobile Bay) of present-day southern Alabama, in addition to the entire area of present-day Florida.
Through the 18th century, the area was ruled variously by Spanish, French, and British colonial governments. The colonists imported African slaves as laborers. Under French and Spanish rule, there developed a class of free people of color (gens de couleur libres), mostly multiracial descendants of European men and enslaved or free black women, and their mixed-race children. In the early days the French and Spanish colonists were chiefly men. Even as more European women joined the settlements, the men had interracial unions among women of African descent (and increasingly, multiracial descent), both before and after marriages to European women. Often the European men would help their multiracial children get educated or gain apprenticeships for trades, and sometimes they settled property on them; they often freed the mothers and their children if enslaved, as part of contracts of plaçage. With this social capital, the free people of color became artisans, and sometimes educated merchants and property owners, forming a third class between the Europeans and most enslaved Africans in the French and Spanish settlements, although not so large a free community as in the city of New Orleans, Louisiana.
After Great Britain's victory in the French and Indian War (Seven Years' War), the French surrendered the Mississippi area to them under the terms of the Treaty of Paris (1763). They also ceded their areas to the north that were east of the Mississippi River, including the Illinois Country and Quebec. After the Peace of Paris (1783), the lower third of Mississippi came under Spanish rule as part of West Florida. In 1819 the United States completed the purchase of West Florida and all of East Florida in the Adams–Onís Treaty, and in 1822 both were merged into the Florida Territory.
After the American Revolution (1765–83), Britain ceded this area to the new United States of America. The Mississippi Territory was organized on April 7, 1798, from territory ceded by Georgia and South Carolina to the United States. Their original colonial charters theoretically extended west to the Pacific Ocean. The Mississippi Territory was later twice expanded to include disputed territory claimed by both the United States and Spain.
From 1800 to about 1830, the United States purchased some lands (Treaty of Doak's Stand) from Native American tribes for new settlements of European Americans. The latter were mostly migrants from other Southern states, particularly Virginia and North Carolina, where soils were exhausted.[31] New settlers kept encroaching on Choctaw land, and they pressed the federal government to expel the Native Americans. On September 27, 1830, the Treaty of Dancing Rabbit Creek was signed between the U.S. Government and the Choctaw. The Choctaw agreed to sell their traditional homelands in Mississippi and Alabama, for compensation and removal to reservations in Indian Territory (now Oklahoma). This opened up land for sale to European-American migrant settlement.
Article 14 in the treaty allowed those Choctaw who chose to remain in the states to become U.S. citizens, as they were considered to be giving up their tribal membership. They were the second major Native American ethnic group to do so (some Cherokee were the first, who chose to stay in North Carolina and other areas during rather than join the removal).[32][33] Today their descendants include approximately 9,500 persons identifying as Choctaw, who live in Neshoba, Newton, Leake, and Jones counties. The Mississippi Band of Choctaw Indians reorganized in the 20th century and is a Federally recognized tribe.
Many slaveholders brought enslaved African Americans with them or purchased them through the domestic slave trade, especially in New Orleans. Through the trade, an estimated nearly one million slaves were forcibly transported to the Deep South, including Mississippi, in an internal migration that broke up many slave families of the Upper South, where planters were selling excess slaves. The Southerners imposed slave laws in the Deep South and restricted the rights of free blacks.
Beginning in 1822, slaves in Mississippi were protected by law from cruel and unusual punishment by their owners.[34] The Southern slave codes made the willful killing of a slave illegal in most cases.[35] For example, the 1860 Mississippi case of Oliver v. State charged the defendant with murdering his own slave.[36]
Mississippi became the 20th state on December 10, 1817. David Holmes was the first governor.[37] The state was still occupied as ancestral land by several Native American tribes, including Choctaw, Natchez, Houma, Creek, and Chickasaw.[38][39]
Plantations were developed primarily along the major rivers, where the waterfront provided access to the major transportation routes. This is also where early towns developed, linked by the steamboats that carried commercial products and crops to markets. The remainder of Native American ancestral land remained largely undeveloped but was sold through treaties until 1826, when the Choctaws and Chickasaws refused to sell more land.[40] 
The combination of the Mississippi state legislature's abolition of Choctaw Tribal Government in 1829,[41] President Andrew Jackson's Indian Removal Act and the Treaty of Dancing Rabbit Creek[42] of 1830, the Choctaw were effectively forced to sell their land and were transported to Oklahoma Territory. The forced migration of the Choctaw, together with other southeastern tribes removed as a result of the Act, became known as the Trail of Tears.
When cotton was king during the 1850s, Mississippi plantation owners—especially those of the Delta and Black Belt central regions—became wealthy due to the high fertility of the soil, the high price of cotton on the international market, and free labor gained through their holding enslaved African Americans. They used some of their profits to buy more cotton land and more slaves. The planters' dependence on hundreds of thousands of slaves for labor and the severe wealth imbalances among whites, played strong roles both in state politics and in planters' support for secession. Mississippi was a slave society, with the economy dependent on slavery. The state was thinly settled, with population concentrated in the riverfront areas and towns.
By 1860, the enslaved African-American population numbered 436,631 or 55% of the state's total of 791,305 persons. Fewer than 1000 were free people of color.[43] The relatively low population of the state before the Civil War reflected the fact that land and villages were developed only along the riverfronts, which formed the main transportation corridors. Ninety percent of the Delta bottomlands were still frontier and undeveloped.[44] The state needed many more settlers for development. The land further away from the rivers was cleared by freedmen and white migrants during Reconstruction and later.[44]
On January 9, 1861, Mississippi became the second state to declare its secession from the Union, and it was one of the founding members of the Confederate States. The first six states to secede were those with the highest number of slaves. During the war, Union and Confederate forces struggled for dominance on the Mississippi River, critical to supply routes and commerce. More than 80,000 Mississippians fought in the Civil War, and casualties were extremely heavy. Union General Ulysses S. Grant's long siege of Vicksburg finally gained the Union control of the river in 1863.
In the postwar period, freedmen withdrew from white-run churches to set up independent congregations. The majority of blacks left the Southern Baptist Church, sharply reducing its membership. They created independent black Baptist congregations. By 1895 they had established numerous black Baptist state associations and the National Baptist Convention of black churches.[45]
In addition, independent black denominations, such as the African Methodist Episcopal Church (established in Philadelphia, Pennsylvania in the early 19th century) and the African Methodist Episcopal Zion Church (established in New York City), sent missionaries to the South in the postwar years. They quickly attracted hundreds of thousands of converts and founded new churches across the South. Southern congregations brought their own influences to those denominations as well.[45][46]
During Reconstruction, the first Mississippi constitutional convention in 1868, with delegates both black and white, framed a constitution whose major elements would be maintained for 22 years.[47] The convention was the first political organization in the state to include African-American representatives, 17 among the 100 members (32 counties had black majorities at the time). Some among the black delegates were freedmen, but others were educated free blacks who had migrated from the North. The convention adopted universal suffrage; did away with property qualifications for suffrage or for office, a change that also benefited both blacks and poor whites; provided for the state's first public school system; forbade race distinctions in the possession and inheritance of property; and prohibited limiting civil rights in travel.[47] Under the terms of Reconstruction, Mississippi was restored to the Union on February 23, 1870.
Because the Mississippi Delta contained so much fertile bottomland that had not been developed before the Civil War, 90 percent of the land was still frontier. After the Civil War, tens of thousands of migrants were attracted to the area by higher wages offered by planters trying to develop land. In addition, black and white workers could earn money by clearing the land and selling timber, and eventually advance to ownership. The new farmers included many freedmen, who by the late 19th century achieved unusually high rates of land ownership in the Mississippi bottomlands. In the 1870s and 1880s, many black farmers succeeded in gaining land ownership.[44]
Around the start of the 20th century, two-thirds of the Mississippi farmers who owned land in the Delta were African American.[44] But many had become overextended with debt during the falling cotton prices of the difficult years of the late 19th century. Cotton prices fell throughout the decades following the Civil War. As another agricultural depression lowered cotton prices into the 1890s, numerous African-American farmers finally had to sell their land to pay off debts, thus losing the land which they had developed by hard, personal labor.[44]
Democrats had regained control of the state legislature in 1875, after a year of expanded violence against blacks and intimidation of whites in what was called the ""white line"" campaign, based on asserting white supremacy. Democratic whites were well armed and formed paramilitary organizations such as the Red Shirts to suppress black voting. From 1874 to the elections of 1875, they pressured whites to join the Democrats, and conducted violence against blacks in at least 15 known ""riots"" in cities around the state to intimidate blacks. They killed a total of 150 blacks, although other estimates place the death toll at twice as many. A total of three white Republicans and five white Democrats were reported killed. In rural areas, deaths of blacks could be covered up. Riots (better described as massacres of blacks) took place in Vicksburg, Clinton, Macon, and in their counties, as well-armed whites broke up black meetings and lynched known black leaders, destroying local political organizations.[48] Seeing the success of this deliberate ""Mississippi Plan"", South Carolina and other states followed it and also achieved white Democratic dominance. In 1877 by a national compromise, the last of federal troops were withdrawn from the region.
Even in this environment, black Mississippians continued to be elected to local office. However, black residents were deprived of all political power after white legislators passed a new state constitution in 1890 specifically to ""eliminate the nigger from politics"", according to the state's Democratic governor, James K. Vardaman.[49] It erected barriers to voter registration and instituted electoral provisions that effectively disenfranchised most black Mississippians and many poor whites. Estimates are that 100,000 black and 50,000 white men were removed from voter registration rolls in the state over the next few years.[50]
The loss of political influence contributed to the difficulties of African Americans in their attempts to obtain extended credit in the late 19th century. Together with imposition of Jim Crow and racial segregation laws, whites increased violence against blacks, lynching mostly men, through the period of the 1890s and extending to 1930. Cotton crops failed due to boll weevil infestation and successive severe flooding in 1912 and 1913, creating crisis conditions for many African Americans. With control of the ballot box and more access to credit, white planters bought out such farmers, expanding their ownership of Delta bottomlands. They also took advantage of new railroads sponsored by the state.[44]
In 1900, blacks made up more than half of the state's population. By 1910, a majority of black farmers in the Delta had lost their land and become sharecroppers. By 1920, the third generation after freedom, most African Americans in Mississippi were landless laborers again facing poverty.[44] Starting about 1913, tens of thousands of black Americans left Mississippi for the North in the Great Migration to industrial cities such as St. Louis, Chicago, Detroit, Cleveland, Philadelphia and New York. They sought jobs, better education for their children, the right to vote, relative freedom from discrimination, and better living. In the migration of 1910–1940, they left a society that had been steadily closing off opportunity. Most migrants from Mississippi took trains directly north to Chicago and often settled near former neighbors.
Blacks also faced violence in the form of lynching, shooting, and the burning of churches. In 1923, the National Association for the Advancement of Colored People stated ""the Negro feels that life is not safe in Mississippi and his life may be taken with impunity at any time upon the slightest pretext or provocation by a white man"".[51]
In the early 20th century, some industries were established in Mississippi, but jobs were generally restricted to whites, including child workers. The lack of jobs also drove some southern whites north to cities such as Chicago and Detroit, seeking employment, where they also competed with European immigrants. The state depended on agriculture, but mechanization put many farm laborers out of work.
By 1900, many white ministers, especially in the towns, subscribed to the Social Gospel movement, which attempted to apply Christian ethics to social and economic needs of the day. Many strongly supported Prohibition, believing it would help alleviate and prevent many sins.[52] Mississippi became a dry state in 1908 by an act of the State legislature.[53] It remained dry until the legislature passed a local option bill in 1966.[54]
African-American Baptist churches grew to include more than twice the number of members as their white Baptist counterparts. The African-American call for social equality resonated throughout the Great Depression in the 1930s and World War II in the 1940s.
The Second Great Migration from the South started in the 1940s, lasting until 1970. Almost half a million people left Mississippi in the second migration, three-quarters of them black. Nationwide during the first half of the 20th century, African Americans became rapidly urbanized and many worked in industrial jobs. The Second Great Migration included destinations in the West, especially California, where the buildup of the defense industry offered higher-paying jobs to both African Americans and whites.
Blacks and whites in Mississippi generated rich, quintessentially American music traditions: gospel music, country music, jazz, blues and rock and roll. All were invented, promulgated or heavily developed by Mississippi musicians, many of them African American, and most came from the Mississippi Delta. Many musicians carried their music north to Chicago, where they made it the heart of that city's jazz and blues.
So many African Americans left in the Great Migration that after the 1930s, they became a minority in Mississippi. In 1960 they made up 42% of the state's population.[55] The whites maintained their discriminatory voter registration processes established in 1890, preventing most blacks from voting, even if they were well educated. Court challenges were not successful until later in the century. After World War II, African-American veterans returned with renewed commitment to be treated as full citizens of the United States and increasingly organized to gain enforcement of their constitutional rights.
The Civil Rights Movement had many roots in religion, and the strong community of churches helped supply volunteers and moral purpose for their activism. Mississippi was a center of activity, based in black churches, to educate and register black voters, and to work for integration. In 1954 the state had created the Mississippi State Sovereignty Commission, a tax-supported agency, chaired by the Governor, that claimed to work for the state's image but effectively spied on activists and passed information to the local White Citizens' Councils to suppress black activism. White Citizens Councils had been formed in many cities and towns to resist integration of schools following the unanimous 1954 United States Supreme Court ruling (Brown v. Board of Education) that segregation of public schools was unconstitutional. They used intimidation and economic blackmail against activists and suspected activists, including teachers and other professionals. Techniques included loss of jobs and eviction from rental housing.
In the summer of 1964 students and community organizers from across the country came to help register black voters in Mississippi and establish Freedom Schools. The Mississippi Freedom Democratic Party was established to challenge the all-white Democratic Party of the Solid South. Most white politicians resisted such changes. Chapters of the Ku Klux Klan and its sympathizers used violence against activists, most notably the murders of Chaney, Goodman, and Schwerner in 1964 during the Freedom Summer campaign. This was a catalyst for Congressional passage the following year of the Voting Rights Act of 1965. Mississippi earned a reputation in the 1960s as a reactionary state.[56][57]
After decades of disenfranchisement, African Americans in the state gradually began to exercise their right to vote again for the first time since the 19th century, following the passage of federal civil rights legislation in 1964 and 1965, which ended de jure segregation and enforced constitutional voting rights. Registration of African-American voters increased and black candidates ran in the 1967 elections for state and local offices. The Mississippi Freedom Democratic Party fielded some candidates. Teacher Robert G. Clark of Holmes County was the first African American to be elected to the State House since Reconstruction. He continued as the only African American in the state legislature until 1976 and was repeatedly elected into the 21st century, including three terms as Speaker of the House.[58]
In 1966, the state was the last to repeal officially statewide prohibition of alcohol. Before that, Mississippi had taxed the illegal alcohol brought in by bootleggers. Governor Paul Johnson urged repeal and the sheriff ""raided the annual Junior League Mardi Gras ball at the Jackson Country Club, breaking open the liquor cabinet and carting off the Champagne before a startled crowd of nobility and high-ranking state officials"".[59]
On August 17, 1969, Category 5 Hurricane Camille hit the Mississippi coast, killing 248 people and causing US$1.5 billion in damage (1969 dollars).
Mississippi ratified the Nineteenth Amendment to the United States Constitution, in March 1984, which had already entered into force by August 1920; granting women the right to vote.[60]
In 1987, 20 years after the U.S. Supreme Court had ruled in 1967's Loving v. Virginia that a similar Virginian law was unconstitutional, Mississippi repealed its ban on interracial marriage (also known as miscegenation), which had been enacted in 1890. It also repealed the segregationist-era poll tax in 1989. In 1995, the state symbolically ratified the Thirteenth Amendment, which had abolished slavery in 1865. Though ratified in 1995, the state never officially notified the Federal Archivist, which kept the ratification unofficial until 2013, when Ken Sullivan contacted the office of Secretary of State of Mississippi, Delbert Hosemann, who agreed to file the paperwork and make it official.[61][62][63] In 2009, the legislature passed a bill to repeal other discriminatory civil rights laws, which had been enacted in 1964, the same year as the federal Civil Rights Act, but ruled unconstitutional in 1967 by federal courts. Republican Governor Haley Barbour signed the bill into law.[64]
The end of legal segregation and Jim Crow led to the integration of some churches, but most today remain divided along racial and cultural lines, having developed different traditions. After the Civil War, most African Americans left white churches to establish their own independent congregations, particularly Baptist churches, establishing state associations and a national association by the end of the century. They wanted to express their own traditions of worship and practice.[65] In more diverse communities, such as Hattiesburg, some churches have multiracial congregations.[66]
On August 29, 2005, Hurricane Katrina, though a Category 3 storm upon final landfall, caused even greater destruction across the entire 90 miles (145 km) of the Mississippi Gulf Coast from Louisiana to Alabama.
The previous flag of Mississippi, used until June 30, 2020, featured the Confederate battle flag. Mississippi became the last state to remove the Confederate battle flag as an official state symbol on June 30, 2020, when Governor Tate Reeves signed a law officially retiring the second state flag. A new flag, The ""New Magnolia"" flag, was selected via referendum as part of the general election on November 3, 2020.[67][68] It officially became the state flag on January 11, 2021, after being signed into law by the state legislature and governor.
The center of population of Mississippi is located in Leake County, in the town of Lena.[71]
The United States Census Bureau estimates that the population of Mississippi was 2,976,149 on July 1, 2019, a 0.30% increase since the 2010 census.[70] The state's economist characterized the state as losing population as job markets elsewhere have caused 3.2 per 1000 to migrate recently.[72]
From 2000 to 2010, the United States Census Bureau reported that Mississippi had the highest rate of increase in people identifying as mixed-race, up 70 percent in the decade; it amounts to a total of 1.1 percent of the population.[66] In addition, Mississippi led the nation for most of the last decade in the growth of mixed marriages among its population. The total population has not increased significantly, but is young. Some of the above change in identification as mixed race is due to new births. But, it appears mostly to reflect those residents who have chosen to identify as more than one race, who in earlier years may have identified by just one ethnicity. A binary racial system had been in place since slavery times and the days of racial segregation. In the civil rights era, people of African descent banded together in an inclusive community to achieve political power and gain restoration of their civil rights.
As the demographer William H. Frey noted, ""In Mississippi, I think it's [identifying as mixed race] changed from within.""[66] Historically in Mississippi, after Indian removal in the 1830s, the major groups were designated as black (African American), who were then mostly enslaved, and white (primarily European American). Matthew Snipp, also a demographer, commented on the increase in the 21st century in the number of people identifying as being of more than one race: ""In a sense, they're rendering a more accurate portrait of their racial heritage that in the past would have been suppressed.""[66]
After having accounted for a majority of the state's population since well before the Civil War and through the 1930s, today African Americans constitute approximately 37.8 percent of the state's population. Most have ancestors who were enslaved, with many forcibly transported from the Upper South in the 19th century to work on the area's new plantations. Some of these slaves were mixed race, with European ancestors, as there were many children born into slavery with white fathers. Some also have Native American ancestry.[73] During the first half of the 20th century, a total of nearly 400,000 African Americans left the state during the Great Migration, for opportunities in the North, Midwest and West. They became a minority in the state for the first time since early in its development.[74]
At the 2010 U.S. census, the racial makeup of the population was:
Ethnically, 2.7% of the total population, among all racial groups, was of Hispanic or Latino origin (they may be of any race).[75] As of 2011, 53.8% of Mississippi's population younger than age 1 were minorities, meaning that they had at least one parent who was not non-Hispanic white.[76] For more information on racial and ethnic classifications in the United States see race and ethnicity in the United States Census.
Americans of Scots-Irish, English and Scottish ancestry are present throughout the state. It is believed that there are more people with such ancestry than identify as such on the census, in part because their immigrant ancestors are more distant in their family histories. English, Scottish and Scots-Irish are generally the most under-reported ancestry groups in both the South Atlantic States and the East South Central States. The historian David Hackett Fischer estimated that a minimum 20% of Mississippi's population is of English ancestry, though the figure is probably much higher, and another large percentage is of Scottish ancestry. Many Mississippians of such ancestry identify simply as American on questionnaires, because their families have been in North America for centuries.[80][81] In the 1980 census 656,371 Mississippians of a total of 1,946,775 identified as being of English ancestry, making them 38% of the state at the time.[82]
The state in 2010 had the highest proportion of African Americans in the nation. The African-American percentage of population has begun to increase due mainly to a younger population than the whites (the total fertility rates of the two races are approximately equal). Due to patterns of settlement and whites putting their children in private schools, in almost all of Mississippi's public school districts, a majority of students are African American. African Americans are the majority ethnic group in the northwestern Yazoo Delta, and the southwestern and the central parts of the state. These are areas where, historically, African Americans owned land as farmers in the 19th century following the Civil War, or worked on cotton plantations and farms.[83]
People of French Creole ancestry form the largest demographic group in Hancock County on the Gulf Coast. The African-American; Choctaw, mostly in Neshoba County; and Chinese American portions of the population are also almost entirely native born.
The Chinese first came to Mississippi as contract workers from Cuba and California in the 1870s, and they originally worked as laborers on the cotton plantations. However, most Chinese families came later between 1910 and 1930 from other states, and most operated small family-owned groceries stores in the many small towns of the Delta.[84] In these roles, the ethnic Chinese carved out a niche in the state between black and white, where they were concentrated in the Delta. These small towns have declined since the late 20th century, and many ethnic Chinese have joined the exodus to larger cities, including Jackson. Their population in the state overall has increased in the 21st century.[85][86][87][88]
In the early 1980s many Vietnamese immigrated to Mississippi and other states along the Gulf of Mexico, where they became employed in fishing-related work.[89]
In 2000, 96.4% of Mississippi residents five years old and older spoke only English in the home, a decrease from 97.2% in 1990.[90] English is largely Southern American English, with some South Midland speech in northern and eastern Mississippi. There is a common absence of final /r/, particularly in the elderly natives and African Americans, and the lengthening and weakening of the diphthongs /aɪ/ and /ɔɪ/ as in 'ride' and 'oil'. South Midland terms in northern Mississippi include: tow sack (burlap bag), dog irons (andirons), plum peach (clingstone peach), snake doctor (dragonfly), and stone wall (rock fence).[90]
Under French and Spanish rule beginning in the 17th century, European colonists were mostly Roman Catholics. The growth of the cotton culture after 1815 brought in tens of thousands of Anglo-American settlers each year, most of whom were Protestants from Southeastern states. Due to such migration, there was rapid growth in the number of Protestant churches, especially Methodist, Presbyterian and Baptist.[92]
The revivals of the Great Awakening in the late 18th and early 19th centuries initially attracted the ""plain folk"" by reaching out to all members of society, including women and blacks. Both slaves and free blacks were welcomed into Methodist and Baptist churches. Independent black Baptist churches were established before 1800 in Virginia, Kentucky, South Carolina and Georgia, and later developed in Mississippi as well.
In the post-Civil War years, religion became more influential as the South became known as the ""Bible Belt"".
Since the 1970s, fundamentalist conservative churches have grown rapidly, fueling Mississippi's conservative political trends among whites.[92] In 1973 the Presbyterian Church in America attracted numerous conservative congregations. As of 2010, Mississippi remained a stronghold of the denomination, which originally was brought by Scots immigrants. The state has the highest adherence rate of the PCA in 2010, with 121 congregations and 18,500 members. It is among the few states where the PCA has higher membership than the PC(USA).[93]
According to the Association of Religion Data Archives (ARDA), in 2010 the Southern Baptist Convention had 907,384 adherents and was the largest religious denomination in the state, followed by the United Methodist Church with 204,165, and the Roman Catholic Church with 112,488.[94] Other religions have a small presence in Mississippi; as of 2010, there were 5,012 Muslims; 4,389 Hindus; and 816 of the Baháʼí Faith.[94]
Public opinion polls have consistently ranked Mississippi as the most religious state in the United States, with 59% of Mississippians considering themselves ""very religious"". The same survey also found that 11% of the population were non-Religious.[95] In a 2009 Gallup poll, 63% of Mississippians said that they attended church weekly or almost weekly—the highest percentage of all states (U.S. average was 42%, and the lowest percentage was in Vermont at 23%).[96] Another 2008 Gallup poll found that 85% of Mississippians considered religion an important part of their daily lives, the highest figure among all states (U.S. average 65%).[97]
Note: Births in table don't add up, because Hispanics are counted both by their ethnicity and by their race, giving a higher overall number.
The 2010 United States Census counted 6,286 same-sex unmarried-partner households in Mississippi, an increase of 1,512 since the 2000 United States census.[105] Of those same-sex couples roughly 33% contained at least one child, giving Mississippi the distinction of leading the nation in the percentage of same-sex couples raising children.[106] Mississippi has the largest percentage of African-American same-sex couples among total households. The state capital, Jackson, ranks tenth in the nation in concentration of African-American same-sex couples. The state ranks fifth in the nation in the percentage of Hispanic same-sex couples among all Hispanic households and ninth in the highest concentration of same-sex couples who are seniors.[107]
The state is ranked 50th or last place among all the states for health care, according to the Commonwealth Fund, a nonprofit foundation working to advance performance of the health care system.[108]
Mississippi has the highest rate of infant and neonatal deaths of any U.S. state. Age-adjusted data also shows Mississippi has the highest overall death rate, and the highest death rate from heart disease, hypertension and hypertensive renal disease, influenza and pneumonia.[109]
In 2011, Mississippi (and Arkansas) had the fewest dentists per capita in the United States.[110]
For three years in a row, more than 30 percent of Mississippi's residents have been classified as obese. In a 2006 study, 22.8 percent of the state's children were classified as such. Mississippi had the highest rate of obesity of any U.S. state from 2005 to 2008, and also ranks first in the nation for high blood pressure, diabetes, and adult inactivity.[111][112] In a 2008 study of African-American women, contributing risk factors were shown to be: lack of knowledge about body mass index (BMI), dietary behavior, physical inactivity and lack of social support, defined as motivation and encouragement by friends.[113] A 2002 report on African-American adolescents noted a 1999 survey which suggests that a third of children were obese, with higher ratios for those in the Delta.[114]
The study stressed that ""obesity starts in early childhood extending into the adolescent years and then possibly into adulthood"". It noted impediments to needed behavioral modification, including the Delta likely being ""the most underserved region in the state"" with African Americans the major ethnic group; lack of accessibility and availability of medical care; and an estimated 60% of residents living below the poverty level. Additional risk factors were that most schools had no physical education curriculum and nutrition education is not emphasized. Previous intervention strategies may have been largely ineffective due to not being culturally sensitive or practical.[114] A 2006 survey found nearly 95 percent of Mississippi adults considered childhood obesity to be a serious problem.[115]
A 2017 study found that Blue Cross and Blue Shield of Mississippi was the leading health insurer with 53% followed by UnitedHealth Group at 13%.[116]
The Bureau of Economic Analysis estimates that Mississippi's total state product in 2010 was $98 billion.[117] GDP growth was .5 percent in 2015 and is estimated to be 2.4 in 2016 according to Dr. Darrin Webb, the state's chief economist, who noted it would make two consecutive years of positive growth since the recession.[118] Per capita personal income in 2006 was $26,908, the lowest per capita personal income of any state, but the state also has the nation's lowest living costs. 2015 data records the adjusted per capita personal income at $40,105.[118] Mississippians consistently rank as one of the highest per capita in charitable contributions.[119]
At 56 percent, the state has one of the lowest workforce participation rates in the country. Approximately 70,000 adults are disabled, which is 10 percent of the workforce.[118]
Mississippi's rank as one of the poorest states is related to its dependence on cotton agriculture before and after the Civil War, late development of its frontier bottomlands in the Mississippi Delta, repeated natural disasters of flooding in the late 19th and early 20th century that required massive capital investment in levees, and ditching and draining the bottomlands, and slow development of railroads to link bottomland towns and river cities.[120] In addition, when Democrats regained control of the state legislature, they passed the 1890 constitution that discouraged corporate industrial development in favor of rural agriculture, a legacy that would slow the state's progress for years.[121]
Before the Civil War, Mississippi was the fifth-wealthiest state in the nation, its wealth generated by the labor of slaves in cotton plantations along the rivers.[122]
Slaves were counted as property and the rise in the cotton markets since the 1840s had increased their value. By 1860, a majority—55 percent—of the population of Mississippi was enslaved.[123] Ninety percent of the Delta bottomlands were undeveloped and the state had low overall density of population.
Largely due to the domination of the plantation economy, focused on the production of agricultural cotton, the state's elite was reluctant to invest in infrastructure such as roads and railroads. They educated their children privately. Industrialization did not reach many areas until the late 20th century. The planter aristocracy, the elite of antebellum Mississippi, kept the tax structure low for their own benefit, making only private improvements. Before the war the most successful planters, such as Confederate President Jefferson Davis, owned riverside properties along the Mississippi and Yazoo rivers in the Mississippi Delta. Away from the riverfronts, most of the Delta was undeveloped frontier.
During the Civil War, 30,000 Mississippi soldiers, mostly white, died from wounds and disease, and many more were left crippled and wounded. Changes to the labor structure and an agricultural depression throughout the South caused severe losses in wealth. In 1860 assessed valuation of property in Mississippi had been more than $500 million, of which $218 million (43 percent) was estimated as the value of slaves. By 1870, total assets had decreased in value to roughly $177 million.[124]
Poor whites and landless former slaves suffered the most from the postwar economic depression. The constitutional convention of early 1868 appointed a committee to recommend what was needed for relief of the state and its citizens. The committee found severe destitution among the laboring classes.[125] It took years for the state to rebuild levees damaged in battles. The upset of the commodity system impoverished the state after the war. By 1868 an increased cotton crop began to show possibilities for free labor in the state, but the crop of 565,000 bales produced in 1870 was still less than half of prewar figures.[126]
Blacks cleared land, selling timber and developing bottomland to achieve ownership. In 1900, two-thirds of farm owners in Mississippi were blacks, a major achievement for them and their families. Due to the poor economy, low cotton prices and difficulty of getting credit, many of these farmers could not make it through the extended financial difficulties. Two decades later, the majority of African Americans were sharecroppers. The low prices of cotton into the 1890s meant that more than a generation of African Americans lost the result of their labor when they had to sell their farms to pay off accumulated debts.[44]
After the Civil War, the state refused for years to build human capital by fully educating all its citizens. In addition, the reliance on agriculture grew increasingly costly as the state suffered loss of cotton crops due to the devastation of the boll weevil in the early 20th century, devastating floods in 1912–1913 and 1927, collapse of cotton prices after 1920, and drought in 1930.[120]
It was not until 1884, after the flood of 1882, that the state created the Mississippi-Yazoo Delta District Levee Board and started successfully achieving longer-term plans for levees in the upper Delta.[26] Despite the state's building and reinforcing levees for years, the Great Mississippi Flood of 1927 broke through and caused massive flooding of 27,000 square miles (70,000 km2) throughout the Delta, homelessness for hundreds of thousands, and millions of dollars in property damages. With the Depression coming so soon after the flood, the state suffered badly during those years. In the Great Migration, hundreds of thousands of African Americans migrated North and West for jobs and chances to live as full citizens.
The legislature's 1990 decision to legalize casino gambling along the Mississippi River and the Gulf Coast has led to increased revenues and economic gains for the state. Gambling towns in Mississippi have attracted increased tourism: they include the Gulf Coast resort towns of Bay St. Louis, Gulfport and Biloxi, and the Mississippi River towns of Tunica (the third largest gaming area in the United States), Greenville, Vicksburg and Natchez.
Before Hurricane Katrina struck the Gulf Coast, Mississippi was the second-largest gambling state in the Union, after Nevada and ahead of New Jersey.[citation needed] An estimated $500,000 per day in tax revenue was lost following Hurricane Katrina's severe damage to several coastal casinos in Biloxi in August 2005.[127] Because of the destruction from this hurricane, on October 17, 2005, Governor Haley Barbour signed a bill into law that allows casinos in Hancock and Harrison counties to rebuild on land (but within 800 feet (240 m) of the water). The only exception is in Harrison County, where the new law states that casinos can be built to the southern boundary of U.S. Route 90.[citation needed]
In 2012, Mississippi had the sixth largest gambling revenue of any state, with $2.25 billion.[128] The federally recognized Mississippi Band of Choctaw Indians has established a gaming casino on its reservation, which yields revenue to support education and economic development.[citation needed]
Momentum Mississippi, a statewide, public–private partnership dedicated to the development of economic and employment opportunities in Mississippi, was adopted in 2005.[129]
Mississippi, like the rest of its southern neighbors, is a right-to-work state. It has some major automotive factories, such as the Toyota Mississippi Plant in Blue Springs and a Nissan Automotive plant in Canton. The latter produces the Nissan Titan.
Mississippi collects personal income tax in three tax brackets, ranging from 3% to 5%. The retail sales tax rate in Mississippi is 7%. Tupelo levies a local sales tax of 2.5%.[130] State sales tax growth was 1.4 percent in 2016 and estimated to be slightly less in 2017.[118] For purposes of assessment for ad valorem taxes, taxable property is divided into five classes.[131]
On August 30, 2007, a report by the United States Census Bureau indicated that Mississippi was the poorest state in the country. Major cotton farmers in the Delta have large, mechanized plantations, and they receive the majority of extensive federal subsidies going to the state, yet many other residents still live as poor, rural, landless laborers. The state's sizable poultry industry has faced similar challenges in its transition from family-run farms to large mechanized operations.[132] Of $1.2 billion from 2002 to 2005 in federal subsidies to farmers in the Bolivar County area of the Delta, only 5% went to small farmers. There has been little money apportioned for rural development. Small towns are struggling. More than 100,000 people have left the region in search of work elsewhere.[133] The state had a median household income of $34,473.[134]
As of December 2018, the state's unemployment rate was 4.7%, the seventh highest in the country after Arizona (4.9%), Louisiana (4.9%), New Mexico (5.0%), West Virginia (5.1%), District of Columbia (5.4%) and Alaska (6.5%).[135]
With Mississippi's fiscal conservatism, in which Medicaid, welfare, food stamps, and other social programs are often cut, eligibility requirements are tightened, and stricter employment criteria are imposed, Mississippi ranks as having the second-highest ratio of spending to tax receipts of any state. In 2005, Mississippi citizens received approximately $2.02 per dollar of taxes in the way of federal spending. This ranks the state second-highest nationally, and represents an increase from 1995, when Mississippi received $1.54 per dollar of taxes in federal spending and was 3rd highest nationally.[136] This figure is based on federal spending after large portions of the state were devastated by Hurricane Katrina, requiring large amounts of federal aid from the Federal Emergency Management Agency (FEMA). However, from 1981 to 2005, it was at least number four in the nation for federal spending vs. taxes received.[137]
A proportion of federal spending in Mississippi is directed toward large federal installations such as Camp Shelby, John C. Stennis Space Center, Meridian Naval Air Station, Columbus Air Force Base, and Keesler Air Force Base. Three of these installations are located in the area affected by Hurricane Katrina.
As with all other U.S. states and the federal government, Mississippi's government is based on the separation of legislative, executive and judicial power. Executive authority in the state rests with the Governor, currently Tate Reeves (R). The lieutenant governor, currently Delbert Hosemann (R), is elected on a separate ballot. Both the governor and lieutenant governor are elected to four-year terms of office. Unlike the federal government, but like many other U.S. States, most of the heads of major executive departments are elected by the citizens of Mississippi rather than appointed by the governor.
Mississippi is one of five states that elects its state officials in odd-numbered years (the others are Kentucky, Louisiana, New Jersey and Virginia). Mississippi holds elections for these offices every four years, always in the year preceding presidential elections.
In 2004, Mississippi voters approved a state constitutional amendment banning same-sex marriage and prohibiting Mississippi from recognizing same-sex marriages performed elsewhere. The amendment passed 86% to 14%, the largest margin in any state.[138][139] Same-sex marriage became legal in Mississippi on June 26, 2015, when the United States Supreme Court invalidated all state-level bans on same-sex marriage as unconstitutional in the landmark case Obergefell v. Hodges.[140]
With the passing of HB 1523 in April 2016, from July it became legal in Mississippi to refuse service to same-sex couples, based on one's religious beliefs.[141][142] The bill has become the subject of controversy.[143] A federal judge blocked the law in July,[144] however it was challenged and a federal appeals court ruled in favor of the law in October 2017.[145][146]
Mississippi has banned sanctuary cities.[147] Mississippi is one of thirty-one states which have capital punishment (see Capital punishment in Mississippi).
Section 265 of the Constitution of the State of Mississippi declares that ""No person who denies the existence of a Supreme Being shall hold any office in this state.""[148] This religious test restriction was held to be unconstitutional by the U.S. Supreme Court in Torcaso v. Watkins (1961).
Mississippi led the South in developing a disenfranchising constitution, passing it in 1890. By raising barriers to voter registration, the state legislature disenfranchised most blacks and many poor whites, excluding them from politics until the late 1960s. It established a one-party state dominated by white Democrats.
In the 1980s whites divided evenly between the parties. In the 1990s those voters shifted their allegiance to the Republican Party, first for national and then for state offices.[149] Most blacks were still disenfranchised under the state's 1890 constitution and discriminatory practices, until passage of the Voting Rights Act of 1965 and concerted grassroots efforts to achieve registration and encourage voting.
In 2019, a lawsuit was filed against an 1890 election law known as The Mississippi Plan, which requires that candidates must win the popular vote and a majority of districts.[150] In the following year, 79%  Missisippians voted to remove the requirement of doing so.[151]
Mississippi has six airports with commercial passenger service, the busiest in Jackson (Jackson-Evers International Airport) and one in Gulfport (Gulfport-Biloxi International Airport)
Mississippi is the only American state where people in cars may legally consume beer. Some localities have laws restricting the practice.[152] In 2018, the state was ranked number eight in the Union in terms of impaired driving deaths.[153]
Mississippi is served by nine interstate highways:

and fourteen main U.S. Routes:

as well as a system of State Highways.
Amtrak provides scheduled passenger service along two routes, the Crescent and City of New Orleans. Prior to severe damage from Hurricane Katrina, the Sunset Limited traversed the far south of the state; the route originated in Los Angeles, California and it terminated in Florida.
All but two of the United States Class I railroads serve Mississippi (the exceptions are the Union Pacific and Canadian Pacific):
http://www.mvk.usace.army.mil/Missions/Recreation/Enid-Lake/
Until the Civil War era, Mississippi had a small number of schools and no educational institutions for African Americans. The first school for black students was not established until 1862.
During Reconstruction in 1871, black and white Republicans drafted a constitution that was the first to provide for a system of free public education in the state. The state's dependence on agriculture and resistance to taxation limited the funds it had available to spend on any schools. In the early 20th century, there were still few schools in rural areas, particularly for black children. With seed money from the Julius Rosenwald Fund, many rural black communities across Mississippi raised matching funds and contributed public funds to build new schools for their children. Essentially, many black adults taxed themselves twice and made significant sacrifices to raise money for the education of children in their communities, in many cases donating land and/or labor to build such schools.[157]
Blacks and whites attended segregated and separate public schools in Mississippi until the late 1960s, although such segregation had been declared unconstitutional by the United States Supreme Court in its 1954 ruling in Brown v. Board of Education. In the majority-black Mississippi Delta counties, white parents worked through White Citizens' Councils to set up private segregation academies, where they enrolled their children. Often funding declined for the public schools.[158]
But in the state as a whole, only a small minority of white children were withdrawn from public schools. State officials believed they needed to maintain public education to attract new businesses. After several years of integration, whites often dominated local systems anyway, maintaining white supremacy. Many black parents complained that they had little representation in school administration, and that many of their former administrators and teachers had been pushed out. They have had to work to have their interests and children represented.[158]
In the late 1980s Mississippi's 954 public schools enrolled about 369,500 elementary and 132,500 secondary students. Some 45,700 students attended private schools.
In the 21st century, 91% of white children and most of the black children in the state attend public schools.[159] In 2008, Mississippi was ranked last among the fifty states in academic achievement by the American Legislative Exchange Council's Report Card on Education,[160] with the lowest average ACT scores and sixth-lowest spending per pupil in the nation. In contrast, Mississippi had the 17th-highest average SAT scores in the nation. As an explanation, the Report noted that 92% of Mississippi high school graduates took the ACT, but only 3% of graduates took the SAT, apparently a self-selection of higher achievers. This breakdown compares to the national average of high school graduates taking the ACT and SAT, of 43% and 45%, respectively.[160]
Generally prohibited in the West at large, school corporal punishment is not unusual in Mississippi, with 31,236 public school students[161] paddled at least one time circa 2016.[162] A greater percentage of students were paddled in Mississippi than in any other state, according to government data for the 2011–2012 school year.[162]
In 2007, Mississippi students scored the lowest of any state on the National Assessments of Educational Progress in both math and science.[163]
Jackson, the state's capital city, is the site of the state residential school for deaf and hard of hearing students. The Mississippi School for the Deaf was established by the state legislature in 1854 before the civil war.
While Mississippi has been especially known for its music and literature, it has embraced other forms of art. Its strong religious traditions have inspired striking works by outsider artists who have been shown nationally.
Jackson established the USA International Ballet Competition, which is held every four years. This ballet competition attracts the most talented young dancers from around the world.[164]
The Magnolia Independent Film Festival, still held annually in Starkville, is the first and oldest in the state.
George Ohr, known as the ""Mad Potter of Biloxi"" and the father of abstract expressionism in pottery, lived and worked in Biloxi, MS.
Musicians of the state's Delta region were historically significant to the development of the blues. Although by the end of the 19th century, two-thirds of the farm owners were black, continued low prices for cotton and national financial pressures resulted in most of them losing their land. More problems built up with the boll weevil infestation, when thousands of agricultural jobs were lost.
Jimmie Rodgers, a native of Meridian and guitarist/singer/songwriter known as the ""Father of Country Music"", played a significant role in the development of the blues. He and Chester Arthur Burnett were friends and admirers of each other's music. Their friendship and respect is an important example of Mississippi's musical legacy. While the state has had a reputation for being racist, Mississippi musicians created new forms by combining and creating variations on musical traditions from Africa, African American traditions, and the musical traditions of white Southerners strongly shaped by Scots-Irish and other styles.
The state is creating a Mississippi Blues Trail, with dedicated markers explaining historic sites significant to the history of blues music, such as Clarksdale's Riverside Hotel, where Bessie Smith died after her auto accident on Highway 61. The Riverside Hotel is just one of many historical blues sites in Clarksdale. The Delta Blues Museum there is visited by tourists from all over the world. Close by is ""Ground Zero"", a contemporary blues club and restaurant co-owned by actor Morgan Freeman.
Elvis Presley, who created a sensation in the 1950s as a crossover artist and contributed to rock 'n' roll, was a native of Tupelo. From opera star Leontyne Price to the alternative rock band 3 Doors Down, to gulf and western singer Jimmy Buffett, modern rock/jazz/world music guitarist-producer Clifton Hyde, to rappers David Banner, Big K.R.I.T. and Afroman, Mississippi musicians have been significant in all genres.
Coordinates: .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}32°44′11″N 89°40′04″W﻿ / ﻿32.7364°N 89.6678°W﻿ / 32.7364; -89.6678﻿ (State of Mississippi)
"
Hypertension - Wikipedia," 
Hypertension (HTN or HT), also known as high blood pressure (HBP), is a long-term medical condition in which the blood pressure in the arteries is persistently elevated.[10]  High blood pressure typically does not cause symptoms.[1] Long-term high blood pressure, however, is a major risk factor for coronary artery disease, stroke, heart failure, atrial fibrillation, peripheral arterial disease, vision loss, chronic kidney disease, and dementia.[2][3][4][11]
High blood pressure is classified as primary (essential) hypertension or secondary hypertension.[5] About 90–95% of cases are primary, defined as high blood pressure due to nonspecific lifestyle and genetic factors.[5][6] Lifestyle factors that increase the risk include excess salt in the diet, excess body weight, smoking, and alcohol use.[1][5] The remaining 5–10% of cases are categorized as secondary high blood pressure, defined as high blood pressure due to an identifiable cause, such as chronic kidney disease, narrowing of the kidney arteries, an endocrine disorder, or the use of birth control pills.[5]
Blood pressure is expressed by two measurements, the systolic and diastolic pressures, which are the maximum and minimum pressures, respectively.[1] For most adults, normal blood pressure at rest is within the range of 100–130 millimeters mercury (mmHg) systolic and 60–80 mmHg diastolic.[7][12] For most adults, high blood pressure is present if the resting blood pressure is persistently at or above 130/80 or 140/90 mmHg.[5][7] Different numbers apply to children.[13] Ambulatory blood pressure monitoring over a 24-hour period appears more accurate than office-based blood pressure measurement.[5][10]
Lifestyle changes and medications can lower blood pressure and decrease the risk of health complications.[8] Lifestyle changes include weight loss, physical exercise, decreased salt intake, reducing alcohol intake, and a healthy diet.[5] If lifestyle changes are not sufficient then blood pressure medications are used.[8] Up to three medications taken concurrently can control blood pressure in 90% of people.[5] The treatment of moderately high arterial blood pressure (defined as >160/100 mmHg) with medications is associated with an improved life expectancy.[14] The effect of treatment of blood pressure between 130/80 mmHg and 160/100 mmHg is less clear, with some reviews finding benefit[7][15][16] and others finding unclear benefit.[17][18][19] High blood pressure affects between 16 and 37% of the population globally.[5] In 2010 hypertension was believed to have been a factor in 18% of all deaths (9.4 million globally).[9]
Hypertension is rarely accompanied by symptoms, and its identification is usually through screening, or when seeking healthcare for an unrelated problem. Some people with high blood pressure report headaches (particularly at the back of the head and in the morning), as well as lightheadedness, vertigo, tinnitus (buzzing or hissing in the ears), altered vision or fainting episodes.[20] These symptoms, however, might be related to associated anxiety rather than the high blood pressure itself.[21]
On physical examination, hypertension may be associated with the presence of changes in the optic fundus seen by  ophthalmoscopy.[22] The severity of the changes typical of hypertensive retinopathy is graded from I to IV; grades I and II may be difficult to differentiate.[22] The severity of the retinopathy correlates roughly with the duration or the severity of the hypertension.[20]
Hypertension with certain specific additional signs and symptoms may suggest secondary hypertension, i.e. hypertension due to an identifiable cause. For example, Cushing's syndrome frequently causes truncal obesity, glucose intolerance, moon face, a hump of fat behind the neck/shoulder (referred to as a buffalo hump), and purple abdominal stretch marks.[23] Hyperthyroidism frequently causes weight loss with increased appetite, fast heart rate, bulging eyes, and tremor. Renal artery stenosis (RAS) may be associated with a localized abdominal bruit to the left or right of the midline (unilateral RAS), or in both locations (bilateral RAS). Coarctation of the aorta  frequently causes a decreased blood pressure in the lower extremities relative to the arms, or delayed or absent femoral arterial pulses. Pheochromocytoma may cause abrupt (""paroxysmal"") episodes of hypertension accompanied by headache, palpitations, pale appearance, and excessive sweating.[23]
Severely elevated blood pressure (equal to or greater than a systolic 180 or diastolic of 110) is referred to as a hypertensive crisis. Hypertensive crisis is categorized as either hypertensive urgency or hypertensive emergency, according to the absence or presence of end organ damage, respectively.[24][25]
In hypertensive urgency, there is no evidence of end organ damage resulting from the elevated blood pressure. In these cases, oral medications are used to lower the BP gradually over 24 to 48 hours.[26]
In hypertensive emergency, there is evidence of direct damage to one or more organs.[27][28] The most affected organs include the brain, kidney, heart and lungs, producing symptoms which may include confusion, drowsiness, chest pain and breathlessness.[26] In hypertensive emergency, the blood pressure must be reduced more rapidly to stop ongoing organ damage,[26] however, there is a lack of randomized controlled trial evidence for this approach.[28]
Hypertension occurs in approximately 8–10% of pregnancies.[23] Two blood pressure measurements six hours apart of greater than 140/90 mm Hg are diagnostic of hypertension in pregnancy.[29] High blood pressure in pregnancy can be classified as pre-existing hypertension, gestational hypertension, or pre-eclampsia.[30]
Pre-eclampsia is a serious condition of the second half of pregnancy and following delivery characterised by increased blood pressure and the presence of protein in the urine.[23] It occurs in about 5% of pregnancies and is responsible for approximately 16% of all maternal deaths globally.[23] Pre-eclampsia also doubles the risk of death of the baby around the time of birth.[23] Usually there are no symptoms in pre-eclampsia and it is detected by routine screening. When symptoms of pre-eclampsia occur the most common are headache, visual disturbance (often ""flashing lights""), vomiting, pain over the stomach, and swelling. Pre-eclampsia can occasionally progress to a life-threatening condition called eclampsia, which is a hypertensive emergency and has several serious complications including vision loss, brain swelling, seizures, kidney failure, pulmonary edema, and disseminated intravascular coagulation (a blood clotting disorder).[23][31]
In contrast, gestational hypertension is defined as new-onset hypertension during pregnancy without protein in the urine.[30]
Failure to thrive, seizures, irritability, lack of energy, and difficulty in breathing[32] can be associated with hypertension in newborns and young infants. In older infants and children, hypertension can cause headache, unexplained irritability, fatigue, failure to thrive, blurred vision, nosebleeds, and facial paralysis.[32][33]
Hypertension results from a complex interaction of genes and environmental factors. Numerous common genetic variants with small effects on blood pressure have been identified[34] as well as some rare genetic variants with large effects on blood pressure.[35] Also, genome-wide association studies (GWAS) have identified 35 genetic loci related to blood pressure; 12 of these genetic loci influencing blood pressure were newly found.[36] Sentinel SNP for each new genetic locus identified has shown an association with DNA methylation at multiple nearby CpG sites. These sentinel SNP are located within genes related to vascular smooth muscle and renal function. DNA methylation might affect in some way linking common genetic variation to multiple phenotypes even though mechanisms underlying these associations are not understood. Single variant test performed in this study for the 35 sentinel SNP (known and new) showed that genetic variants singly or in aggregate contribute to risk of clinical phenotypes related to high blood pressure.[36]
Blood pressure rises with aging and the risk of becoming hypertensive in later life is significant.[37] Several environmental factors influence blood pressure. High salt intake raises the blood pressure in salt sensitive individuals; lack of exercise, central obesity can play a role in individual cases. The possible roles of other factors such as caffeine consumption,[38] and vitamin D deficiency[39] are less clear. Insulin resistance, which is common in obesity and is a component of syndrome X (or the metabolic syndrome), also contributes to hypertension.[40]
Events in early life, such as low birth weight, maternal smoking, and lack of breastfeeding may be risk factors for adult essential hypertension, although the mechanisms linking these exposures to adult hypertension remain unclear.[41] An increased rate of high blood uric acid has been found in untreated people with hypertension in comparison with people with normal blood pressure, although it is uncertain whether the former plays a causal role or is subsidiary to poor kidney function.[42] Average blood pressure may be higher in the winter than in the summer.[43] Periodontal disease is also associated with high blood pressure.[44]
Secondary hypertension results from an identifiable cause. Kidney disease is the most common secondary cause of hypertension.[23] Hypertension can also be caused by endocrine conditions, such as Cushing's syndrome, hyperthyroidism, hypothyroidism, acromegaly, Conn's syndrome or hyperaldosteronism, renal artery stenosis (from atherosclerosis or fibromuscular dysplasia), hyperparathyroidism, and pheochromocytoma.[23][45] Other causes of secondary hypertension include obesity, sleep apnea, pregnancy, coarctation of the aorta, excessive eating of liquorice, excessive drinking of alcohol, certain prescription medicines, herbal remedies, and stimulants such as cocaine and methamphetamine.[23][46] Arsenic exposure through drinking water has been shown to correlate with elevated blood pressure.[47][48] Depression was also linked to hypertension.[49] Loneliness is also a risk factor.[50]
A 2018 review found that any alcohol increased blood pressure in males while over one or two drinks increased the risk in females.[51]
In most people with established essential hypertension, increased resistance to blood flow (total peripheral resistance) accounts for the high pressure while cardiac output remains normal.[52] There is evidence that some younger people with prehypertension or 'borderline hypertension' have high cardiac output, an elevated heart rate and normal peripheral resistance, termed hyperkinetic borderline hypertension.[53] These individuals develop the typical features of established essential hypertension in later life as their cardiac output falls and peripheral resistance rises with age.[53] Whether this pattern is typical of all people who ultimately develop hypertension is disputed.[54] The increased peripheral resistance in established hypertension is mainly attributable to structural narrowing of small arteries and arterioles,[55] although a reduction in the number or density of capillaries may also contribute.[56]
It is not clear whether or not vasoconstriction of arteriolar blood vessels plays a role in hypertension.[57] Hypertension is also associated with decreased peripheral venous compliance[58] which may increase venous return, increase cardiac preload and, ultimately, cause diastolic dysfunction.
Pulse pressure (the difference between systolic and diastolic blood pressure) is frequently increased in older people with hypertension. This can mean that systolic pressure is abnormally high, but diastolic pressure may be normal or low, a condition termed isolated systolic hypertension.[59] The high pulse pressure in elderly people with hypertension or isolated systolic hypertension is explained by increased arterial stiffness, which typically accompanies aging and may be exacerbated by high blood pressure.[60]
Many mechanisms have been proposed to account for the rise in peripheral resistance in hypertension. Most evidence implicates either disturbances in the kidneys' salt and water handling (particularly abnormalities in the intrarenal renin–angiotensin system)[61] or abnormalities of the sympathetic nervous system.[62] These mechanisms are not mutually exclusive and it is likely that both contribute to some extent in most cases of essential hypertension. It has also been suggested that endothelial dysfunction and vascular inflammation may also contribute to increased peripheral resistance and vascular damage in hypertension.[63][64] Interleukin 17 has garnered interest for its role in increasing the production of several other immune system chemical signals thought to be involved in hypertension such as tumor necrosis factor alpha, interleukin 1, interleukin 6, and interleukin 8.[65]
Excessive sodium or insufficient potassium in the diet leads to excessive intracellular sodium, which contracts vascular smooth muscle, restricting blood flow and so increases blood pressure.[66][67]
Hypertension is diagnosed on the basis of a persistently high resting blood pressure. The American Heart Association recommends at least three resting measurements on at least two separate health care visits.[68] The UK National Institute for Health and Care Excellence recommends ambulatory blood pressure monitoring to confirm the diagnosis of hypertension if a clinic blood pressure is 140/90 mmHg or higher.[69]
For an accurate diagnosis of hypertension to be made, it is essential for proper blood pressure measurement technique to be used.[70] Improper measurement of blood pressure is common and can change the blood pressure reading by up to 10 mmHg, which can lead to misdiagnosis and misclassification of hypertension.[70] Correct blood pressure measurement technique involves several steps. Proper blood pressure measurement requires the person whose blood pressure is being measured to sit quietly for at least five minutes which is then followed by application of a properly fitted blood pressure cuff to a bare upper arm.[70] The person should be seated with their back supported, feet flat on the floor, and with their legs uncrossed.[70] The person whose blood pressure is being measured should avoid talking or moving during this process.[70] The arm being measured should be supported on a flat surface at the level of the heart.[70] Blood pressure measurement should be done in a quiet room so the medical professional checking the blood pressure can hear the Korotkoff sounds while listening to the brachial artery with a stethoscope for accurate blood pressure measurements.[70][71] The blood pressure cuff should be deflated slowly (2-3 mmHg per second) while listening for the Korotkoff sounds.[71] The bladder should be emptied before a person's blood pressure is measured since this can increase blood pressure by up to 15/10 mmHg.[70] Multiple blood pressure readings (at least two) spaced 1–2 minutes apart should be obtained to ensure accuracy.[71] Ambulatory blood pressure monitoring over 12 to 24 hours is the most accurate method to confirm the diagnosis.[72] An exception to this is those with very high blood pressure readings especially when there is poor organ function.[73]
With the availability of 24-hour ambulatory blood pressure monitors and home blood pressure machines, the importance of not wrongly diagnosing those who have white coat hypertension has led to a change in protocols. In the United Kingdom, current best practice is to follow up a single raised clinic reading with ambulatory measurement, or less ideally with home blood pressure monitoring over the course of 7 days.[73] The United States Preventive Services Task Force also recommends getting measurements outside of the healthcare environment.[72] Pseudohypertension in the elderly or noncompressibility artery syndrome may also require consideration. This condition is believed to be due to calcification of the arteries resulting in abnormally high blood pressure readings with a blood pressure cuff while intra arterial measurements of blood pressure are normal.[74] Orthostatic hypertension is when blood pressure increases upon standing.[75]
Once the diagnosis of hypertension has been made, healthcare providers should attempt to identify the underlying cause based on risk factors and other symptoms, if present. Secondary hypertension is more common in preadolescent children, with most cases caused by kidney disease. Primary or essential hypertension is more common in adolescents and adults and has multiple risk factors, including obesity and a family history of hypertension.[82] Laboratory tests can also be performed to identify possible causes of secondary hypertension, and to determine whether hypertension has caused damage to the heart, eyes, and kidneys. Additional tests for diabetes and high cholesterol levels are usually performed because these conditions are additional risk factors for the development of heart disease and may require treatment.[6]
Initial assessment of the hypertensive people should include a complete history and physical examination. Serum creatinine is measured to assess for the presence of kidney disease, which can be either the cause or the result of hypertension. Serum creatinine alone may overestimate glomerular filtration rate and recent guidelines advocate the use of predictive equations such as the Modification of Diet in Renal Disease (MDRD) formula to estimate glomerular filtration rate (eGFR).[27] eGFR can also provide a baseline measurement of kidney function that can be used to monitor for side effects of certain anti-hypertensive drugs on kidney function. Additionally, testing of urine samples for protein is used as a secondary indicator of kidney disease. Electrocardiogram (EKG/ECG) testing is done to check for evidence that the heart is under strain from high blood pressure. It may also show whether there is thickening of the heart muscle (left ventricular hypertrophy) or whether the heart has experienced a prior minor disturbance such as a silent heart attack. A chest X-ray or an echocardiogram may also be performed to look for signs of heart enlargement or damage to the heart.[23]
In people aged 18 years or older hypertension is defined as either a systolic or a diastolic blood pressure measurement consistently higher than an accepted normal value (this is above 129 or 139 mmHg systolic, 89 mmHg diastolic depending on the guideline).[5][7] Other thresholds are used (135 mmHg systolic or 85 mmHg diastolic) if measurements are derived from 24-hour ambulatory or home monitoring.[73] Recent international hypertension guidelines have also created categories below the hypertensive range to indicate a continuum of risk with higher blood pressures in the normal range. The Seventh Report of the Joint National Committee on Prevention, Detection, Evaluation and Treatment of High Blood Pressure (JNC7) published in 2003[27] uses the term prehypertension for blood pressure in the range 120–139 mmHg systolic or 80–89 mmHg diastolic, while European Society of Hypertension Guidelines (2007)[87] and British Hypertension Society (BHS) IV (2004)[88] use optimal, normal and high normal categories to subdivide pressures below 140 mmHg systolic and 90 mmHg diastolic. Hypertension is also sub-classified: JNC7 distinguishes hypertension stage I, hypertension stage II, and isolated systolic hypertension. Isolated systolic hypertension refers to elevated systolic pressure with normal diastolic pressure and is common in the elderly.[27] The ESH-ESC Guidelines (2007)[87] and BHS IV (2004)[88] additionally define a third stage (stage III hypertension) for people with systolic blood pressure exceeding 179 mmHg or a diastolic pressure over 109 mmHg. Hypertension is classified as ""resistant"" if medications do not reduce blood pressure to normal levels.[27] In November 2017, the American Heart Association and American College of Cardiology published a joint guideline which updates the recommendations of the JNC7 report.[89] The 2020 International Society of Hypertension guidelines define hypertension based on office blood pressure ≥140/90 mmHg or home monitoring blood pressure ≥135/85 mmHg, or 24-hour ambulatory blood pressure average ≥130/80 mmHg (daytime average ≥135/85 mmHg or nighttime average BP ≥120/70 mmHg).[90]
Hypertension occurs in around 0.2 to 3% of newborns; however, blood pressure is not measured routinely in healthy newborns.[33] Hypertension is more common in high risk newborns. A variety of factors, such as gestational age, postconceptional age and birth weight needs to be taken into account when deciding if a blood pressure is normal in a newborn.[33]
Hypertension defined as elevated blood pressure over several visits affects 1% to 5% of children and adolescents and is associated with long term risks of ill-health.[91] Blood pressure rises with age in childhood and, in children, hypertension is defined as an average systolic or diastolic blood pressure on three or more occasions equal or higher than the 95th percentile appropriate for the sex, age and height of the child. High blood pressure must be confirmed on repeated visits however before characterizing a child as having hypertension.[91] Prehypertension in children has been defined as average systolic or diastolic blood pressure that is greater than or equal to the 90th percentile, but less than the 95th percentile.[91] In adolescents, it has been proposed that hypertension and pre-hypertension are diagnosed and classified using the same criteria as in adults.[91]
The value of routine screening for hypertension in children over the age of 3 years is debated.[92][93] In 2004 the National High Blood Pressure Education Program recommended that children aged 3 years and older have blood pressure measurement at
least once at every health care visit[91] and the National Heart, Lung, and Blood Institute and American Academy of Pediatrics made a similar recommendation.[94] However, the American Academy of Family Physicians[95] supports the view of the U.S. Preventive Services Task Force that the available evidence is insufficient to determine the balance of benefits and harms of screening for hypertension in children and adolescents who do not have symptoms.[96][97]
Much of the disease burden of high blood pressure is experienced by people who are not labeled as hypertensive.[88] Consequently, population strategies are required to reduce the consequences of high blood pressure and reduce the need for antihypertensive medications. Lifestyle changes are recommended to lower blood pressure, before starting medications. The 2004 British Hypertension Society guidelines[88] proposed lifestyle changes consistent with those outlined by the US National High BP Education Program in 2002[98] for the primary prevention of hypertension:
Effective lifestyle modification may lower blood pressure as much as an individual antihypertensive medication. Combinations of two or more lifestyle modifications can achieve even better results.[88] There is considerable evidence that reducing dietary salt intake lowers blood pressure, but whether this translates into a reduction in mortality and cardiovascular disease remains uncertain.[99] Estimated sodium intake ≥6g/day and <3g/day are both associated with high risk of death or major cardiovascular disease, but the association between high sodium intake and adverse outcomes is only observed in people with hypertension.[100] Consequently, in the absence of results from randomized controlled trials, the wisdom of reducing levels of dietary salt intake below 3g/day has been questioned.[99] ESC guidelines mention periodontitis is associated with poor cardiovascular health status.[101]
According to one review published in 2003, reduction of the blood pressure by 5 mmHg can decrease the risk of stroke by 34%, of ischemic heart disease by 21%, and reduce the likelihood of dementia, heart failure, and mortality from cardiovascular disease.[102]
Various expert groups have produced guidelines regarding how low the blood pressure target should be when a person is treated for hypertension. These groups recommend a target below the range 140–160 / 90–100 mmHg for the general population.[13][12][103][104][105] Cochrane reviews recommend similar targets for subgroups such as people with diabetes[106] and people with prior cardiovascular disease.[107]
Many expert groups recommend a slightly higher target of 150/90 mmHg for those over somewhere between 60 and 80 years of age.[12][103][104][108] The JNC-8 and American College of Physicians recommend the target of 150/90 mmHg for those over 60 years of age,[13][109] but some experts within these groups disagree with this recommendation.[110] Some expert groups have also recommended slightly lower targets in those with diabetes[12] or chronic kidney disease with protein loss in the urine,[111] but others recommend the same target as for the general population.[13][106] The issue of what is the best target and whether targets should differ for high risk individuals is unresolved,[112] although some experts propose more intensive blood pressure lowering than advocated in some guidelines.[113]
For people who have never experienced cardiovascular disease who are at a 10-year risk of cardiovascular disease of less than 10%, the 2017 American Heart Association guidelines recommend medications if the systolic blood pressure is >140 mmHg or if the diastolic BP is >90 mmHg.[7] For people who have experienced cardiovascular disease or those who are at a 10-year risk of cardiovascular disease of greater than 10%, it recommends medications if the systolic blood pressure is >130 mmHg or if the diastolic BP is >80 mmHg.[7]
The first line of treatment for hypertension is lifestyle changes, including dietary changes, physical exercise, and weight loss. Though these have all been recommended in scientific advisories,[114] a  Cochrane systematic review found no evidence for effects of weight loss diets on death, long-term complications or adverse events in persons with hypertension.[115] The review did find a decrease in blood pressure.[115] Their potential effectiveness is similar to and at times exceeds a single medication.[12] If hypertension is high enough to justify immediate use of medications, lifestyle changes are still recommended in conjunction with medication.
Dietary changes shown to reduce blood pressure include diets with low sodium,[116][117] the DASH diet (Dietary Approaches to Stop Hypertension),[118] vegetarian diets,[119] and green tea consumption.[120][121][122][123]
Increasing dietary potassium has a potential benefit for lowering the risk of hypertension.[124][125] The 2015 Dietary Guidelines Advisory Committee (DGAC) stated that potassium is one of the shortfall nutrients which is under-consumed in the United States.[126] However, people who take certain antihypertensive medications (such as ACE-inhibitors or ARBs) should not take potassium supplements or potassium-enriched salts due to the risk of high levels of potassium.[127]
Physical exercise regimens which are shown to reduce blood pressure include isometric resistance exercise, aerobic exercise, resistance exercise, and device-guided breathing.[128]
Stress reduction techniques such as biofeedback or transcendental meditation may be considered as an add-on to other treatments to reduce hypertension, but do not have evidence for preventing cardiovascular disease on their own.[128][129][130] Self-monitoring and appointment reminders might support the use of other strategies to improve blood pressure control, but need further evaluation.[131]
Several classes of medications, collectively referred to as antihypertensive medications, are available for treating hypertension.
First-line medications for hypertension include thiazide-diuretics, calcium channel blockers, angiotensin converting enzyme inhibitors (ACE inhibitors), and angiotensin receptor blockers (ARBs).[132][13] These medications may be used alone or in combination (ACE inhibitors and ARBs are not recommended for use in combination); the latter option may serve to minimize counter-regulatory mechanisms that act to restore blood pressure values to pre-treatment levels.[13][133] Most people require more than one medication to control their hypertension.[114] Medications for blood pressure control should be implemented by a stepped care approach when target levels are not reached.[131]
Previously beta-blockers such as atenolol were thought to have similar beneficial effects when used as first-line therapy for hypertension. However, a Cochrane review that included 13 trials found that the effects of beta-blockers are inferior to that of other antihypertensive medications in preventing cardiovascular disease.[134]
Resistant hypertension is defined as high blood pressure that remains above a target level, in spite of being prescribed three or more antihypertensive drugs simultaneously with different mechanisms of action.[135] Failing to take prescribed medications as directed is an important cause of resistant hypertension.[136] Resistant hypertension may also result from chronically high activity of the autonomic nervous system, an effect known as ""neurogenic hypertension"".[137] Electrical therapies that stimulate the baroreflex are being studied as an option for lowering blood pressure in people in this situation.[138]
Refractory hypertension is characterized by uncontrolled elevated blood pressure unmitigated by five or more antihypertensive agents of different classes, including a long-acting thiazide-like diuretic, a calcium channel blocker, and a blocker of the renin-angiotensin system.[139] People with refractory hypertension typically have increased sympathetic nervous system activity, and are at high risk for more severe cardiovascular diseases and all-cause mortality.[139][140]
As of 2014[update], approximately one billion adults or ~22% of the population of the world have hypertension.[143] It is slightly more frequent in men,[143] in those of low socioeconomic status,[6] and it becomes more common with age.[6] It is common in high, medium, and low income countries.[143][144] In 2004 rates of high blood pressure were highest in Africa, (30% for both sexes) and lowest in the Americas (18% for both sexes). Rates also vary markedly within regions with rates as low as 3.4% (men) and 6.8% (women) in rural India and as high as 68.9% (men) and 72.5% (women) in Poland.[145] Rates in Africa were about 45% in 2016.[146]
In Europe hypertension occurs in about 30-45% of people as of 2013[update].[12] In 1995 it was estimated that 43 million people (24% of the population) in the United States had hypertension or were taking antihypertensive medication.[147] By 2004 this had increased to 29%[148][149] and further to 32% (76 million US adults) by 2017.[7] In 2017, with the change in definitions for hypertension, 46% of people in the United States are affected.[7] African-American adults in the United States have among the highest rates of hypertension in the world at 44%.[150] It is also more common in Filipino Americans and less common in US whites and Mexican Americans.[6][151] Differences in hypertension rates are multifactorial and under study.[152]
Rates of high blood pressure in children and adolescents have increased in the last 20 years in the United States.[153] Childhood hypertension, particularly in pre-adolescents, is more often secondary to an underlying disorder than in adults. Kidney disease is the most common secondary cause of hypertension in children and adolescents. Nevertheless, primary or essential hypertension accounts for most cases.[154]
Hypertension is the most important preventable risk factor for premature death worldwide.[155] It increases the risk of ischemic heart disease,[156] strokes,[23] peripheral vascular disease,[157] and other cardiovascular diseases, including heart failure, aortic aneurysms, diffuse atherosclerosis, chronic kidney disease, atrial fibrillation, and pulmonary embolism.[11][23] Hypertension is also a risk factor for cognitive impairment and dementia.[23] Other complications include hypertensive retinopathy and hypertensive nephropathy.[27]
Modern understanding of the cardiovascular system began with the work of physician William Harvey (1578–1657), who described the circulation of blood in his book ""De motu cordis"". The English clergyman Stephen Hales made the first published measurement of blood pressure in 1733.[158][159]  However, hypertension as a clinical entity came into its own with the invention of the cuff-based sphygmomanometer by Scipione Riva-Rocci in 1896.[160] This allowed easy measurement of systolic pressure in the clinic. In 1905, Nikolai Korotkoff improved the technique by describing the Korotkoff sounds that are heard when the artery is ausculted with a stethoscope while the sphygmomanometer cuff is deflated.[159] This permitted systolic and diastolic pressure to be measured.
The symptoms similar to symptoms of patients with hypertensive crisis are discussed in medieval Persian medical texts in the chapter of ""fullness disease"".[161] The symptoms include headache, heaviness in the head, sluggish movements, general redness and warm to touch feel of the body, prominent, distended and tense vessels, fullness of the pulse, distension of the skin, coloured and dense urine, loss of appetite, weak eyesight, impairment of thinking, yawning, drowsiness, vascular rupture, and hemorrhagic stroke.[162] Fullness disease was presumed to be due to an excessive amount of blood within the blood vessels.
Descriptions of hypertension as a disease came among others from Thomas Young in 1808 and especially Richard Bright in 1836.[158] The first report of elevated blood pressure in a person without evidence of kidney disease was made by Frederick Akbar Mahomed (1849–1884).[163]
Historically the treatment for what was called the ""hard pulse disease"" consisted in reducing the quantity of blood by bloodletting or the application of leeches.[158] This was advocated by The Yellow Emperor of China, Cornelius Celsus, Galen, and Hippocrates.[158]  The therapeutic approach for the treatment of hard pulse disease included changes in lifestyle (staying away from anger and sexual intercourse) and dietary program for patients (avoiding the consumption of wine, meat, and pastries, reducing the volume of food in a meal, maintaining a low-energy diet and the dietary usage of spinach and vinegar).
In the 19th and 20th centuries, before effective pharmacological treatment for hypertension became possible, three treatment modalities were used, all with numerous side-effects: strict sodium restriction (for example the rice diet[158]), sympathectomy (surgical ablation of parts of the sympathetic nervous system), and pyrogen therapy (injection of substances that caused a fever, indirectly reducing blood pressure).[158][164]
The first chemical for hypertension, sodium thiocyanate, was used in 1900 but had many side effects and was unpopular.[158] Several other agents were developed after the Second World War, the most popular and reasonably effective of which were tetramethylammonium chloride, hexamethonium, hydralazine, and reserpine (derived from the medicinal plant Rauvolfia serpentina). None of these were well tolerated.[165][166] A major breakthrough was achieved with the discovery of the first well-tolerated orally available agents. The first was chlorothiazide, the first thiazide diuretic and developed from the antibiotic sulfanilamide, which became available in 1958.[158][167] Subsequently, beta blockers, calcium channel blockers, angiotensin converting enzyme (ACE) inhibitors, angiotensin receptor blockers, and renin inhibitors were developed as antihypertensive agents.[164]
The World Health Organization has identified hypertension, or high blood pressure, as the leading cause of cardiovascular mortality.[168] The World Hypertension League (WHL), an umbrella organization of 85 national hypertension societies and leagues, recognized that more than 50% of the hypertensive population worldwide are unaware of their condition.[168] To address this problem, the WHL initiated a global awareness campaign on hypertension in 2005 and dedicated May 17 of each year as World Hypertension Day (WHD). Over the past three years, more national societies have been engaging in WHD and have been innovative in their activities to get the message to the public. In 2007, there was record participation from 47 member countries of the WHL. During the week of WHD, all these countries – in partnership with their local governments, professional societies, nongovernmental organizations and private industries – promoted hypertension awareness among the public through several media and public rallies. Using mass media such as Internet and television, the message reached more than 250 million people. As the momentum picks up year after year, the WHL is confident that almost all the estimated 1.5 billion people affected by elevated blood pressure can be reached.[169]
High blood pressure is the most common chronic medical problem prompting visits to primary health care providers in USA. The American Heart Association estimated the direct and indirect costs of high blood pressure in 2010 as $76.6 billion.[150] In the US 80% of people with hypertension are aware of their condition, 71% take some antihypertensive medication, but only 48% of people aware that they have hypertension adequately control it.[150] Adequate management of hypertension can be hampered by inadequacies in the diagnosis, treatment, or control of high blood pressure.[170] Health care providers face many obstacles to achieving blood pressure control, including resistance to taking multiple medications to reach blood pressure goals. People also face the challenges of adhering to medicine schedules and making lifestyle changes. Nonetheless, the achievement of blood pressure goals is possible, and most importantly, lowering blood pressure significantly reduces the risk of death due to heart disease and stroke, the development of other debilitating conditions, and the cost associated with advanced medical care.[171][172]
A 2015 review of several studies found that restoring blood vitamin D levels by using supplements (more than 1,000 IU per day) reduced blood pressure in hypertensive individuals when they had existing vitamin D deficiency.[173] The results also demonstrated a correlation of chronically low vitamin D levels with a higher chance of becoming hypertensive. Supplementation with vitamin D over 18 months in normotensive individuals with vitamin D deficiency did not significantly affect blood pressure.[173]
There is tentative evidence that an increased calcium intake may help in preventing hypertension. However, more studies are needed to assess the optimal dose and the possible side effects.[174]
Hypertension in cats is indicated with a systolic blood pressure greater than 150 mm Hg, with amlodipine the usual first-line treatment.[175]
Normal blood pressure can differ substantially between breeds but hypertension in dogs is often diagnosed if systolic blood pressure is above 160 mm Hg particularly if this is associated with target organ damage.[176] Inhibitors of the renin-angiotensin system and calcium channel blockers are often used to treat hypertension in dogs, although other drugs may be indicated for specific conditions causing high blood pressure.[176]
"
Digital object identifier - Wikipedia," 
A digital object identifier (DOI) is a persistent identifier or handle used to identify objects uniquely, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. However, they also have been used to identify other types of information resources, such as commercial videos.
A DOI aims to be ""resolvable"", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to identify their referents uniquely. The DOI system uses the indecs Content Model for representing metadata.
The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI is supposed to provide a more stable link than simply using its URL. But every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link leaving the DOI useless.
The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.
A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.
The prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is at least a four digit number greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]
For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The ""10."" part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).
DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.
The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.
The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]
Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL – providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]
The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyperlinked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.[19]
Since DOI is a namespace within the Handle system, it is semantically correct to represent it as the URI info:doi/10.1000/182.
Major applications of the DOI system currently include:
In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[20]
Other registries include Crossref and the multilingual European DOI Registration Agency.[21] Since 2015, RFCs can be referenced as doi:10.17487/rfc….[22]
The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.
The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[23] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[24] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[25]
The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.
The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.
A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[26]
A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[27]
The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as ""identifiers"" doesn't mean that they can be compared easily. Other ""identifier systems"" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).
A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.
DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.
To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.
Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as https://doi.org/ (preferred)[28] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.
Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, and https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects the user to that instead of the publisher's version.[29][30] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016[31] (later Unpaywall). While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[29][31]
An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[32] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.
The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[33] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.
The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.
Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[34]
Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.
The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[35] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[36] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[37] The final standard was published on 23 April 2012.[1]
DOI is a registered URI under the info URI scheme specified by IETF .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:""\""""""\""""""'""""'""}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg"")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[38]
The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[39]
The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:
URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.Usually a prefix of DOI code corresponds to a publisher.
"
ISO 9362 - Wikipedia," ISO 9362 defines a standard format of Business Identifier Codes (also known as SWIFT-BIC, BIC, SWIFT ID or SWIFT code) approved by the International Organization for Standardization (ISO). It is a unique identification code for both financial and non-financial institutions.[1]  The acronym SWIFT stands for the Society for Worldwide Interbank Financial Telecommunication. The ISO has designated SWIFT as the BIC registration authority.[2] When assigned to a non-financial institution, the code may also be known as a Business Entity Identifier or BEI. These codes are used when transferring money between banks, particularly for international wire transfers, and also for the exchange of other messages between banks. The codes can sometimes be found on account statements.
The overlapping issue between ISO 9362 and ISO 13616 is discussed in the article International Bank Account Number (also called IBAN). The SWIFT network does not require a specific format for the transaction so the identification of accounts and transaction types is left to agreements of the transaction partners. In the process of the Single Euro Payments Area the European central banks have agreed on a common format based on IBAN and BIC including an XML-based transmission format for standardized transactions. TARGET2 is a joint gross clearing system in the European Union that does not require the SWIFT network for transmission (see EBICS). The TARGET directory lists all the BICs of the banks that are attached to the TARGET2-network being a subset of the SWIFT-directory of BICs.[3]
The previous edition is ISO 9362:2009 (dated 2009-10-01). The SWIFT code is 8 or 11 characters, made up of:
Where an eight digit code is given, it may be assumed that it refers to the primary office.
SWIFT Standards, a division of The Society for Worldwide Interbank Financial Telecommunication (SWIFT), handles the registration of these codes. Because SWIFT originally introduced what was later standardized as Business Identifier Codes (BICs), they are still often called SWIFT addresses or codes.
The 2009 update of ISO 9362 broadened the scope to include non-financial institutions; before then BIC was commonly understood to be an acronym for Bank Identifier Code.
There are over 7,500 ""live"" codes (for partners actively connected to the BIC network) and an estimated 10,000 additional BIC codes which can be used for manual transactions.
2009 version is now replaced by the latest edition (ISO 9362:2014 dated 2014-12-01).[4]
Deutsche Bank  is an international bank, with its head office in Frankfurt, Germany. The SWIFT code for its primary office is DEUTDEFF:
Deutsche Bank uses an extended code of 11 characters and has assigned branches or processing areas individual extended codes. This allows the payment to be directed to a specific office. For example, DEUTDEFF500 would direct the payment to an office of Deutsche Bank in Bad Homburg.
Nedbank is a primarily South African bank, with its head office in Johannesburg. The SWIFT code for its primary office is NEDSZAJJ:
Nedbank has not implemented the extended code of 11 characters and all SWIFT transfers to its accounts are directed to the primary office for processing. Those transfer interfaces that require an 11 digit code would enter NEDSZAJJXXX.
Danske Bank is a primarily Danish bank, with its head office in Copenhagen. The SWIFT code for its primary office is DABADKKK:
UniCredit Banca is a primarily Italian bank with its head office in Milan.
The SWIFT code for its primary office is UNCRITMM:
Dah Sing Bank is a bank based in Hong Kong that has five branches in mainland China (primary mainland China branch in Shenzhen).
The SWIFT code for the branch in Shanghai is DSBACNBXSHA.
It uses the 11-digit extended code, and SHA identifies the Shanghai branch.
BDO Unibank is the biggest bank in the Philippines, with its head office in Makati. The SWIFT Code for BDO is BNORPHMM. All BDO branches have the same SWIFT Code.
Note that one bank can seem to have more than one bank identifier in a given country for separation purposes. Bank of East Asia separates its representative branch in the US and its US-based operations for local customers into BEASUS33xxx (following the code used in its home country) and BEAKUS33xxx respectively. This differs from its local mainland China operations which are also BEASCNxxxxx following Hong Kong rather than having a separate identifier code.
In the past, SEPA payments required both BIC and IBAN. Since 2016-02-01 only the IBAN is needed inside the SEPA (European Union and some more countries).
To identify endpoints on its network, SWIFT also uses twelve-character codes that are derived from the BIC of the institution. Such a code consists of the 'BIC8', followed by a one-character code that identifies the Logic Terminal (LT), (also referred to as ""local destination"" or ""Logic Terminal address""), and the three-character branch code. While 'BIC12's are not part of the ISO standard, and are only relevant in the context of the messaging platform, they play a role in FIN system messaging.  According to SWIFT, Logic Terminals are the ""entity through which users send and receive FIN messages."", thus, may play a role within routing of the message.
- To see a breakdown within a SWIFT header using a Logic Terminal:
SWIFT Headers - BizTalk Server
- For more information on use of LT's, see the following:
FIN Logical Terminals
Business Identifier Codes are primarily used for identifying financial and non-financial institutions involving day-to-day business transactions among one or more institutions in transaction lifecycle.
Example: In SWIFT messages these BICs are embedded within the messages. Consider message type for cash transfer MT103, here we can find BIC under different tags like 50a (ordering customer), 56a (intermediary), 57a (account with institution), etc.
"
Motion JPEG 2000 - Wikipedia," Motion JPEG 2000 (MJ2 or MJP2) is a file format for motion sequences of JPEG 2000 images and associated audio, based on the MP4/QuickTime format. Filename extensions for Motion JPEG 2000 video files are .mj2 and .mjp2, as defined in RFC 3745.
MJ2, first defined by Part 3 of the ISO Standard for JPEG 2000 ISO/IEC 15444 in November 2001[1] (ISO/IEC 15444-3:2002) as a standalone document, has later been defined by ISO/IEC 15444-3:2007, ISO/IEC 15444-3:2007/Amd 1:2010, additional profiles for archiving applications, and by ISO/IEC 15444-12 which defines the JPEG 2000 base media format, which contains the timing, structure, and media information for timed sequences of media data.
The standard is available for download from ITU-T as their Recommendation T.802.[2]
Motion JPEG2000 was always intended to coexist with MPEG. Unlike MPEG, MJ2 does not implement inter-frame coding; each frame is coded independently using JPEG 2000. This makes MJ2 more resilient to propagation of errors over time, more scalable, and better suited to networked and point-to-point environments, with additional advantages over MPEG with respect to random frame access,[3] but at the expense of increased storage and bandwidth requirements.
From 1997 to 2000, the JPEG 2000 image compression standard was developed by a Joint Photographic Experts Group (JPEG) committee chaired by Swiss-Iranian engineer Touradj Ebrahimi (later the JPEG president).[4] In contrast to the original 1992 JPEG standard, which is a discrete cosine transform (DCT) based lossy compression format for static digital images, JPEG 2000 is a discrete wavelet transform (DWT) based compression standard that could be adapted for motion imaging video compression with the Motion JPEG 2000 extension. JPEG 2000 technology was later selected as the video coding standard for digital cinema in 2004.[5]
"
ISO 19600 - Wikipedia," ISO 19600, Compliance management systems - Guidelines, is a compliance standard introduced by the International Organization for Standardisation (ISO) in April 2014. As its title suggests, it operates as an advisory standard and is not used for accreditation or certification.
This standard was developed by ISO Project Committee ISO/PC 271, which was chaired by Martin Tolar. In recent times technical committee ISO/TC 309 has been created and the maintenance and future development of ISO 19600 will be undertaken by members of this committee.
Currently, ISO/TC 309 is in the process of developing ISO/DIS 37301 [1], which is except to replace ISO 19600. The main difference between these two standards is that, when published, ISO 37301 will establish requirements for the implementation of a compliance management system, as opposed to USO 19600 which only provides recommendations. This means that in the future, organizations can have their compliance management system (CMS) verified through an independent third party [2]
Standards Australia proposed a new ISO standard, based on the existing Australian standard ""AS 3806 - Compliance Programs"", which was issued in 1998 and updated in 2006. This standard is more widely used in the financial industry, being endorsed by Australian Prudential Regulation Authority and the Australian Securities and Investment Commission. The published version of ISO 19600:2014 is similar to AS 3806:2006 standard, and will replace it.
The draft stage of ISO 19600 was completed in April 2014;[1] the final version was published on 5 December 2014.
The ISO 19600:2014 adopts the ""ISO High Level Structure (HSL)"" in 10 main clauses in the following breakdown [3]:
ISO 19600 helps organizations establish, develop, evaluate, and maintain a compliance management system. It brings together separate standards of compliance management and risk management, and its processes align very closely with ISO 31000, another risk management standard.[2]
Many existing compliance standards focus on one specific regulatory requirement or topic area; ISO 19600 aims to unify these, so organizations can work within a single framework rather than several different ones focussing on different standards. Unlike PS 980, ISO does not mandate any specific auditing requirements.[3] ISO 19600 is ""based on the principles of good governance, proportionality, transparency and sustainability"".[4]
Like other related ISO standards, it emphasises the use of a Plan, Do, Check, Act (PDCA) cycle.
"
ISO/IEC 8859-3 - Wikipedia," ISO/IEC 8859-3:1999, Information technology — 8-bit single-byte coded graphic character sets — Part 3: Latin alphabet No. 3,[2] is part of the ISO/IEC 8859 series of ASCII-based standard character encodings, first edition published in 1988. It is informally referred to as Latin-3 or South European. It was designed to cover Turkish, Maltese and Esperanto, though the introduction of ISO/IEC 8859-9 superseded it for Turkish. The encoding was popular for users of Esperanto, but fell out of use as application support for Unicode became more common.
ISO-8859-3 is the IANA preferred charset name for this standard when supplemented with the C0 and C1 control codes from ISO/IEC 6429. Microsoft has assigned code page 28593 a.k.a. Windows-28593 to ISO-8859-3 in Windows. IBM has assigned code page 913 (CCSID 913) to ISO 8859-3.[3]
.mw-parser-output .legend{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .legend-color{display:inline-block;min-width:1.25em;height:1.25em;line-height:1.25;margin:1px 0;text-align:center;border:1px solid black;background-color:transparent;color:black}.mw-parser-output .legend-text{}  Letter   Number   Punctuation   Symbol   Other   Undefined   Differences from ISO-8859-1
"
C0 and C1 control codes - Wikipedia," The C0 and C1 control code or control character sets define control codes for use in text by computer systems that use ASCII and derivatives of ASCII. The codes represent additional information about the text, such as the position of a cursor, an instruction to start a new line, or a message that the text has been received.
C0 codes are the range 00HEX–1FHEX and the default C0 set was originally defined in ISO 646 (ASCII). C1 codes are the range 80HEX–9FHEX and the default C1 set was originally defined in ECMA-48 (harmonized later with ISO 6429). The ISO/IEC 2022 system of specifying control and graphic characters allows other C0 and C1 sets to be available for specialized applications, but they are rarely used.
ASCII defined 32 control characters, plus a necessary extra one for the all-1 DEL character (needed to punch out all the holes on a paper tape and erase it).
This large number of codes was desirable at the time, as multi-byte controls would require implementation of a state machine in the terminal, which was very difficult with contemporary electronics and mechanical terminals. Since then, only a few of the original controls have maintained their use: the ""whitespace"" range of BS, TAB, LF, VT, FF, and CR; the BEL code; and ESC (but, except in ISO-2022-JP, almost always as part of an ESC,'[' .mw-parser-output .control-code-link-internal{border-bottom:1px dashed #86a1ff}.mw-parser-output .control-code-link-internal a{color:inherit}CSI representation starting an ANSI escape sequence).  Others are unused or have acquired different meanings such as NUL being the C string terminator.
Some serial transmission protocols such as ANPA-1312, Kermit, and XMODEM do make extensive use of control characters SOH, STX, ETX, EOT, ACK, NAK and SYN for purposes approximating their original definitions.
These are the standard ASCII control codes, originally defined in ANSI X3.4.  If using the ISO/IEC 2022 extension mechanism, they are designated as the active C0 control character set with the octet sequence 0x1B 0x21 0x40 (ESC ! @).[1]
Standards such as (the now-withdrawn) ECMA-37 existed for specific applications of the Data Link Escape character for accessing additional transmission control functions.[11]
Standard Compression Scheme for Unicode suggests replacing all C0-range bytes with DLE, followed by that byte plus 0x40, if SCSU data must be transmitted over a system which would be confused by SCSU's repurposing of the C0 bytes.[12]

If it is not in use for another purpose, IPTC 7901 recommends interpreting ETB as an end of paragraph character.[2]
If it is not in use for another purpose, IPTC 7901 recommends repurposing EM as an em space for indenting the first line of a paragraph[2] (see also EMSP).

The Unix info format uses US, followed by an optional form-feed and a line break, to mark the beginning of a node.[14]
MARC 21 uses US as a subfield delimiter, RS as a field terminator and GS as a record terminator.[15]
In the current edition of IPTC 7901, if they are not used for other purposes, US is recommended for use as a column separator in tables, FS as a ""Central Field Separator"" in tables, and GS and RS respectively for marking a following space or hyphen-minus as non-breaking or soft respectively (in character sets not supplying explicit NBSP and SHY characters).[2]
Python's splitlines string method treats FS, GS and RS, but not US, as separators in addition to the line-breaking characters.[16]
Several of the basic ASCII control codes are classified into a few categories, and sometimes given alternative abbreviated names consisting of that category and a number:[1]
ISO/IEC 2022 (ECMA-35) refers to the C0 locking shifts as LS0 and LS1 in 8-bit environments, and as SI and SO in 7-bit environments.[17]
The first, 1963 edition of ASCII classified DLE as a device control, rather than a transmission control, and gave it the abbreviation DC0 (""device control reserved for data link escape"").[18]
Format effector codes affect how graphical characters are laid out and rendered, as opposed to controlling other functions of hardware devices or having other side effects. The C0 format effectors are permitted in ISO/IEC 6429 DCS, OSC, PM and APC sequences.
The information separators and C0 format effectors (minus BS) are the only C0 control codes with semantics defined by the Unicode Standard, the interpretation of the remainder of the C0 controls being left to higher-level protocols.[19]
ISO/IEC 2022 (ECMA-35) requires that if C0 control code sets include the ten ASCII transmission control codes, they must be encoded at their ASCII locations.[20] It also prohibits those ten transmission controls from being included in a C1 control code set,[21] and prohibits transmission controls besides those ten from being included in a C0 control set.[20]
Although C0 control code sets usually preserve most of the ASCII control codes unchanged, a number are registered which replace certain control functions with alternatives. A selection of these, excluding those related to Videotex, are shown below.
Teletext defines an entirely different set of control codes. In formats where compatibility with ECMA-48's C0 control codes is not required, these control codes are sometimes mapped transparently to the Unicode C0 control code range (U+0000 through U+001F).[30]
In parallel to the development of the 1972 edition of ISO 646, which revised the standard to introduce the concept of national versions of the code in addition to the US-originated ASCII, work was also underway with the purpose of defining extension mechanisms for ASCII, applicable to both 7-bit and 8-bit environments, which would be published as ECMA-35 and ISO 2022.[31]
These mechanisms were designed so that any conformant 8-bit code could be converted to a corresponding 7-bit code, and vice versa.[32] In a 7-bit environment, the Shift Out (SO) control would change the meaning of the 94 bytes 0x21 through 0x7E (i.e. the graphical codes, excluding the space) to invoke characters from an alternative set, and the Shift In (SI) control would change them back.[33] In an 8-bit environment, instead of using shift codes, the eighth bit was set on a byte referencing the additional graphic character set. This meant that bytes 0xA1 through 0xFE were used for the additional graphic characters. The C0 control characters, being unaffected by the shift state of a 7-bit code, were to always be represented in an 8-bit code with the eighth bit unset.[32] The consequently otherwise-unused bytes in the range 0x80 through 0x9F could be used for additional control codes, which would instead be represented as 0x1B 0x40 through 0x1B 0x5F (ESC @ through ESC _) in a 7-bit code.[32] These additional control codes become known as the C1 control codes. To retain compatibility with the 7-bit representation, the behaviour of bytes 0xA0 and 0xFF was originally left undefined.[34]
The first C1 control code set to be registered for use with ISO 2022 was DIN 31626,[35] a specialised set for bibliographic use which was registered in 1979.[36] The general-use ISO/IEC 6429 set was registered in 1983,[37] although the ECMA-48 specification upon which it was based had been first published in 1976.[38]
Further editions of the standards altered the provisions to an extent. For instance, a further revision to ECMA-35 and ISO 2022 in 1985 introduced the concept of a 96-code graphical character set.[39] In an 8-bit code, this allowed the entire range from 0xA0 to 0xFF to be used for graphical characters. Use of 96-code sets also meant that the meaning of the bytes 0x20 and 0x7F in the corresponding 7-bit code could differ from ""Space"" and ""Delete"", unless the code was in the Shift In state.[40] Using 96-code sets for the G0 (Shift In) set was not made possible.[39]
It was this 8-bit code structure, with the bytes with the eighth bit set being divided between a range of C1 control codes and a 96-code set of graphical characters, which was used as the basis for ISO 8859.
These are the most common extended control codes, and are defined in ISO/IEC 6429, ECMA-48 and JIS X 0211 (formerly JIS C 6323).[41] If using the ISO/IEC 2022 extension mechanism, they are designated as the active C1 control character set with the sequence 0x1B 0x22 0x43 (ESC "" C).[37] Although Unicode does not require a particular C1 control code set, leaving their interpretation to be specified by higher-level protocols, and only specifies a behaviour for U+0085, it suggests interpreting C1 control codes as specified in ISO/IEC 6429 in the absence of use for other purposes.[19] Also listed in the table below are three control codes listed alongside the ISO/IEC 6429 codes in .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:""\""""""\""""""'""""'""}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg"")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}RFC 1345, but not actually defined by ISO/IEC 6429 (PAD, HOP and SGC).[5][42]
Except for NEL (and SS2 and SS3 in EUC-JP), the 8-bit forms of these codes are almost never used. CSI, DCS and OSC are used to control text terminals and terminal emulators, but almost always by using their 7-bit escape code representations. Their ISO/IEC 2022 compliant single-byte representations are invalid in UTF-8, and the UTF-8 encodings of their corresponding codepoints are two bytes long like their escape code forms (for instance, CSI at U+009B is encoded as the bytes 0xC2, 0x9B in UTF-8), so there is no advantage to using them rather than the equivalent two-byte escape sequence. When these codes appear in modern documents, web pages, e-mail messages, etc., they are usually intended to be printing characters at that position in a proprietary encoding such as Windows-1252 or Mac OS Roman that use the C1 codes to provide additional graphic characters.
The official English language names of some C1 codes were revised in the most recent edition of the standard for control codes in general (ISO 6429:1992 or ECMA-48:1991) to be neutral with respect to the graphic characters used with them, and to not assume that, as in the Latin script, lines are written on a page from top to bottom and that characters are written on a line from left to right. The abbreviations used were not changed, as the standard had already specified that those would remain unchanged when the standard is translated to other languages. Where the name has been changed, the original name from which the abbreviation was derived is also given in parenthesis in the tables below.
MARC 21 uses SOS and ST in Unicode-format records to mark up a string which should be ignored for collation purposes, while MARC-8 format records use NSB and NSE for the same purpose.[15][46]
Some terminal emulators, including xterm, support OSC sequences for setting the window title and reconfiguring the available colour palette. They may also support terminating an OSC sequence with BEL as a non-standard alternative to the standard ST.[47]
The following alternative C1 control code set is defined for bibliographic applications such as library systems. It is mostly concerned with string collation, and with markup of bibliographic fields. Slightly different variants are defined in the German standard DIN 31626[36] and the ISO standard ISO 6630.[48][49] Where these differ is noted in the table below where applicable. MARC-8 uses the coding of NSB and NSE from this set, and adds some additional format effectors in locations not used by the ISO version; however, MARC 21 uses this control set only in MARC-8 records, not in Unicode-format records.[15]
If using the ISO/IEC 2022 extension mechanism, the DIN version is designated as the active C1 control character set with the sequence 0x1B 0x22 0x45 (ESC "" E),[36] and the ISO version is designated with the sequence 0x1B 0x22 0x42 (ESC "" B).[48] The 1985 edition of the ISO version can also be explicitly specified by using the sequence 0x1B 0x26 0x40 0x1B 0x22 0x42 (ESC & @ ESC "" B).[49]
EBCDIC defines 16 additional control codes, besides those present in ASCII. When mapped to Unicode or to ISO 8859, these codes are mapped to C1 control characters in a manner specified by IBM's Character Data Representation Architecture (CDRA).[50][51] Although the default mapping of the New Line (NL) control does correspond to the ISO/IEC 6429 NEL (0x85; although its mapping is sometimes swapped with LF, following UNIX line ending convention),[50] the remainder of the control codes do not correspond to ISO/IEC 6429. Even when they have the same behaviour, such as SPS and PLU, the C1-mapped representations of the EBCDIC controls (e.g. 0x8D for SPS) do not correspond to the ISO/IEC 6429 codes (e.g. 0x8C for PLU). Extended-ASCII-mapped EBCDIC can therefore be regarded as having its own C1 set, although it is not registered with the ISO-IR registry for use with ISO/IEC 2022.[35]
Various specialised C1 control code sets are registered for use by various Videotex formats.[35]
Unicode sets aside 65 code points in the general category ""Cc"" (Control) for compatibility with ISO/IEC 2022.  The Unicode control characters cover U+0000—U+001F (C0 controls), U+007F (delete), and U+0080—U+009F (C1 controls). Unicode only specifies semantics for U+0009—U+000D, U+001C—U+001F, and U+0085.  The rest of the control characters are transparent to Unicode and their meanings are left to higher-level protocols.[19]
Unicode has no category ""Cc"" code points allocated other than the C0 and C1 ones. However, it does include additional format effector characters besides those in the C0 and C1 control sets, such as marks, embeds, isolates and pops for explicit bidirectional formatting, and the zero-width joiner and non-joiner for controlling ligature use. These are given the general category ""Cf"" (Format) rather than ""Cc"".
"
ISO/IEC 646 - Wikipedia," 

ISO/IEC 646 is the name of a set of ISO standards, described as Information technology — ISO 7-bit coded character set for information interchange and developed in cooperation with ASCII at least since 1964.[1][2] Since its first edition in 1967[3] it has specified a 7-bit character code from which several national standards are derived.
ISO/IEC 646 was also ratified by ECMA as ECMA-6. The first version of ECMA-6 had been published in 1965,[4] based on work the ECMA's Technical Committee TC1 had carried out since December 1960.[4]
Characters in the ISO/IEC 646 Basic Character Set are invariant characters.[5] Since that portion of ISO/IEC 646, that is the invariant character set shared by all countries, specified only those letters used in the ISO basic Latin alphabet, countries using additional letters needed to create national variants of ISO 646 to be able to use their native scripts. Since transmission and storage of 8-bit codes was not standard at the time, the national characters had to be made to fit within the constraints of 7 bits, meaning that some characters that appear in ASCII do not appear in other national variants of ISO 646.
ISO/IEC 646 and its predecessor ASCII (ASA X3.4) largely endorsed existing practice regarding character encodings in the telecommunications industry.
As ASCII did not provide a number of characters needed for languages other than English, a number of national variants were made that substituted some less-used characters with needed ones. Due to the incompatibility of the various national variants, an International Reference Version (IRV) of ISO/IEC 646 was introduced, in an attempt to at least restrict the replaced set to the same characters in all variants. The original version (ISO 646 IRV) differed from ASCII only in that code point 0x24, ASCII's dollar sign ($) was replaced by the international currency symbol (¤). The final 1991 version of the code ISO 646:1991 is also known as ITU T.50, International Reference Alphabet or IRA, formerly International Alphabet No. 5 (IA5). This standard allows users to exercise the 12 variable characters (i.e., two alternative graphic characters and 10 national defined characters). Among these exercises, ISO 646:1991 IRV (International Reference Version) is explicitly defined and identical to ASCII.[6]
The ISO 8859 series of standards governing 8-bit character encodings supersede the ISO 646 international standard and its national variants, by providing 96 additional characters with the additional bit and thus avoiding any substitution of ASCII codes. The ISO 10646 standard, directly related to Unicode, supersedes all of the ISO 646 and ISO 8859 sets with one unified set of character encodings using a larger 21-bit value.
A legacy of ISO/IEC 646 is visible on Windows, where in many East Asian locales the backslash character used in filenames is rendered as ¥ or other characters such as ₩. Despite the fact that a different code for ¥ was available even on the original IBM PC's code page 437, and a separate double-byte code for ¥ is available in Shift JIS (although this often uses alternative mapping), so much text was created with the backslash code used for ¥ (due to Shift_JIS being officially based on ISO 646:JP, although Microsoft maps it as ASCII) that even modern Windows fonts have found it necessary to render the code that way. A similar situation exists with ₩ and EUC-KR. Another legacy is the existence of trigraphs in the C programming language.
The following table shows the ISO/IEC 646 Invariant character set. Each character is shown with the hex code of its Unicode equivalent. National code points are gray with the ASCII character that is replaced. A heavy box indicates a character that, in some regions, could be combined with a previous character as a diacritic using the backspace character, which may affect glyph choice.
In addition to the invariant set restrictions, 0x23 is restricted to be either # or £ and 0x24 is restricted to be either $ or ¤ in ECMA-6:1991, equivalent to ISO 646:1991.[11] However, these restrictions are not followed by all national variants.[12][13]
.mw-parser-output .legend{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .legend-color{display:inline-block;min-width:1.25em;height:1.25em;line-height:1.25;margin:1px 0;text-align:center;border:1px solid black;background-color:transparent;color:black}.mw-parser-output .legend-text{}  Letter   Number   Punctuation   Symbol   Other   Undefined
The National Replacement Character Set (NRCS) is a family of 7-bit encodings introduced in 1983 by DEC with the VT200 series of computer terminals. It is closely related to ISO 646, being based on a similar invariant subset of ASCII, differing in retaining $ as invariant but not _ (although most NRCS variants retain the _, and hence comply with the ISO 646 invariant set). Most NRCS variants are closely related to corresponding national ISO 646 variants where they exist, with the exception of the Dutch variant.
The European telecommunications standard ETS 300 706, ""Enhanced Teletext specification"", defines Latin, Greek, Cyrillic, Arabic and Hebrew code sets with several national variants for both Latin and Cyrillic.[14] Like NRCS and ISO 646, within the Latin variants, the family of encodings known as the G0 set are based on a similar invariant subset of ASCII, but do not retain either $ nor _ as invariant. Unlike NRCS, variants often differ considerably from corresponding national ISO 646 variants.
Some national variants of ISO 646 are as follows:
Some national character sets also exist which are based on ISO 646 but do not strictly follow its invariant set (see also § Derivatives for other alphabets):
All the variants listed above are solely graphical character sets, and are to be used with a C0 control character set such as listed in the following table:
The following table lists supplementary graphical character sets defined by the same standard as specific ISO 646 variants. These would be selected by using a mechanism such as shift out or the NATS super shift (single shift),[48] or by setting the eighth bit in environments where one was available:
The specifics of the changes for some of these variants are given in the following table. Character assignments unchanged across all listed variants (i.e. which remain the same as ASCII) are not shown.
For ease of comparison, variants detailed include national variants of ISO 646, DEC's closely related National Replacement Character Set (NRCS) series used on VT200 terminals, the related European World System Teletext encoding series defined in ETS 300 706, and a few other closely related encodings based on ISO 646. Individual code charts are linked from the second column. The cells with non-white background emphasize the differences from US-ASCII (also the Basic Latin subset of ISO/IEC 10646 and Unicode).
Several characters could be used as combining characters, when preceded or followed with a backspace C0 control. This is attested in the code charts for IRV, GB, FR1, CA and CA2, which note that ""',^ would behave as the diaeresis, acute accent, cedilla and circumflex (rather than quotation marks, a comma and an upward arrowhead) when preceded or followed by a backspace. The tilde character (~) was similarly introduced as a diacritic (˜). This encoding method originated in the typewriter/teletype era when use of backspace would overstamp a glyph, and may be considered deprecated.
Later, when wider character sets gained more acceptance, ISO 8859, vendor-specific character sets and eventually Unicode became the preferred methods of coding most of these variants.
Some 7-bit character sets for non-Latin alphabets are derived from the ISO 646 standard: these do not themselves constitute ISO 646 due to not following its invariant code points (often replacing the letters of at least one case), due to supporting differing alphabets which the set of national code points provide insufficient encoding space for. Examples include:
A comparison of some of these encodings is below. Only one case is shown, except in instances where the cases are mapped to different letters. In such instances, the mapping with the smallest code is shown first. Possible transcriptions are given for some letters; where this is omitted, the letter can be considered to correspond to the Roman one which it is mapped over.
"
ETSI - Wikipedia," ETSI (European Telecommunications Standards Institute) is located in Sophia-Antipolis, in the south of France, and is an independent, not-for-profit, standardization organization in the field of information and communications. ETSI supports the development and testing of global technical standards for ICT-enabled systems, applications and services widely deployed across all sectors of industry and society.
ETSI was set up in 1988 by the European Conference of Postal and Telecommunications Administrations (CEPT) following a proposal from the European Commission. ETSI is the officially recognized body with a responsibility for the standardization of Information and Communication Technologies (ICT). It is one of the three bodies, the others being CEN and CENELEC, officially recognized by the European Union as a European Standards Organization (ESO). The role of the European Standards Organizations is to support EU regulation and policies through the production of Harmonised European Standards and other deliverables. The standards developed by ESOs are the only ones that can be recognized as European Standards (ENs).
ETSI develops standards that have enabled the deployment of key global technologies and have become success stories: GSM™, TETRA, 3G, 4G, 5G, DECT™.
ETSI’s standardization activities are organized around sectors: Home & Office, Better Living with ICT, Content Delivery, Networks, Wireless Systems, Transportation, Connecting Things, Interoperability, Public Safety and Security. 
Each sector represents a key element of the global ICT architecture and gathers a number of technical activities carried out in the different ETSI technical groups (Technical Committee (TC), ETSI Project (EP), ETSI Partnership Project (EPP), Industry Specification Group (ISG), and Special Committee (SC).
Today, ETSI develops standards for a wide range of technologies and among them some essential technologies: SmartM2M (for machine-to-machine communications), Intelligent Transport Systems, Network Functions Virtualisation, Cyber Security, Electronic Signatures and Infrastructures, Quantum-Safe Cryptography, 5G, edge computing, distributed ledger, non-IP networking, etc.  
The list of all ETSI technical committees, working and industry specification groups is accessible via the ETSI Website.

"
History of the transistor - Wikipedia," A transistor is a semiconductor device with at least three terminals for connection to an electric circuit. In the common case, the third terminal controls the flow of current between the other two terminals. This can be used for amplification, as in the case of a radio receiver, or for rapid switching, as in the case of digital circuits. The transistor replaced the vacuum-tube triode, also called a (thermionic) valve,  which was larger and used significantly more power to operate. The introduction of the transistor is often considered one of the most important inventions in history.[1]
The principle of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925. John Bardeen, Walter Brattain and William Shockley invented the first working transistors at Bell Labs, the point-contact transistor in 1947. Shockley introduced the improved bipolar junction transistor in 1948, which entered production in the early 1950s and led to the first widespread use of transistors.
The MOSFET (metal-oxide-semiconductor field-effect transistor), also known as the MOS transistor, was invented by Mohamed Atalla and Dawon Kahng at Bell Labs in 1959. MOSFETs use even less power, which led to the mass-production of MOS transistors for a wide range of uses. The MOSFET has since become the most widely manufactured device in history.
The first patent[2] for the field-effect transistor principle was filed in Canada by Austrian-Hungarian physicist Julius Edgar Lilienfeld on October 22, 1925, but Lilienfeld published no research articles about his devices, and his work was ignored by industry. In 1934 German physicist Dr. Oskar Heil patented another field-effect transistor.[3] There is no direct evidence that these devices were built, but later work in the 1990s show that one of Lilienfeld's designs worked as described and gave substantial gain. Legal papers from the Bell Labs patent show that William Shockley and a co-worker at Bell Labs, Gerald Pearson, had built operational versions from Lilienfeld's patents, yet they never referenced this work in any of their later research papers or historical articles.[4]
The Bell Labs work on the transistor emerged from war-time efforts to produce extremely pure germanium ""crystal"" mixer diodes, used in radar units as a frequency mixer element in microwave radar receivers. UK researchers had produced models using a tungsten filament on a germanium disk, but these were difficult to manufacture and not particularly robust.[5] Bell's version was a single-crystal design that was both smaller and completely solid. A parallel project on germanium diodes at Purdue University succeeded in producing the good-quality germanium semiconducting crystals that were used at Bell Labs.[6] Early tube-based circuits did not switch fast enough for this role, leading the Bell team to use solid state diodes instead.
After the war, Shockley decided to attempt the building of a triode-like semiconductor device. He secured funding and lab space, and went to work on the problem with Bardeen and Brattain. John Bardeen eventually developed a new branch of quantum mechanics known as surface physics to account for the ""odd"" behavior they saw, and Bardeen and Walter Brattain eventually succeeded in building a working device.
The key to the development of the transistor was the further understanding of the process of the electron mobility in a semiconductor. It was realized that if there was some way to control the flow of the electrons from the emitter to the collector of this newly discovered diode (discovered 1874; patented 1906), one could build an amplifier. For instance, if one placed contacts on either side of a single type of crystal the current would not flow through it. However, if a third contact could then ""inject"" electrons or holes into the material, the current would flow.
Actually doing this appeared to be very difficult. If the crystal were of any reasonable size, the number of electrons (or holes) required to be injected would have to be very large -– making it less useful as an amplifier because it would require a large injection current to start with. That said, the whole idea of the crystal diode was that the crystal itself could provide the electrons over a very small distance, the depletion region. The key appeared to be to place the input and output contacts very close together on the surface of the crystal on either side of this region.
Brattain started working on building such a device, and tantalizing hints of amplification continued to appear as the team worked on the problem. Sometimes the system would work but then stop working unexpectedly. In one instance a non-working system started working when placed in water. The electrons in any one piece of the crystal would migrate about due to nearby charges. Electrons in the emitters, or the ""holes"" in the collectors, would cluster at the surface of the crystal where they could find their opposite charge ""floating around"" in the air (or water). Yet they could be pushed away from the surface with the application of a small amount of charge from any other location on the crystal. Instead of needing a large supply of injected electrons, a very small number in the right place on the crystal would accomplish the same thing.
Their understanding solved the problem of needing a very small control area to some degree. Instead of needing two separate semiconductors connected by a common, but tiny, region, a single larger surface would serve. The emitter and collector leads would both be placed very close together on the top, with the control lead placed on the base of the crystal. When current was applied to the ""base"" lead, the electrons or holes would be pushed out, across the block of semiconductor, and collect on the far surface. As long as the emitter and collector were very close together, this should allow enough electrons or holes between them to allow conduction to start.
An early witness of the phenomenon was Ralph Bray, a young graduate student. He joined the germanium effort at Purdue University in November 1943 and was given the tricky task of measuring the spreading resistance at the metal-semiconductor contact. Bray found a great many anomalies, such as internal high-resistivity barriers in some samples of germanium. The most curious phenomenon was the exceptionally low resistance observed when voltage pulses were applied. This effect remained a mystery because nobody realised, until 1948, that Bray had observed minority carrier injection - the effect that was identified by William Shockley at Bell Labs and made the transistor a reality.
Bray wrote: ""That was the one aspect that we missed, but even had we understood the idea of minority carrier injection...we would have said, 'Oh, this explains our effects.' We might not necessarily have gone ahead and said, 'Let's start making transistors,' open up a factory and sell them... At that time the important device was the high back voltage rectifier"".[7]
Shockley's research team initially attempted to build a field-effect transistor (FET), by trying to modulate the conductivity of a semiconductor, but was unsuccessful, mainly due to problems with the surface states, the dangling bond, and the germanium and copper compound materials. In the course of trying to understand the mysterious reasons behind their failure to build a working FET, this led them to instead inventing the bipolar point-contact and junction transistors.[8][9]
The Bell team made many attempts to build such a system with various tools, but generally failed. Setups where the contacts were close enough were invariably as fragile as the original cat's whisker detectors had been, and would work briefly, if at all. Eventually they had a practical breakthrough. A piece of gold foil was glued to the edge of a triangular plastic wedge, and then the foil was sliced with a razor at the tip of the triangle. The result was two very closely spaced contacts of gold. When the plastic was pushed down onto the surface of a crystal and voltage applied to the other side (on the base of the crystal), current started to flow from one contact to the other as the base voltage pushed the electrons away from the base towards the other side near the contacts. The point-contact transistor had been invented.
On 15 December 1947, ""When the points were, very close together got voltage amp about 2 but not power amp. This voltage amplification was independent of frequency 10 to 10,000 cycles"".[10]
On 16 December 1947, ""Using this double point contact, contact was made to a germanium surface that had been anodized to 90 volts, electrolyte washed off in H2O and then had some gold spots evaporated on it. The gold contacts were pressed down on the bare surface. Both gold contacts to the surface rectified nicely... The separation between points was about 4x10−3 cm. One point was used as a grid and the other point as a plate. The bias (D.C.) on the grid had to be positive to get amplification... power gain 1.3 voltage gain 15 on a plate bias of about 15 volts"".[11]
Brattain and H. R. Moore made a demonstration to several of their colleagues and managers at Bell Labs on the afternoon of 23 December 1947, often given as the birth date of the transistor. The ""PNP point-contact germanium transistor"" operated as a speech amplifier with a power gain of 18 in that trial. In 1956 John Bardeen, Walter Houser Brattain, and William Bradford Shockley were honored with the Nobel Prize in Physics ""for their researches on semiconductors and their discovery of the transistor effect"".
Twelve people are mentioned as directly involved in the invention of the transistor in the Bell Laboratory.[12]
At the same time some European scientists were led by the idea of solid-state amplifiers. In August 1948 German physicists Herbert F. Mataré (1912–2011) and Heinrich Welker (1912–1981), working at Compagnie des Freins et Signaux Westinghouse in Aulnay-sous-Bois, France applied for a patent on an amplifier based on the minority carrier injection process which they called the ""transistron"".[13][14][15][16] Since Bell Labs did not make a public announcement of the transistor until June 1948, the transistron was considered to be independently developed.  Mataré had first observed transconductance effects during the manufacture of silicon diodes for German radar equipment during WWII. Transistrons were commercially manufactured for the French telephone company and military, and in 1953 a solid-state radio receiver with four transistrons was demonstrated at the Düsseldorf Radio Fair.
Bell Telephone Laboratories needed a generic name for the new invention: ""Semiconductor Triode"", ""Surface States Triode"", ""Crystal Triode"", ""Solid Triode"" and ""Iotatron"" were all considered, but ""Transistor,"" coined by John R. Pierce, was the clear winner of an internal ballot (owing in part to the affinity that Bell engineers had developed for the suffix ""-istor"").[17][18] The rationale for the name is described in the following extract from the company's Technical Memorandum calling for votes:
Transistor. This is an abbreviated combination of the words ""transconductance"" or ""transfer"", and ""varistor"". The device logically belongs in the varistor family, and has the transconductance or transfer impedance of a device having gain, so that this combination is descriptive.Pierce recalled the naming somewhat differently:
The way I provided the name, was to think of what the device did. And at that time, it was supposed to be the dual of the vacuum tube. The vacuum tube had transconductance, so the transistor would have 'transresistance.' And the name should fit in with the names of other devices, such as varistor and thermistor. And. . . I suggested the name 'transistor.'The Nobel Foundation states that the term is a combination of the words ""transfer"" and ""resistor"".[19]
Shockley was upset about the device being credited to Brattain and Bardeen, who he felt had built it ""behind his back"" to take the glory.  Matters became worse when Bell Labs lawyers found that some of Shockley's own writings on the transistor were close enough to those of an earlier 1925 patent by Julius Edgar Lilienfeld that they thought it best that his name be left off the patent application.
Germanium was difficult to purify, and had a limited operational temperature range. Scientists theorized that silicon would be easier to fabricate, but few bothered to investigate this possibility. Morris Tanenbaum et al. at Bell Laboratories [20] were the first to develop a working silicon transistor on January 26, 1954.[21] A few months later, Gordon Teal, working independently at Texas Instruments, developed a similar device. Both of these devices were made by controlling the doping of single silicon crystals while they were grown from molten silicon. A superior method was developed by Morris Tanenbaum and Calvin S. Fuller at Bell Laboratories in early 1955 by the gaseous diffusion of donor and acceptor impurities into single crystal silicon chips.[22]
Up until the late 1950s, however, germanium remained the dominant semiconductor material for transistors and other semiconductor devices. Germanium was initially considered the more effective semiconductor material, as it was able to demonstrate better performance due to higher carrier mobility.[23][24] The relative lack of performance in early silicon semiconductors was due to electrical conductivity being limited by unstable quantum surface states,[25] preventing electricity from reliably penetrating the surface to reach the semiconducting silicon layer.[26][27]
In 1955, Carl Frosch and Lincoln Derick at Bell Telephone Laboratories (BTL) accidentally discovered that silicon dioxide (SiO2) could be grown on silicon. They showed that oxide layer prevented certain dopants into the silicon wafer, while allowing for others, thus discovering the passivating effect of oxidation on the semiconductor surface.[28] In the 1950s, Mohamed Atalla, picked up Frosch's work on oxidation, investigated the surface properties of silicon semiconductors at Bell Labs, where he proposed a new method of semiconductor device fabrication, coating a silicon wafer with an insulating layer of silicon oxide so that electricity could reliably penetrate to the conducting silicon below, overcoming the surface states that prevented electricity from reaching the semiconducting layer. This is known as surface passivation, a method that became critical to the semiconductor industry as it later made possible the mass-production of silicon integrated circuits.[26] He presented his findings in 1957.[29] He studied the passivation of p-n junctions by oxide, and published his experimental results in 1957 BTL memos.[29] Atalla's surface passivation method was later the basis for two inventions in 1959: the MOS transistor by Atalla and Dawon Kahng, and the planar process by Jean Hoerni.[30]
At a 1958 Electrochemical Society meeting, Atalla presented a paper about the surface passivation of PN junctions by oxide (based on his 1957 BTL memos),[29] and demonstrated silicon dioxide's passivating effect on a silicon surface.[30] Jean Hoerni attended the same meeting, and was intrigued by Atalla's presentation. Hoerni came up with a ""planar idea"" one morning while thinking about Atalla's device.[29] Taking advantage of silicon dioxide's passivating effect on the silicon surface, Hoerni proposed to make transistors that were protected by a layer of silicon dioxide.[29]
The planar process was developed by Jean Hoerni while working at Fairchild Semiconductor, with a first patent issued in 1959.[31][32] The planar process used to make these transistors made mass-produced monolithic silicon integrated circuits possible.
In 1959 the MOSFET was introduced and in 2020 it is still the dominant transistor type in use, with  an estimated total of 13 sextillion (1.3×1022) MOSFETs manufactured between 1960 and 2018. The key advantages of a MOSFET transistors over BJTs are that they consume no current except when switching states and they have faster switching speed (ideal for digital signals).
The world's first commercial transistor production line was at the Western Electric plant on Union Boulevard in Allentown, Pennsylvania. Production began on Oct. 1, 1951 with the point contact germanium transistor.[33]
The first commercial application of transistors in telecommunication was in the Fall of 1952 in tone generators for multifrequency signaling of the No. 5 Crossbar switching system in the Englewood, NJ installation, used for the first field trial of direct distance dialing (DDD).[34]
By 1953, the transistor was being used in some products, such as hearing aids and telephone exchanges, but there were still significant issues preventing its broader application, such as sensitivity to moisture and the fragility of the wires attached to germanium crystals.[35] Donald G. Fink, Philco's director of research, summarized the status of the transistor's commercial potential with an analogy:  ""Is it a pimpled adolescent, now awkward, but promising future vigor? Or has it arrived at maturity, full of languor, surrounded by disappointments?""[35]
Semiconductor companies initially focused on junction transistors in the early years of the semiconductor industry. However, the junction transistor was a relatively bulky device that was difficult to manufacture on a mass-production basis, which limited it to a number of specialised applications.[36]
Prototypes of all-transistor AM radio receivers were demonstrated, but were really only laboratory curiosities. However, in 1950 Shockley developed a radically different type of solid-state amplifier which became known as the bipolar junction transistor, which works on a completely different principle to the point-contact transistor. Morgan Sparks made the bipolar junction transistor into a practical device.[37][38] These were also licensed to a number of other electronics companies, including Texas Instruments, who produced a limited run of transistor radios as a sales tool. Early transistors were chemically unstable and only suitable for low-power, low-frequency applications, but as transistor design developed, these problems were slowly overcome.
There are numerous claimants to the title of the first company to produce practical transistor radios. Texas Instruments had demonstrated all-transistor AM radios as early as 1952, but their performance was well below that of equivalent battery tube models. A workable all-transistor radio was demonstrated in August 1953 at the Düsseldorf Radio Fair by the German firm Intermetall. It was built with four of Intermetall's hand-made transistors, based upon the 1948 invention of Herbert Mataré and Heinrich Welker. However, as with the early Texas units (and others) only prototypes were ever built; it was never put into commercial production.
The first transistor radio is often incorrectly attributed to Sony (originally Tokyo Tsushin Kogyo), which released the TR-55 in 1955. However, it was predated by the Regency TR-1, made by the Regency Division of I.D.E.A. (Industrial Development Engineering Associates) of Indianapolis, Indiana, which was the first practical transistor radio.[citation needed] The TR-1 was announced on October 18, 1954 and put on sale in November 1954 for US$49.95 (the equivalent of about US$361 in year-2005 dollars) and sold about 150,000 units.[citation needed]
The TR-1 used four Texas NPN transistors and had to be powered by a 22.5-volt battery, since the only way to get adequate radio frequency performance out of early transistors was to run them close to their collector-to-emitter breakdown voltage. This made the TR-1 very expensive to run, and it was far more popular for its novelty or status value than its actual performance, rather in the fashion of the first MP3 players.
Still, aside from its indifferent performance, the TR-1 was a very advanced product for its time, using printed circuit boards, and what were then considered micro-miniature components.
Masaru Ibuka, co-founder of the Japanese firm Sony, was visiting the United States when Bell Labs announced the availability of manufacturing licenses, including detailed instructions on how to manufacture junction transistors. Ibuka obtained special permission from the Japanese Ministry of Finance to pay the $50,000 license fee, and in 1955 the company introduced their own five-transistor ""coatpocket"" radio, the TR-55, under the new brand name Sony. This product was soon followed by more ambitious designs, but it is generally regarded as marking the commencement of Sony's growth into a manufacturing superpower.
The TR-55 was quite similar to the Regency TR-1 in many ways, being powered by the same sort of 22.5-volt battery, and was not much more practical. Note: according to the schematic, the TR-55 used a 6 volt supply.[39] Very few were distributed outside Japan. It was not until 1957 that Sony produced their ground-breaking ""TR-63"" shirt pocket radio, a much more advanced design that ran on a standard 9-volt battery and could compete favorably with vacuum tube portables. The TR-63 was also the first transistor radio to use all miniature components. (The term ""pocket"" was a matter of some interpretation, as Sony allegedly had special shirts made with oversized pockets for their salesmen.)
In the April 28th 1955 edition of the Wall Street Journal, Chrysler and Philco announced that they had developed and produced the world's first all-transistor car radio.[40] Chrysler made the all-transistor car radio, Mopar model 914HR, available as an ""option"" in Fall 1955 for its new line of 1956 Chrysler and Imperial cars, which hit the showroom floor on October 21, 1955. The all-transistor car radio was a $150 option.[41][42][43]
The Sony TR-63, released in 1957, was the first mass-produced transistor radio, leading to the mass-market penetration of transistor radios.[44] The TR-63 went on to sell seven million units worldwide by the mid-1960s.[45]  With the visible success of the TR-63, Japanese competitors such as Toshiba and Sharp Corporation  joined the market.[46] Sony's success with transistor radios led to transistors replacing vacuum tubes as the dominant electronic technology in the late 1950s.[47]
The  first low-cost junction transistor available to the general public was the CK722, a PNP germanium small signal unit introduced by Raytheon in early 1953 for $7.60 each. In the 1950s and 1960s, hundreds of hobbyist electronics projects based around the CK722 transistor were published in popular books and magazines.[48][49] Raytheon also participated in expanding the role of the CK722 as a hobbyist electronics device by publishing ""Transistor Applications"" and ""Transistor Applications- Volume 2"" during the mid-1950s.
The world's first transistor computer was built at the University of Manchester in November 1953. The computer was built by Richard Grimsdale, then a research student in the Department of Electrical Engineering and later a Professor of Electronic Engineering at Sussex University. The machine used point-contact transistors, made in small quantities by STC and Mullard. These consisted of a single crystal of germanium with two fine wires, resembling the crystal and cat's whisker of the 1920s. These transistors had the useful property that a single transistor could possess two stable states. ... The development of the machine was severely hampered by the unreliability of the transistors. It consumed 150 watts.[50]
Metropolitan Vickers Ltd rebuilt the full 200 transistor (& 1300 diode) design in 1956 using junction transistors (for internal use).[51]
The IBM 7070 (1958), IBM 7090 (1959), and CDC 1604 (1960) were the first computers (as products for sale) based on transistors.
Building on his silicon surface passivation method, Mohamed Atalla developed the metal–oxide–semiconductor (MOS) process in the late 1950s.[26] He proposed the MOS process could be used to build the first working silicon field-effect transistor (FET), which he began working on building with the help of Dawon Kahng at Bell Labs.[26]
The metal–oxide–semiconductor field-effect transistor (MOSFET) was invented by Atalla and Kahng at Bell Labs.[52][53] They fabricated the device in November 1959,[54] and presented it as the ""silicon-silicon dioxide field induced surface device"" in early 1960.[55] With its high scalability,[56] and much lower power consumption and higher density than bipolar junction transistors,[57] the MOSFET made it possible to build high-density integrated circuits (ICs),[58] allowing the integration of more than 10,000 transistors in a single IC.[59]
The first gallium-arsenide Schottky-gate field-effect transistor (MESFET) was made by Carver Mead and reported in 1966.[60] The first report of a floating-gate MOSFET (FGMOS) was made by Dawon Kahng and Simon Sze in 1967.[61]
The MOSFET has since become the most widely manufactured device in history.[62][63]  As of 2018, an estimated total of 13 sextillion MOS transistors have been manufactured.[62]
There were originally two types of MOSFET logic, PMOS (p-type MOS) and NMOS (n-type MOS).[64] Both types were developed by Atalla and Kahng when they originally invented the MOSFET, fabricating both PMOS and NMOS devices with a 20 µm process.[53]
A new type of MOSFET logic, CMOS (complementary MOS), was invented by Chih-Tang Sah and Frank Wanlass at Fairchild Semiconductor, and in February 1963 they published the invention in a research paper.[65][66]
The self-aligned gate (silicon-gate) MOSFET transistor was invented by Robert Kerwin, Donald Klein and John Sarace at Bell Labs in 1967. Fairchild Semiconductor researchers Federico Faggin and Tom Klein later used self-aligned gate MOSFETs to develop the first silicon-gate MOS integrated circuit.[67]
The MOSFET, also known as the MOS transistor, was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses.[36] It revolutionized the wider electronics industry,[68] including power electronics,[69] consumer electronics, control systems, and computers.[70] The MOSFET has since become the most common type of transistor in the world, with uses including computers, electronics,[27] and communications technology (such as smartphones).[71] The MOS transistor has been described as the ""workhorse of the electronics industry"" due to being the building block of every microprocessor, memory chip and telecommunication circuit in use.[72] Billions of MOS transistors are manufactured every day, as of 2013.[58]
General Microelectronics introduced the first commercial MOS integrated circuits in 1964, consisting of 120 p-channel transistors.[73] It was a 20-bit shift register, developed by Robert Norman[74] and Frank Wanlass.[75] In 1967, Bell Labs researchers Robert Kerwin, Donald Klein and John Sarace developed the self-aligned gate (silicon-gate) MOS transistor, which Fairchild Semiconductor researchers Federico Faggin and Tom Klein used to develop the first silicon-gate MOS IC.[76]
By 1972, MOS LSI (large-scale integration) circuits were commercialized for numerous applications, including automobiles, trucks, home appliances, business machines, electronic musical instruments, computer peripherals, cash registers, calculators, data transmission and telecommunication equipment.[77]
The first modern memory cells were introduced in 1965, when John Schmidt designed the first 64-bit MOS SRAM (static RAM).[78] In 1967, Robert H. Dennard of IBM filed a patent for a single-transistor DRAM (dynamic RAM) memory cell, using a MOSFET.[79]
The earliest practical application of floating-gate MOSFET (FGMOS) was floating-gate memory cells, which Dawon Kahng and Simon Sze proposed could be used to produce reprogrammable ROM (read-only memory).[80] Floating-gate memory cells later became the basis for non-volatile memory (NVM) technologies including EPROM (erasable programmable ROM), EEPROM (electrically erasable programmable ROM) and flash memory.
The MOSFET is the basis of every microprocessor.[72] The earliest microprocessors were all MOS microprocessors, built with MOS LSI circuits. The first multi-chip microprocessors, the Four-Phase Systems AL1 in 1969 and the Garrett AiResearch MP944 in 1970, were developed with multiple MOS LSI chips. The first commercial single-chip microprocessor, the Intel 4004, was developed by Federico Faggin, using his silicon-gate MOS IC technology, with Intel engineers Marcian Hoff and Stan Mazor, and Busicom engineer Masatoshi Shima.[81] With the arrival of CMOS microprocessors in 1975, the term ""MOS microprocessors"" began to refer to chips fabricated entirely from PMOS logic or fabricated entirely from NMOS logic, contrasted with ""CMOS microprocessors"" and ""bipolar bit-slice processors"".[82]
One of the earliest influential consumer electronic products enabled by MOS transistors was the electronic pocket calculator.[59] In 1965, the Victor 3900 desktop calculator was the first MOS LSI calculator, with 29 MOS LSI chips.[83] In 1967 the Texas Instruments Cal-Tech was the first prototype electronic handheld calculator, with three MOS LSI chips, and it was later released as the Canon Pocketronic in 1970.[84] The Sharp QT-8D desktop calculator was the first mass-produced LSI MOS calculator in 1969,[83] and the Sharp EL-8 which used four MOS LSI chips was the first commercial electronic handheld calculator in 1970.[84] The first true electronic pocket calculator was the Busicom LE-120A HANDY LE, which used a single MOS LSI calculator-on-a-chip from Mostek, and was released in 1971.[84]
In the 1970s, the MOS microprocessor was the basis for home computers, microcomputers (micros) and personal computers (PCs). This led to the start of what is known as the personal computer revolution or microcomputer revolution.[85]
The power MOSFET is the most widely used power device in the world.[86] Advantages over bipolar junction transistors in power electronics include MOSFETs not requiring a continuous flow of drive current to remain in the ON state, offering higher switching speeds, lower switching power losses, lower on-resistances, and reduced susceptibility to thermal runaway.[87] The power MOSFET had an impact on power supplies, enabling higher operating frequencies, size and weight reduction, and increased volume production.[88]
The power MOSFET, which is commonly used in power electronics, was developed in the early 1970s.[89] The power MOSFET enables low gate drive power, fast switching speed, and advanced paralleling capability.[86]
"
Wiley (publisher) - Wikipedia," John Wiley & Sons, Inc., commonly known as Wiley (/ˈwaɪli/), is an American multinational publishing company founded in 1807 that focuses on academic publishing and instructional materials. The company produces books, journals, and encyclopedias, in print and electronically, as well as online products and services,[1] training materials, and educational materials for undergraduate, graduate, and continuing education students.[2]
Wiley was established in 1807 when Charles Wiley opened a print shop in Manhattan. The company was the publisher of 19th century American literary figures like James Fenimore Cooper, Washington Irving, Herman Melville, and Edgar Allan Poe, as well as of legal, religious, and other non-fiction titles. The firm took its current name in 1865. Wiley later shifted its focus to scientific, technical, and engineering subject areas, abandoning its literary interests.[3]
Charles Wiley's son John (born in Flatbush, New York, 4 October 1808; died in East Orange, New Jersey, 21 February 1891) took over the business when his father died in 1826. The firm was successively named Wiley, Lane & Co., then Wiley & Putnam, and then John Wiley. The company acquired its present name in 1876, when John's second son William H. Wiley joined his brother Charles in the business.[3][4]
Through the 20th century, the company expanded its publishing activities, the sciences, and higher education.[3]
In 1989, Wiley acquired the life science publisher Liss.[5]
In 1996, Wiley acquired the German technical publisher VCH.[6]
In 1997, Wiley acquired the professional publisher Van Nostrand Reinhold from Thomson Learning.[7]
In 1999, Wiley acquired the professional publisher Jossey-Bass from Pearson.[8]
Wiley marked its bicentennial in 2007. In conjunction with the anniversary, the company published Knowledge for Generations: Wiley and the Global Publishing Industry, 1807-2007, depicting Wiley's role in the evolution of publishing against a social, cultural, and economic backdrop. Wiley has also created an online community called Wiley Living History, offering excerpts from Knowledge for Generations and a forum for visitors and Wiley employees to post their comments and anecdotes.
In December 2010, Wiley opened an office in Dubai.[9] Wiley established publishing operations in India in 2006 (though it has had a sales presence since 1966), and has established a presence in North Africa through sales contracts with academic institutions in Tunisia, Libya, and Egypt.[10] On April 16, 2012, the company announced the establishment of Wiley Brasil Editora LTDA in São Paulo, Brazil, effective May 1, 2012.[11]
Wiley's scientific, technical, and medical business was expanded by the acquisition of Blackwell Publishing in February 2007 for US$1.12 billion, its largest purchase to that time.[12][13] The combined business, named Scientific, Technical, Medical, and Scholarly (also known as Wiley-Blackwell), publishes, in print and online, 1,400 scholarly peer-reviewed journals and an extensive collection of books, reference works, databases, and laboratory manuals in the life and physical sciences, medicine and allied health, engineering, the humanities, and the social sciences. Through a backfile initiative completed in 2007, 8.2 million pages of journal content have been made available online, a collection dating back to 1799. Wiley-Blackwell also publishes on behalf of about 700 professional and scholarly societies; among them are the American Cancer Society (ACS), for which it publishes Cancer, the flagship ACS journal; the Sigma Theta Tau International Honor Society of Nursing; and the American Anthropological Association. Other journals published include Angewandte Chemie, Advanced Materials, Hepatology, International Finance and Liver Transplantation.[14]
Launched as a pilot in 1997 with fifty journals and expanded through 1998,[15] Wiley InterScience provided online access to Wiley journals, reference works, and books, including backfile content. Journals previously from Blackwell Publishing were available online from Blackwell Synergy until they were integrated into Wiley InterScience on June 30, 2008. In December 2007, Wiley also began distributing its technical titles through the Safari Books Online e-reference service. Interscience was supplanted by Wiley Online Library in 2010.[16]
On February 17, 2012, Wiley announced the acquisition of Inscape Holdings Inc., which provides DISC assessments and training for interpersonal business skills.[17]
On March 7, 2012, Wiley announced its intention to divest assets in the areas of travel (including the Frommer's brand), culinary, general interest, nautical, pets, and crafts, as well as the Webster's New World and CliffsNotes brands. The planned divestiture was aligned with Wiley's ""increased strategic focus on content and services for research, learning, and professional practices, and on lifelong learning through digital technology"".[18] On August 13, 2012, Wiley announced it entered into a definitive agreement to sell all of its travel assets, including all of its interests in the Frommer's brand, to Google Inc.[19] On November 6, 2012, Houghton Mifflin Harcourt acquired Wiley's cookbooks, dictionaries and study guides.[20] In 2013, Wiley sold its pets, crafts and general interest lines to Turner Publishing Company and its nautical line to Fernhurst Books.[21] HarperCollins acquired parts of Wiley Canada's trade operations in 2013; the remaining Canadian trade operations were merged into Wiley U.S.[22]
Wiley's Professional Development brands include For Dummies, Jossey-Bass, Pfeiffer, Wrox Press, J.K. Lasser, Sybex, Fisher Investments Press, and Bloomberg Press. The STMS business is also known as Wiley-Blackwell, formed following the acquisition of Blackwell Publishing in February 2007. Brands include The Cochrane Library and more than 1,500 journals.
Wiley has publishing alliances with partners including Microsoft, CFA Institute, the Culinary Institute of America, the American Institute of Architects, the National Geographic Society, and the Institute of Electrical and Electronics Engineers (IEEE). Wiley-Blackwell also publishes journals on behalf of more than 700 professional and scholarly society partners including the New York Academy of Sciences, American Cancer Society, The Physiological Society, British Ecological Society, American Association of Anatomists, Society for the Psychological Study of Social Issues and The London School of Economics and Political Science, making it the world's largest society publisher.[23]
Wiley partners with GreyCampus to provide professional learning solutions[buzzword] around big data and digital literacy.[24] Wiley has also partnered with five other higher-education publishers to create CourseSmart, a company developed to sell college textbooks in eTextbook format on a common platform.[25] In 2002, Wiley created a partnership with French publisher Anuman Interactive in order to launch a series of e-books adapted from the For Dummies collection.[26] In 2013, Wiley partnered with American Graphics Institute to create an online education video and e-book subscription service called The Digital Classroom.[27]
In 2016, Wiley launched a worldwide partnership with Christian H. Cooper to create a program for candidates taking the Financial Risk Manager exam offered by the Global Association of Risk Professionals. The program will be built on the existing Wiley efficient learning platform and Christian's legacy Financial Risk Manager[28] product. The partnership is built on the view the FRM designation will rapidly grow to be one of the premier financial designations for practitioners that will track the growth of the Chartered Financial Analyst designation. The program will serve tens of thousands of FRM candidates worldwide and is based on the adaptive learning technology of Wiley's efficient learning platform and Christian's unique writing style and legacy book series.[29]
With the integration of digital technology and the traditional print medium, Wiley has stated that in the near future its customers will be able to search across all its content regardless of original medium and assemble a custom product in the format of choice.[30] Web resources are also enabling new types of publisher-customer interactions within the company's various businesses.
In 2016, Wiley started a collaboration with the open access publisher Hindawi to help convert nine Wiley journals to full open access. In 2018 a further announcement was made indicating that the Wiley-Hindawi collaboration would launch an additional four new fully open access journals.[31]
On January 18, 2019, Wiley signed a contract with Project DEAL to begin open access to its academic journals for more than 700 academic institutions.[32] It is the first contract between a publisher and a leading research nation (Germany) toward open access to scientific research.
Higher Education's ""WileyPLUS"" is an online product that combines electronic versions of texts with media resources and tools for instructors and students. It is intended to provide a single source from which instructors can manage their courses, create presentations, and assign and grade homework and tests; students can receive hints and explanations as they work on homework, and link back to relevant sections of the text.
""Wiley Custom Select"" launched in February 2009 as a custom textbook system allowing instructors to combine content from different Wiley textbooks and lab manuals and add in their own material. The company has begun to make content from its STMS business available to instructors through the system, with content from its Professional/Trade business to follow.[33]
In September 2019, Wiley entered into a collaboration with IIM Lucknow to offer analytics courses for finance executives[34][35].
In January 2008, Wiley launched a new version of its evidence-based medicine (EBM) product, InfoPOEMs with InfoRetriever, under the name Essential Evidence Plus, providing primary-care clinicians with point-of-care access to the most extensive source of EBM information[36] via their PDAs/handheld devices and desktop computers. Essential Evidence Plus includes the InfoPOEMs daily EBM content alerting service and two new content resources—EBM Guidelines, a collection of practice guidelines, evidence summaries, and images, and e-Essential Evidence, a reference for general practitioners, nurses, and physician assistants providing first-contact care.[37]
In October 2008, Wiley launched a new online service providing continuing education units (CEU) and professional development hour (PDH) credits to architects and designers. The initial courses are adapted from Wiley books, extending their reach into the digital space. Wiley is an accredited AIA continuing education provider.[citation needed]
Wiley Online Library is a subscription-based library of John Wiley & Sons that launched on August 7, 2010, replacing Wiley InterScience.[16] It is a collection of online resources covering life, health, and physical sciences as well as social science and the humanities. To its members, Wiley Online Library delivers access to over 4 million articles from 1,600 journals, more than 22,000 books, and hundreds of reference works, laboratory protocols, and databases from John Wiley & Sons and its imprints, including Wiley-Blackwell, Wiley-VCH, and Jossey-Bass.
While the company is led by an independent management team and Board of Directors, the involvement of the Wiley family is ongoing, with sixth-generation members (and siblings) Peter Booth Wiley as the non-executive Chairman of the Board and Bradford Wiley II as a Director and past Chairman of the Board. Seventh-generation members Jesse and Nate Wiley work in the company's Professional/Trade and Scientific, Technical, Medical, and Scholarly businesses, respectively.
Wiley has been publicly owned since 1962, and listed on the New York Stock Exchange since 1995; its stock is traded under the symbols NYSE: JW.A (for its Class A stock) and NYSE: JW.B (for its class B stock).
Wiley's operations are organized into three business divisions:
The company has approximately 5,000 employees worldwide, with headquarters in Hoboken, New Jersey, since 2002.
In 2008, Wiley was named for the second consecutive year to Forbes magazine's annual list of the ""400 Best Big Companies in America"". In 2007, Book Business magazine cited Wiley as ""One of the 20 Best Book Publishing Companies to Work For"". For two consecutive years, 2006 and 2005, Fortune magazine named Wiley one of the ""100 Best Companies to Work For"". Wiley Canada was named to Canadian Business magazine's 2006 list of ""Best Workplaces in Canada"", and Wiley Australia has received the Australian government's ""Employer of Choice for Women"" citation every year since its inception in 2001. In 2004, Wiley was named to the U.S. Environmental Protection Agency's ""Best Workplaces for Commuters"" list. Working Mother magazine in 2003 listed Wiley as one of the ""100 Best Companies for Working Mothers"", and that same year, the company received the Enterprise Award from the New Jersey Business & Industry Association in recognition of its contribution to the state's economic growth. In 1998, Financial Times selected Wiley as one of the ""most respected companies"" with a ""strong and well thought out strategy"" in its global survey of CEOs.
In August 2009, the company announced a proposed reduction of Wiley-Blackwell staff in content management operations in the UK and Australia by approximately 60, in conjunction with an increase of staff in Asia.[38] In March 2010, it announced a similar reorganization of its Wiley-Blackwell central marketing operations that would lay off approximately 40 employees. The company's position was that the primary goal of this restructuring was to increase workflow efficiency. In June 2012, it announced the proposed closing of its Edinburgh facility in June 2013 with the intention of relocating journal content management activities currently performed there to Oxford and Asia. The move would lay off approximately 50 employees.[39]
Wiley reported a mean 2017 gender pay gap of 21.1 % for its UK workforce, while the median was 21.5 %. The gender bonus gaps are far higher, at 50.7% for the median measure and 42.3% for the mean. Wiley said: ""Our mean and median bonus gaps are driven by our highest earners, who are predominantly male.""[40]
The entire editorial board of the European Law Journal resigned over a dispute about contract terms and the behavior of its publisher, Wiley. Wiley did not allow the editorial board members to decide over editorial appointments and decisions.[41]
A majority of the editorial board of the journal Diversity & Distributions resigned in 2018 after Wiley allegedly blocked the publication of a letter protesting the publisher's decision to make the journal entirely open access. [42]
Wiley makes some articles disappear from their journals without any explanation.[43]
According to Goodhart's law and concerned academics like the signatories of the San Francisco Declaration on Research Assessment, commercial academic publishers benefit from manipulation of bibliometrics and scientometrics like the journal impact factor, which is often used as proxy of prestige and can influence revenues, including public subsidies in the form of subscriptions and free work from academics.[44]
Five Wiley journals, which exhibited unusual levels of self-citation, had their journal impact factor of 2019 suspended from Journal Citation Reports in 2020, a sanction which hit 34 journals in total.[45]
A 2013 lawsuit brought by a stock photo agency for alleged violation of a 1997 license was dismissed for procedural reasons.[46]
A 2014 ruling by the District Court for the Southern District of New York,[47] later affirmed by the Second Circuit,[48] says that Wiley infringed on the copyright of photographer Tom Bean by using his photos beyond the scope of the license it had purchased. The case was connected to a larger set of copyright infringement cases brought by photo agency DRK against various publishers.[49]
A 2015 9th Circuit Court of Appeals opinion established that another photo agency had standing to sue Wiley for its usage of photos beyond the scope of the license acquired.[50]
In 2018, a Southern District of New York court upheld the award of over $39 million to Wiley and other textbook publishers in a vast litigation against Book Dog Books, a re-seller of used books which was found to hold and distribute counterfeit copies. The Court found that circumstantial evidence was sufficient to establish distribution of 116 titles for which counterfeit copies had been presented and of other 5 titles. It also found that unchallenged testimony on how the publishers' usually acquired licenses from authors was sufficient to establish the publishers' copyright on the books in question.[51][52]
In 2008, John Wiley & Sons filed suit against Thailand native Supap Kirtsaeng over the sale of textbooks made outside of the United States and then imported into the country.[53] In 2013, the U.S. Supreme Court held 6–3 that the first-sale doctrine applied to copies of copyrighted works made and sold abroad at lower prices, reversing the Second Circuit decision which had favored Wiley.[54]
In June 2020, Wiley was one of a group of publishers who sued the Internet Archive, arguing that its collection of e-books was denying authors and publishers revenue and accusing the library of ""willful mass copyright infringement"".[55][56]
"
Impact factor - Wikipedia," The impact factor (IF) or journal impact factor (JIF) of an academic journal is a scientometric index calculated by Clarivate that reflects the yearly average number of citations that articles published in the last two years in a given journal received. It is frequently used as a proxy for the relative importance of a journal within its field; journals with higher impact factors are often deemed to be more important, or carry more intrinsic prestige in their respective fields, than those with lower values.
The impact factor was devised by Eugene Garfield, the founder of the Institute for Scientific Information (ISI). Impact factors are calculated yearly starting from 1975 for journals listed in the Journal Citation Reports (JCR). ISI was acquired by Thomson Scientific & Healthcare in 1992,[1] and became known as Thomson ISI. In 2018, Thomson ISI was sold to Onex Corporation and Baring Private Equity Asia.[2] They founded a new corporation, Clarivate, which is now the publisher of the JCR.[3]
In any given year, the two-year journal impact factor is the ratio between the number of citations received in that year for publications in that journal that were published in the two preceding years and the total number of ""citable items"" published in that journal during the two preceding years:[4][5]
For example, Nature had an impact factor of 41.577 in 2017:[6]
This means that, on average, its papers published in 2015 and 2016 received roughly 42 citations each in 2017. Note that 2017 impact factors are reported in 2018; they cannot be calculated until all of the 2017 publications have been processed by the indexing agency.
The value of impact factor depends on how to define ""citations"" and ""publications""; the latter are often referred to as ""citable items"". In current practice, both ""citations"" and ""publications"" are defined exclusively by ISI as follows. ""Publications"" are items that are classed as ""article"", ""review"" or ""proceedings paper""[7] in the Web of Science (WoS) database; other items like editorials, corrections, notes, retractions and discussions are excluded. WoS is accessible to all registered users, who can independently verify the number of citable items for a given journal. In contrast, the number of citations is extracted not from the WoS database, but from a dedicated JCR database, which is not accessible to general readers. Hence, the commonly used ""JCR Impact Factor"" is a proprietary value, which is defined and calculated by ISI and can not be verified by external users.[8]
New journals, which are indexed from their first published issue, will receive an impact factor after two years of indexing; in this case, the citations to the year prior to Volume 1, and the number of articles published in the year prior to Volume 1, are known zero values. Journals that are indexed starting with a volume other than the first volume will not get an impact factor until they have been indexed for three years. Occasionally, Journal Citation Reports assigns an impact factor to new journals with less than two years of indexing, based on partial citation data.[9][10] The calculation always uses two complete and known years of item counts, but for new titles one of the known counts is zero. Annuals and other irregular publications sometimes publish no items in a particular year, affecting the count. The impact factor relates to a specific time period; it is possible to calculate it for any desired period. For example, the JCR also includes a five-year impact factor, which is calculated by dividing the number of citations to the journal in a given year by the number of articles published in that journal in the previous five years.[11][12]
The impact factor is used to compare different journals within a certain field. The Web of Science indexes more than 11,500 science and social science journals.[13]
Journal impact factors are often used to evaluate the merit of individual articles and individual researchers.[14] This use of impact factors was summarised by Hoeffel:[15]
Impact Factor is not a perfect tool to measure the quality of articles but there is nothing better and it has the advantage of already being in existence and is, therefore, a good technique for scientific evaluation. Experience has shown that in each specialty the best journals are those in which it is most difficult to have an article accepted, and these are the journals that have a high impact factor. Most of these journals existed long before the impact factor was devised. The use of impact factor as a measure of quality is widespread because it fits well with the opinion we have in each field of the best journals in our specialty....In conclusion, prestigious journals publish papers of high level. Therefore, their impact factor is high, and not the contrary.As impact factors are a journal-level metric, rather than an article- or individual-level metric, this use is controversial. Garfield agrees with Hoeffel,[16] but warns about the ""misuse in evaluating individuals"" because there is ""a wide variation [of citations] from article to article within a single journal"".[17]
Numerous critiques have been made regarding the use of impact factors.[18][19][20][21] A 2007 study noted that the most fundamental flaw is that impact factors present the mean of data that are not normally distributed, and suggested that it would be more appropriate to present the median of these data.[22] There is also a more general debate on the validity of the impact factor as a measure of journal importance and the effect of policies that editors may adopt to boost their impact factor (perhaps to the detriment of readers and writers). Other criticism focuses on the effect of the impact factor on behavior of scholars, editors and other stakeholders.[23] Others have made more general criticisms, arguing that emphasis on impact factor results from negative influence of neoliberal policies on academia claiming that what is needed is not just replacement of the impact factor with more sophisticated metrics for science publications but also discussion on the social value of research assessment and the growing precariousness of scientific careers in higher education.[24][25]
It has been stated that impact factors and citation analysis in general are affected by field-dependent factors[26] which may invalidate comparisons not only across disciplines but even within different fields of research of one discipline.[27] The percentage of total citations occurring in the first two years after publication also varies highly among disciplines from 1–3% in the mathematical and physical sciences to 5–8% in the biological sciences.[28] Thus impact factors cannot be used to compare journals across disciplines.
Impact factors are sometimes used to evaluate not only the journals but the papers therein, thereby devaluing papers in certain subjects.[29] The Higher Education Funding Council for England was urged by the House of Commons Science and Technology Select Committee to remind Research Assessment Exercise panels that they are obliged to assess the quality of the content of individual articles, not the reputation of the journal in which they are published.[30] The effect of outliers can be seen in the case of the article ""A short history of SHELX"", which included this sentence: ""This paper could serve as a general literature citation when one or more of the open-source SHELX programs (and the Bruker AXS version SHELXTL) are employed in the course of a crystal-structure determination"". This article received more than 6,600 citations. As a consequence, the impact factor of the journal Acta Crystallographica Section A rose from 2.051 in 2008 to 49.926 in 2009, more than Nature (at 31.434) and Science (at 28.103).[31] The second-most cited article in Acta Crystallographica Section A in 2008 only had 28 citations.[32] Additionally, impact factor is a journal metric and should not be used to assess individual researchers or institutions.[33][34][35]
Journal rankings constructed based solely on impact factors only moderately correlate with those compiled from the results of expert surveys.[36]
A.E. Cawkell, former Director of Research at the Institute for Scientific Information remarked that the Science Citation Index (SCI), on which the impact factor is based, ""would work perfectly if every author meticulously cited only the earlier work related to his theme; if it covered every scientific journal published anywhere in the world; and if it were free from economic constraints.""[37]
A journal can adopt editorial policies to increase its impact factor.[38][39] For example, journals may publish a larger percentage of review articles which generally are cited more than research reports.[5]
Journals may also attempt to limit the number of ""citable items""—i.e., the denominator of the impact factor equation—either by declining to publish articles that are unlikely to be cited (such as case reports in medical journals) or by altering articles (e.g., by not allowing an abstract or bibliography in hopes that Journal Citation Reports will not deem it a ""citable item""). As a result of negotiations over whether items are ""citable"", impact factor variations of more than 300% have been observed.[40] Items considered to be uncitable—and thus are not incorporated in impact factor calculations—can, if cited, still enter into the numerator part of the equation despite the ease with which such citations could be excluded. This effect is hard to evaluate, for the distinction between editorial comment and short original articles is not always obvious. For example, letters to the editor may refer to either class.
Another less insidious tactic journals employ is to publish a large portion of its papers, or at least the papers expected to be highly cited, early in the calendar year. This gives those papers more time to gather citations. Several methods, not necessarily with nefarious intent, exist for a journal to cite articles in the same journal which will increase the journal's impact factor.[41][42]
Beyond editorial policies that may skew the impact factor, journals can take overt steps to game the system. For example, in 2007, the specialist journal Folia Phoniatrica et Logopaedica, with an impact factor of 0.66, published an editorial that cited all its articles from 2005 to 2006 in a protest against the ""absurd scientific situation in some countries"" related to use of the impact factor.[43] The large number of citations meant that the impact factor for that journal increased to 1.44. As a result of the increase, the journal was not included in the 2008 and 2009 Journal Citation Reports.[44]
Coercive citation is a practice in which an editor forces an author to add extraneous citations to an article before the journal will agree to publish it, in order to inflate the journal's impact factor.[45] A survey published in 2012 indicates that coercive citation has been experienced by one in five researchers working in economics, sociology, psychology, and multiple business disciplines, and it is more common in business and in journals with a lower impact factor.[46] Editors of leading business journals banded together to disavow the practice.[47]  However, cases of coercive citation have occasionally been reported for other disciplines.[48]
The journal impact factor (JIF) was originally designed by Eugene Garfield as a metric to help librarians make decisions about which journals were worth subscribing to, as the JIF aggregates the number of citations to articles published in each journal. Since then, the JIF has become associated as a mark of journal ""quality"", and gained widespread use for evaluation of research and researchers instead, even at the institutional level. It thus has significant impact on steering research practices and behaviours.[49][50]
Already around 2010, national and international research funding institutions have pointed out that numerical indicators such as the JIF should not be referred to as a measure of quality.[note 1] In fact, the JIF is a highly manipulated metric,[51][52][53] and the justification for its continued widespread use beyond its original narrow purpose seems due to its simplicity (easily calculable and comparable number), rather than any actual relationship to research quality.[54][55][56]
Empirical evidence shows that the misuse of the JIF – and journal ranking metrics in general – has a number of negative consequences for the scholarly communication system. These include confusion between outreach of a journal and the quality of individual papers and insufficient coverage of social sciences and humanities as well as research outputs from across Latin America, Africa, and South-East Asia.[21] Additional drawbacks include the marginalization of research in vernacular languages and on locally relevant topics, inducement to unethical authorship and citation practices as well as more generally fostering of a reputation economy in academia based on publishers"" prestige rather than actual research qualities such as rigorous methods, replicability and social impact. Using journal prestige and the JIF to cultivate a competition regime in academia has been shown to have deleterious effects on research quality.[57]
JIFs are still regularly used to evaluate research in many countries which is a problem since a number of outstanding issues remain around the opacity of the metric and the fact that it is often negotiated by publishers.[58][59][60] However, these integrity problems appear to have done little to curb its widespread mis-use.
A number of regional focal points and initiatives are now providing and suggesting alternative research assessment systems, including key documents such as the Leiden Manifesto[note 2] and the San Francisco Declaration on Research Assessment (DORA). Recent developments around 'Plan S' call on a broader adoption and implementation of such initiatives alongside fundamental changes in the scholarly communication system.[note 3] Thus, there is little basis for the popular simplification which connects JIFs with any measure of quality, and the ongoing inappropriate association of the two will continue to have deleterious effects. As appropriate measures of quality for authors and research, concepts of research excellence should be remodelled around transparent workflows and accessible research results.[61][62][63]
The exact method of calculation of the impact factor by Clarivate is not generally known and the results are therefore not predictable nor reproducible. In particular, the result can change dramatically depending on which items are considered as ""citable"" and therefore included in the denominator.[64] One notorious example of this occurred in 1988 when it was decided that meeting abstracts published in FASEB Journal would no longer be included in the denominator. The journal's impact factor jumped from 0.24 in 1988 to 18.3 in 1989.[65] Publishers routinely discuss with Clarivate how to improve the ""accuracy"" of their journals' impact factor and therefore get higher scores.[66][21]
Such discussions routinely produce ""negotiated values"" which result in dramatic changes in the observed scores for dozens of journals, sometimes after unrelated events like the purchase by one of the big five publishers.[67]
Because citation counts have highly skewed distributions,[69] the mean number of citations is potentially misleading if used to gauge the typical impact of articles in the journal rather than the overall impact of the journal itself.[70] For example, about 90% of Nature's 2004 impact factor was based on only a quarter of its publications, and thus the actual number of citations for a single article in the journal is in most cases much lower than the mean number of citations across articles.[71] Furthermore, the strength of the relationship between impact factors of journals and the citation rates of the papers therein has been steadily decreasing since articles began to be available digitally.[72]
Critics of the JIF state that use of the arithmetic mean in its calculation is problematic because the pattern of citation distribution is skewed. Citation distributions for eight selected journals in,[73] along with their JIFs and the percentage of citable items below the JIF shows that the distributions are clearly skewed, making the arithmetic mean an inappropriate statistic to use to say anything about individual papers within the citation distributions. More informative and readily available article-level metrics can be used instead, such as citation counts or ""altmetrics', along with other qualitative and quantitative measures of research ""impact'.[63][74]
Université de Montréal, Imperial College London, PLOS, eLife, EMBO Journal, The Royal Society, Nature and Science proposed citation distributions metrics as alternative to impact factors.[75][76][77]
Because ""the impact factor is not always a reliable instrument"", in November 2007 the European Association of Science Editors (EASE) issued an official statement recommending ""that journal impact factors are used only—and cautiously—for measuring and comparing the influence of entire journals, but not for the assessment of single papers, and certainly not for the assessment of researchers or research programmes"".[19]
In July 2008, the International Council for Science (ICSU) Committee on Freedom and Responsibility in the Conduct of Science (CFRS) issued a ""statement on publication practices and indices and the role of peer review in research assessment"", suggesting many possible solutions—e.g., considering a limit number of publications per year to be taken into consideration for each scientist, or even penalising scientists for an excessive number of publications per year—e.g., more than 20.[78]
In February 2010, the Deutsche Forschungsgemeinschaft (German Research Foundation) published new guidelines to evaluate only articles and no bibliometric information on candidates to be evaluated in all decisions concerning ""performance-based funding allocations, postdoctoral qualifications, appointments, or reviewing funding proposals, [where] increasing importance has been given to numerical indicators such as the h-index and the impact factor"".[79] This decision follows similar ones of the National Science Foundation (US) and the Research Assessment Exercise (UK).[citation needed]
In response to growing concerns over the inappropriate use of journal impact factors in evaluating scientific outputs and scientists themselves, the American Society for Cell Biology together with a group of editors and publishers of scholarly journals created the San Francisco Declaration on Research Assessment (DORA). Released in May 2013, DORA has garnered support from thousands of individuals and hundreds of institutions,[25] including in March 2015 the League of European Research Universities (a consortium of 21 of the most renowned research universities in Europe),[80] who have endorsed the document on the DORA website.
Several publishers and platforms have also chosen not to show impact factors. For instance, the publisher PLOS does not display the impact factors of their journals on their website. Impact factors are also missing from Microsoft Academic, a scholarly search engine. As of 2020, in the FAQs the Microsoft team says that h-index, EI/SCI and journal impact factors are not shown because ""The research literature has provided abundant evidence that these metrics are at best a rough approximation of research impact and scholarly influence.""[81]
Some related values, also calculated and published by the same organization, include:
As with the impact factor, there are some nuances to this: for example, ISI excludes certain article types (such as news items, correspondence, and errata) from the denominator.[83][84][85]
[86]
Additional journal-level metrics are available from other organizations.  For example, CiteScore: is a metric for serial titles in Scopus launched in December 2016 by Elsevier.[87][88]
While these metrics apply only to journals, there are also author-level metrics, such as the H-index, that apply to individual researchers.
In addition, article-level metrics measure impact at an article level instead of journal level.
Other more general alternative metrics, or ""altmetrics"", may include article views, downloads, or mentions in social media.
Fake impact factors or bogus impact factors are produced by companies or individuals not affiliated with the Journal Citation Reports (JCR).[89] According to an article published in the Electronic Physician, these include Global Impact Factor (GIF), Citefactor, and Universal Impact Factor (UIF).[90] Jeffrey Beall maintained a list of such misleading metrics.[91][92] Another deceitful practice is reporting ""alternative impact factors"", calculated as the average number of citations per article using citation indices other than JCR, even if based on reputable sources such as Google Scholar (e.g., ""Google-based Journal Impact Factor"").[93]
False impact factors are often used by predatory publishers.[94] Consulting Journal Citation Reports' master journal list can confirm if a publication is indexed by Journal Citation Reports.[95] The use of fake impact metrics is considered a ""red flag"".[96]

"
Digital object identifier - Wikipedia," 
A digital object identifier (DOI) is a persistent identifier or handle used to identify objects uniquely, standardized by the International Organization for Standardization (ISO).[1] An implementation of the Handle System,[2][3] DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. However, they also have been used to identify other types of information resources, such as commercial videos.
A DOI aims to be ""resolvable"", usually to some form of access to the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL, indicating where the object can be found. Thus, by being actionable and interoperable, a DOI differs from identifiers such as ISBNs and ISRCs which aim only to identify their referents uniquely. The DOI system uses the indecs Content Model for representing metadata.
The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI is supposed to provide a more stable link than simply using its URL. But every time a URL changes, the publisher has to update the metadata for the DOI to link to the new URL.[4][5][6] It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link leaving the DOI useless.
The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000.[7] Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.[8] The DOI system is implemented through a federation of registration agencies coordinated by the IDF.[9] By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations,[10] and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.
A DOI is a type of Handle System handle, which takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash.
The prefix identifies the registrant of the identifier and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal Unicode characters are allowed in these strings, which are interpreted in a case-insensitive manner. The prefix usually takes the form 10.NNNN, where NNNN is at least a four digit number greater than or equal to 1000, whose limit depends only on the total number of registrants.[11][12] The prefix may be further subdivided with periods, like 10.NNNN.N.[13]
For example, in the DOI name 10.1000/182, the prefix is 10.1000 and the suffix is 182. The ""10."" part of the prefix distinguishes the handle as part of the DOI namespace, as opposed to some other Handle System namespace,[A] and the characters 1000 in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. 182 is the suffix, or item ID, identifying a single object (in this case, the latest version of the DOI Handbook).
DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, performances, and abstract works[14] such as licenses, parties to a transaction, etc.
The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the indecs Content Model.
The official DOI Handbook explicitly states that DOIs should display on screens and in print in the format doi:10.1000/182.[15]
Contrary to the DOI Handbook, CrossRef, a major DOI registration agency, recommends displaying a URL (for example, https://doi.org/10.1000/182) instead of the officially specified format (for example, doi:10.1000/182)[16][17] This URL is persistent (there is a contract that ensures persistence in the DOI.ORG domain), so it is a PURL – providing the location of an HTTP proxy server which will redirect web accesses to the correct online location of the linked item.[8][18]
The CrossRef recommendation is primarily based on the assumption that the DOI is being displayed without being hyperlinked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.[19]
Since DOI is a namespace within the Handle system, it is semantically correct to represent it as the URI info:doi/10.1000/182.
Major applications of the DOI system currently include:
In the Organisation for Economic Co-operation and Development's publication service OECD iLibrary, each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.[20]
Other registries include Crossref and the multilingual European DOI Registration Agency.[21] Since 2015, RFCs can be referenced as doi:10.17487/rfc….[22]
The IDF designed the DOI system to provide a form of persistent identification, in which each DOI name permanently and unambiguously identifies the object to which it is associated. It also associates metadata with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the Handle System and the indecs Content Model with a social infrastructure.
The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the URI specification. The DOI name-resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on open architectures, incorporates trust mechanisms, and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.[23] DOI name-resolution may be used with OpenURL to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.[24] However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.[25]
The indecs Content Model as used within the DOI system associates metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.
The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as GS1 and ISBN.
A DOI name differs from commonly used Internet pointers to material, such as the Uniform Resource Locator (URL), in that it identifies an object itself as a first-class entity, rather than the specific place where the object is located at a certain time. It implements the Uniform Resource Identifier (Uniform Resource Name) concept and adds to it a data model and social infrastructure.[26]
A DOI name also differs from standard identifier registries such as the ISBN, ISRC, etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.[27]
The DOI system offers persistent, semantically-interoperable resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as ""identifiers"" doesn't mean that they can be compared easily. Other ""identifier systems"" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include Persistent Uniform Resource Locator (PURL), URLs, Globally Unique Identifiers (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., ARK).
A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.
DOI name resolution is provided through the Handle System, developed by Corporation for National Research Initiatives, and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <type> field, which defines the syntax and semantics of its data. While a DOI persistently and uniquely identifies the object to which it is assigned, DOI resolution may not be persistent, due to technical and administrative issues.
To resolve a DOI name, it may be input to a DOI resolver, such as doi.org.
Another approach, which avoids typing or cutting-and-pasting into a resolver is to include the DOI in a document as a URL which uses the resolver as an HTTP proxy, such as https://doi.org/ (preferred)[28] or http://dx.doi.org/, both of which support HTTPS. For example, the DOI 10.1000/182 can be included in a reference or hyperlink as https://doi.org/10.1000/182. This approach allows users to click on the DOI as a normal hyperlink. Indeed, as previously mentioned, this is how CrossRef recommends that DOIs always be represented (preferring HTTPS over HTTP), so that if they are cut-and-pasted into other documents, emails, etc., they will be actionable.
Other DOI resolvers and HTTP Proxies include http://hdl.handle.net, and https://doi.pangaea.de/. At the beginning of the year 2016, a new class of alternative DOI resolvers was started by http://doai.io. This service is unusual in that it tries to find a non-paywalled version of a title and redirects the user to that instead of the publisher's version.[29][30] Since then, other open-access favoring DOI resolvers have been created, notably https://oadoi.org/ in October 2016[31] (later Unpaywall). While traditional DOI resolvers solely rely on the Handle System, alternative DOI resolvers first consult open access resources such as BASE (Bielefeld Academic Search Engine).[29][31]
An alternative to HTTP proxies is to use one of a number of add-ons and plug-ins for browsers, thereby avoiding the conversion of the DOIs to URLs,[32] which depend on domain names and may be subject to change, while still allowing the DOI to be treated as a normal hyperlink. For example. the CNRI Handle Extension for Firefox, enables the browser to access Handle System handles or DOIs like hdl:4263537/4000 or doi:10.1000/1 directly in the Firefox browser, using the native Handle System protocol. This plug-in can also replace references to web-to-handle proxy servers with native resolution. A disadvantage of this approach for publishers is that, at least at present, most users will be encountering the DOIs in a browser, mail reader, or other software which does not have one of these plug-ins installed.
The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.[33] It safeguards all intellectual property rights relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.
The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.
Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation. The IDF is recognized as one of the federated registrars for the Handle System by the DONA Foundation (of which the IDF is a board member), and is responsible for assigning Handle System prefixes under the top-level 10 prefix.[34]
Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a not-for-profit cost recovery basis.
The DOI system is an international standard developed by the International Organization for Standardization in its technical committee on identification and description, TC46/SC9.[35] The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,[36] which was approved by 100% of those voting in a ballot closing on 15 November 2010.[37] The final standard was published on 23 April 2012.[1]
DOI is a registered URI under the info URI scheme specified by IETF .mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:""\""""""\""""""'""""'""}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg"")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url(""//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg"")right 0.1em center/12px no-repeat}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.[38]
The DOI syntax is a NISO standard, first standardised in 2000, ANSI/NISO Z39.84-2005 Syntax for the Digital Object Identifier.[39]
The maintainers of the DOI system have deliberately not registered a DOI namespace for URNs, stating that:
URN architecture assumes a DNS-based Resolution Discovery Service (RDS) to find the service appropriate to the given URN scheme. However no such widely deployed RDS schemes currently exist.... DOI is not registered as a URN namespace, despite fulfilling all the functional requirements, since URN registration appears to offer no advantage to the DOI System. It requires an additional layer of administration for defining DOI as a URN namespace (the string urn:doi:10.1000/1 rather than the simpler doi:10.1000/1) and an additional step of unnecessary redirection to access the resolution service, already achieved through either http proxy or native resolution. If RDS mechanisms supporting URN specifications become widely available, DOI will be registered as a URN.Usually a prefix of DOI code corresponds to a publisher.
"
ISO 8373 - Wikipedia," ISO 8373 Manipulating industrial robots – Vocabulary
It is the ISO standard that defines terms relevant to manipulating industrial robots operated in a manufacturing environment.
"
Linux Standard Base - Wikipedia," 
The Linux Standard Base (LSB) is a joint project by several Linux distributions under the organizational structure of the Linux Foundation to standardize the software system structure, including the Filesystem Hierarchy Standard used in the Linux kernel. The LSB is based on the POSIX specification, the Single UNIX Specification (SUS), and several other open standards, but extends them in certain areas.
According to the LSB:
The goal of the LSB is to develop and promote a set of open standards that will increase compatibility among Linux distributions and enable software applications to run on any compliant system even in binary form. In addition, the LSB will help coordinate efforts to recruit software vendors to port and write products for Linux Operating Systems.
The LSB compliance may be certified for a product by a certification procedure.[1]
The LSB specifies for example: standard libraries, a number of commands and utilities that extend the POSIX standard, the layout of the file system hierarchy, run levels, the printing system, including spoolers such as CUPS and tools like Foomatic, and several extensions to the X Window System.
LSB also specifies boot facilities, such as $local_fs, $network, which are used to indicate service dependencies in System V-style initialization scripts. A machine readable comment block at the top of a script provides the information necessary to determine at which point of the initialization process the script should be invoked. It is called the LSB header.[2]
The command lsb_release -a is available in many systems to get the LSB version details, or can be made available by installing an appropriate package, for example the redhat-lsb package in Red-Hat-flavored distributions such as Fedora,[3] or the lsb-release package in Debian-based distributions.
The LSB is designed to be binary-compatible and produce a stable application binary interface (ABI) for independent software vendors. To achieve backward compatibility, each subsequent version is purely additive. In other words, interfaces are only added, not removed. The LSB adopted an interface deprecation policy to give application developers enough time in case an interface is removed from the LSB.
This allows the developer to rely on every interface in the LSB for a known time and also to plan for changes, without being surprised. Interfaces are only removed after having been marked ""deprecated"" for at least three major versions, or roughly eleven years.[4]
LSB 5.0 is the first major release that breaks backward compatibility with earlier versions.[5]
The LSB, version 3.1, is registered as an official ISO standard.[9] The main parts of it are:
There is also ISO/IEC TR 24715:2006 which identifies areas of conflict between ISO/IEC 23360 (the Linux Standard Base 3.1 specification) and the ISO/IEC 9945:2003 (POSIX) International Standard.[10]
ISO/IEC 23360 and ISO/IEC TR 24715 can be freely downloaded from ISO website.[11]
While the LSB is a standard and without a competitor, it is followed only by few Linux distributions. For instance, only 21 distribution releases (versions) are certified for LSB version 4.0, notably Red Flag Linux Desktop 6.0, Red Hat Enterprise Linux 6.0, SUSE Linux Enterprise 11, and Ubuntu 9.04 (Jaunty Jackalope);[12] even fewer are certified for version 4.1.
The LSB has been criticized[13][14][15][16] for not taking input from projects, most notably the Debian project, outside the sphere of its member companies.
The LSB specifies that software packages should either be delivered as an LSB-compliant installer,[17] or (preferably) be delivered in a restricted form of the RPM Package Manager format.[18]
This choice of package format precludes the use of the many other, existing package formats not compatible with RPM. To address this, the standard does not dictate which package format the system must use for its own packages, merely that RPM must be supported to allow packages from third-party distributors to be installed on a conforming system.
Debian has included optional support for the LSB early on, at version 1.1 in ""woody"" (3.0; July 19, 2002), 2.0 in ""sarge"" (3.1; June 6, 2005), 3.1 in ""etch"" (4.0; April 8, 2007), 3.2 in ""lenny"" (5.0; February 14, 2009) and 4.1 in ""wheezy"" (7; May 4, 2013). To use foreign LSB-compliant RPM packages, the end-user needs to use Debian's Alien program to transform them into the native package format and then install them.
The LSB-specified RPM format has a restricted subset of RPM features—to block usage of RPM features that would be untranslatable to .deb with Alien or other package conversion programs, and vice versa, as each format has capabilities the other lacks. In practice, not all Linux binary packages are necessarily LSB-compliant, so while most can be converted between .rpm and .deb, this operation is restricted to a subset of packages.
By using Alien, Debian is LSB-compatible for all intents and purposes, but according to the description of their lsb package,[19] the presence of the package ""does not imply that we believe that Debian fully complies with the Linux Standard Base, and should not be construed as a statement that Debian is LSB-compliant.""[19]
Debian strived to comply with the LSB, but with many limitations.[20] However, this effort ceased around July 2015 due to lack of interest and workforce inside the project.[21] In September 2015, the Debian project confirmed that while support for Filesystem Hierarchy Standard (FHS) would continue, support for LSB had been dropped.[22] Ubuntu followed Debian in November 2015.[23]
Additionally, the compliance test suites have been criticized for being buggy and incomplete—most notably, in 2005 Ulrich Drepper criticized the LSB for poorly written tests which can cause incompatibility between LSB-certified distributions when some implement incorrect behavior to make buggy tests work, while others apply for and receive waivers from complying with the tests.[24] He also denounced a lack of application testing, pointing out that testing only distributions can never solve the problem of applications relying on implementation-defined behavior.[24]
For the vendors considering LSB certifications in their portability efforts, the Linux Foundation sponsors a tool that analyzes and provides guidance on symbols and libraries that go beyond the LSB.[25]
"
ISO 10160 - Wikipedia," 
ISO 10160 is the ISO standard, first published in 1993, that defines the terminology that is used for interlibrary loan transactions between various document exchange systems such as VDX.[1][2] It is closely related to ISO 10161, the Interlibrary Loan Application Protocol.[1]

"
ISO/IEC 7811 - Wikipedia," 
ISO/IEC 7811 Identification cards — Recording technique is a set of nine (7811-1 to 7811-9) standards describing the recording technique on identification cards.
It comprises:

"
ISO/IEC 7810 - Wikipedia," 
ISO/IEC 7810 Identification cards — Physical characteristics is an international standard that defines the physical characteristics for identification cards.[1]
The characteristics specified include:
The standard includes test methods for resistance to heat.[2]
The standard defines four card sizes: ID-1, ID-2, ID-3 and ID-000.[3]
All card sizes have a thickness of 0.76 millimetres (1⁄32 in).

The standard defines both metric and imperial measurements, noting that:[4].mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}
Numeric values in the SI and/or Imperial measurement system [...] may have been rounded off and therefore are consistent with, but not exactly equal to, each other. Either system may be used, but the two should not be intermixed or reconverted. The original design was made using the Imperial measurement system.The ID-1 format specifies a size of 85.60 by 53.98 millimetres (3 3⁄8 in × 2 1⁄8 in) and rounded corners with a radius of 2.88–3.48 mm (about ​1⁄8 in). It is commonly used for payment cards (ATM cards, credit cards, debit cards, etc.). Today it is also used for driving licences and personal identity cards in many countries, automated fare collection system cards for public transport, in retail loyalty cards, and even crew member certificates (particularly for aircrew[5]).
The ID-2 format specifies a size of 105 by 74 millimetres (4 1⁄8 in × 2 15⁄16 in). This size is the A7 format. The ID-2 format is used, for example, for visas. It is used for the French and Romanian identity card, and was also used by the German identity card issued until October 2010. Since November 2010, German ID cards are issued in the ID-1 format more widely used in Europe for national ID cards. It was previously also used for Finnish and Swedish drivers' licences before those changed to the ID-1 format.
ID-3 specifies a size of 125 by 88 millimetres (4 15⁄16 in × 3 7⁄16 in). This size is the B7 format. This format is commonly used for passport booklets.[citation needed]
ID-000 specifies a size of 25 by 15 millimetres (1 in × 9⁄16 in), with one corner slightly (3 mm or 1⁄8 in) bevelled. The ID-000 size was first defined by ENV 1375–1, Identification card systems — Intersector integrated circuit(s) card additional formats — Part 1: ID-000 card size and physical characteristics.
This size is used for the ""mini-SIM"" format of subscriber identity modules.
An ""informative"" (i.e. non-mandatory) annex[6] describes how an ID-000 sized card may be included in an ID-1 size card for processing (e.g. in an ID-1 reader), but with ""relief areas around the perimeter of the ID-000 size card to allow it to be removed from the ID-1 size card without punching tools"". An ID-1 size card containing an ID-000 size card is denoted as ID-1/000.
The standard specifies requirements for such physical characteristics as:[7]
"
Salt spray test - Wikipedia," The salt spray test (or salt fog test) is a standardized and popular corrosion test method, used to check corrosion resistance of materials and surface coatings. Usually, the materials to be tested are metallic (although stone, ceramics, and polymers may also be tested) and finished with a surface coating which is intended to provide a degree of corrosion protection to the underlying metal.
Salt spray testing is an accelerated corrosion test that produces a corrosive attack to coated samples in order to evaluate (mostly comparatively) the suitability of the coating for use as a protective finish. The appearance of corrosion products (rust or other oxides) is evaluated after a pre-determined period of time. Test duration depends on the corrosion resistance of the coating; generally, the more corrosion resistant the coating is, the longer the period of testing before the appearance of corrosion or rust.
The salt spray test is one of the most widespread and long-established corrosion tests. ASTM B117 was the first internationally recognized salt spray standard, originally published in 1939. Other important relevant standards are ISO 9227, JIS Z 2371 and ASTM G85.
Salt spray testing is popular because it is relatively inexpensive, quick, well standardized, and reasonably repeatable. Although there may be a weak correlation between the duration in salt spray test and the expected life of a coating in certain coatings such as  hot-dip galvanized steel, this test has gained worldwide popularity due to low cost and quick results. Most Salt Spray Chambers today are being used NOT to predict the corrosion resistance of a coating, but to maintain coating processes such as pre-treatment and painting, electroplating, galvanizing, and the like, on a comparative basis. For example, pre-treated + painted components must pass 96 hours Neutral Salt Spray, to be accepted for production. Failure to meet this requirement implies instability in the chemical process of the pre-treatment, or the paint quality, which must be addressed immediately so that the upcoming batches are of the desired quality. The longer the accelerated corrosion test, the longer the process remains out of control, and larger is the loss in the form of non-conforming batches.
The principal application of the salt spray test is, therefore, enabling quick comparisons to be made between actual and expected corrosion resistance. Most commonly, the time taken for oxides to appear on the samples under test is compared to expectations, to determine whether the test is passed or failed. For this reason, the salt spray test is most often deployed in a quality audit role, where, for example, it can be used to check the effectiveness of a production process, such as the surface coating of a metallic part.
The salt spray test has little application in predicting how materials or surface coatings will resist corrosion in the real world, because it does not create, replicate or accelerate real-world corrosive conditions. Cyclic corrosion testing is better suited to this.
[1]

The apparatus for testing consists of a closed testing cabinet/chamber, where a salt water (5% NaCl) solution is atomized by means of spray nozzle(s) using pressurized air. This produces a corrosive environment of dense salt water fog (also referred to as a mist or spray) in the chamber, so that test samples exposed to this environment are subjected to severely corrosive conditions. Chamber volumes vary from supplier to supplier. If there is a minimum volume required by a particular salt spray test standard, this will be clearly stated and should be complied with. There is a general historical consensus that larger chambers can provide a more homogeneous testing environment.
Variations to the salt spray test solutions depend upon the materials to be tested. The most common test for steel based materials is the Neutral Salt Spray test (often abbreviated to NSS) which reflects the fact that this type of test solution is prepared to a neutral pH of 6.5 to 7.2. Results are represented generally as testing hours in NSS without appearance of corrosion products (e.g. 720 h in NSS according to ISO 9227). Synthetic seawater solutions are also commonly specified by some companies and standards.  Other test solutions have other chemicals added including acetic acid (often abbreviated to ASS) and acetic acid with copper chloride (often abbreviated to CASS) each one chosen for the evaluation of decorative coatings, such as electroplated copper-nickel-chromium, electroplated copper-nickel or anodized aluminum. These acidified test solutions generally have a pH of 3.1 to 3.3
Some sources do not recommend using ASS or CASS test cabinets interchangeably for NSS tests, due to the risk of cross-contamination. It is claimed that a thorough cleaning of the cabinet after CASS test is very difficult. ASTM does not address this issue, but ISO 9227 does not recommend it and if it is to be done, advocates a thorough cleaning.
Although the majority of salt spray tests are continuous, i.e.; the samples under test are exposed to the continuous generation of salt fog for the entire duration of the test, a few do not require such exposure. Such tests are commonly referred to as modified salt spray tests. ASTM G85 is an example of a test standard which contains several modified salt spray tests which are variations to the basic salt spray test.
ASTM G85 is the most popular global test standard covering modified salt spray tests. There are five such tests altogether, referred to in ASTM G85 as annexes A1 through to A5.[2] Many of these modified tests originally arose within particular industry sector, in order to address the need for a corrosion test capable of replicating the effects of naturally occurring corrosion and accelerate these effects.
This acceleration arises through the use of chemically altered salt spray solutions, often combined with other test climates and in most cases, the relatively rapid cycling of these test climates over time. Although popular in certain industries, modified salt spray testing has in many cases been superseded by cyclic corrosion testing (CCT) 
The type of environmental test chambers used for modified salt spray testing to ASTM G85 are generally similar to the chambers used for testing to ASTM B117, but will often have some additional features, such as an automatic climate cycling control system.
ASTM G85 Annex A1 – Acetic Acid Salt Spray Test (non-cyclic) 
This test can be used to determine the relative resistance to corrosion of decorative chromium plating on steel and zinc based die casting when exposed to an acetic acid salt spray climate at an elevated temperature. This test is also referred to as an ASS test.
Test specimens are placed in an enclosed chamber and exposed to a continuous indirect spray of salt water solution, prepared in accordance with the requirements of the test standard and acidified (pH 3.1–3.3) by the addition of acetic acid. This spray is set to fall-out on to the specimens at a rate of 1–2 ml/80 cm²/hour, in a chamber temperature of 35 °C. This climate is maintained under constant steady state conditions. The test duration is variable.[3]
ASTM G85 Annex A2 – Acidified Salt Fog Test (cyclic).
This test can be used to test the relative resistance to corrosion of aluminium alloys when exposed to a changing climate of acetic acid salt spray, followed by air drying, followed by high humidity, all at an elevated temperature. This test is also referred to as a MASTMAASIS test.
Test specimens are placed in an enclosed chamber, and exposed to a changing climate that comprises the following 3 part repeating cycle. 0.75 hours exposure to a continuous indirect spray of salt water solution, prepared in accordance with the requirements of the test standard and acidified (pH 2.8–3.0) by the addition of acetic acid. This spray is set to fall-out on to the specimens at a rate of 1–2 ml/80 cm²/hour. This is followed by a 2 hour exposure to an air drying (purge) climate. This is followed by 3.25 hours exposure to a high humidity climate which gradually rises to between 65% RH and 95% RH. The entire test cycle is at a constant chamber temperature of 49 °C. The number of cycle repeats and therefore the test duration is variable.[3]
ASTM G85 Annex A3 – Seawater Acidified Test (cyclic)
This test can be used to test the relative resistance to corrosion of coated or uncoated aluminium alloys and other metals, when exposed to a changing climate of acidified synthetic seawater spray, followed by a high humidity, both at an elevated temperature. This test is also referred to as a SWAAT test.
Test specimens are placed in an enclosed chamber, and exposed to a changing climate that comprises the following 2 part repeating cycle. First, a 30 minute exposure to a continuous indirect spray of synthetic seawater solution, prepared in accordance with the requirements of the test standard and acidified (pH 2.8–3.0) by the addition of acetic acid. This spray is set to fall-out on to the specimens at a rate of 1–2 ml/80 cm²/hour. This is followed by a 90 minute exposure to a high humidity climate (above 98% RH). The entire test cycle is at a constant chamber temperature of 49 °C (may be reduced to 24–35 °C for organically coated specimens). The number of cycle repeats and therefore the test duration is variable.[3]
ASTM G85 Annex A4 – SO2 Salt Spray Test (cyclic)
This test can be used to test the relative resistance to corrosion of product samples that are likely to encounter a combined SO2/salt spray/acid rain environment during their usual service life.
Test specimens are placed in an enclosed chamber, and exposed to 1 of 2 possible changing climate cycles. In either case, the exposure to salt spray may be salt water spray or synthetic sea water prepared in accordance with the requirements of the test standard. The most appropriate test cycle and spray solutions are to be agreed between parties.
The first climate cycle comprises a continuous indirect spray of neutral (pH 6.5–7.2) salt water/synthetic seawater solution, which falls-out on to the specimens at a rate of 1–2 ml/80 cm²/hour. During this spraying, the chamber is dosed with SO2 gas at a rate of 35 cm³/minute/m³ of chamber volume, for 1 hour in every 6 hours of spraying. The entire test cycle is at a constant chamber temperature of 35 °C. The number of cycle repeats and therefore the test duration is variable.
The second climate cycle comprises 0.5 hours of continuous indirect spray of neutral (pH 6.5–7.2) salt water/synthetic seawater solution, which falls-out on to the specimens at a rate of 1–2 ml/80 cm²/hour. This is followed by 0.5 hours of dosing with SO2 gas at a rate of 35 cm³/minute/m³ of chamber volume. This is followed by 2 hours of high humidity soak. The entire test cycle is at a constant chamber temperature of 35 °C. The number of cycle repeats and therefore the test duration is variable.[3]
ASTM G85 Annex A5 – Dilute Electrolyte Salt Fog/Dry Test (cyclic)
This test can be used to test the relative resistance to corrosion of paints on steel when exposed to a changing climate of dilute salt spray at ambient temperature, followed by air drying at elevated temperature.
It is a popular test in the surface coatings industry, where it is also referred to as the PROHESION™ test.
Test specimens are placed in an enclosed chamber, and exposed to a changing climate with the following 2-part cycle.
First, a 1-hour exposure to a continuous indirect spray of salt water solution, prepared in accordance with the requirements of the test standard and acidified (pH 3.1–3.3) by the addition of acetic acid.
This spray is set to fall on the specimens at a rate of 1–2 ml/80 cm²/hour, in an ambient chamber temperature (21–27 °C). This is followed by a 1-hour exposure to an air drying (purge) climate at 35 °C.
The cycle repeats until the desired duration has been achieved.[3]
Chamber construction, testing procedure and testing parameters are standardized under national and international standards, such as ASTM B 117 and ISO 9227. These standards describe the necessary information to carry out this test; testing parameters such as temperature, air pressure of the sprayed solution, preparation of the spraying solution, concentration, pH, etc. Daily checking of testing parameters is necessary to show compliance with the standards, so records shall be maintained accordingly. ASTM B117 and ISO 9227 are widely used as reference standards. Testing cabinets are manufactured according to the specified requirements here. 
However, these testing standards neither provide information of testing periods for the coatings to be evaluated, nor the appearance of corrosion products in form of salts. Requirements are agreed between customer and manufacturer. In the automotive industry requirements are specified under material specifications. Different coatings have different behavior in salt spray test and consequently, test duration will differ from one type of coating to another. For example, a typical electroplated zinc and yellow passivated steel part lasts 96 hours in salt spray test without white rust. Electroplated zinc-nickel steel parts can last more than 720 hours in NSS test without red rust (or 48 hours in CASS test without red rust) Requirements are established in test duration (hours) and coatings shall comply with minimum testing periods.
Artificial seawater which is sometimes used for Salt Spray Testing can be found at ASTM International. The standard for Artificial Seawater is ASTM D1141-98 which is the standard practice for the preparation of substitute ocean water.
Typical coatings that can be evaluated with this method are:
Hot-dip galvanized surfaces are not generally tested in a salt spray test (see ISO 1461 or ISO 10684). Hot-dip galvanizing produces zinc carbonates when exposed to a natural environment, thus protecting the coating metal and reducing the corrosion rate. The zinc carbonates are not produced when a hot-dip galvanized specimen is exposed to a salt spray fog, therefore this testing method does not give an accurate measurement of corrosion protection. ISO 9223 gives the guidelines for proper measurement of corrosion resistance for hot-dip galvanized specimens.
Painted surfaces with an underlying hot-dip galvanized coating can be tested according to this method. See ISO 12944-6.
Testing periods range from a few hours (e.g. 8 or 24 hours of phosphated steel) to more than a month (e.g. 720 hours of zinc-nickel coatings, 1000 hours of certain zinc flake coatings).
"
Rust - Wikipedia," Rust is an iron oxide, a usually reddish-brown oxide formed by the reaction of iron and oxygen in the catalytic presence of water or air moisture. Rust consists of hydrous iron(III) oxides (Fe2O3·nH2O) and iron(III) oxide-hydroxide (FeO(OH), Fe(OH)3), and is typically associated with the corrosion of refined iron.
Given sufficient time, any iron mass, in the presence of water and oxygen, could eventually convert entirely to rust. Surface rust is commonly flaky and friable, and provides no passivational protection to the underlying iron, unlike the formation of patina on copper surfaces. Rusting is the common term for corrosion of elemental iron and its alloys such as steel. Many other metals undergo similar corrosion, but the resulting oxides are not commonly called ""rust"".[1]
Several forms of rust are distinguishable both visually and by spectroscopy, and form under different circumstances.[2] Other forms of rust include the result of reactions between iron and chloride in an environment deprived of oxygen. Rebar used in underwater concrete pillars, which generates green rust, is an example. Although rusting is generally a negative aspect of iron, a particular form of rusting, known as stable rust, causes the object to have a thin coating of rust over the top, and if kept in low relative humidity, makes the ""stable"" layer protective to the iron below, but not to the extent of other oxides such as aluminium oxide on aluminium.[3]
Rust is a general name for a complex of oxides and hydroxides of iron,[4] which occur when iron or some alloys that contain iron are exposed to oxygen and moisture for a long period of time. Over time, the oxygen combines with the metal  forming  new compounds collectively called rust. Although rust may generally be termed as ""oxidation"", that term is much more general and describes a vast number of processes involving the loss of electrons or increased oxidation state, as part of a reaction. The best-known of these reactions involve oxygen, hence the name ""oxidation"". The terms ""rust"" and ""rusting"" only mean oxidation of iron and its resulting products. Many other oxidation reactions exist which do not involve iron or produce rust. But only iron or alloys that contain iron can rust. However, other metals can corrode in similar ways.
The main catalyst for the rusting process is water. Iron or steel structures might appear to be solid, but water molecules can penetrate the microscopic pits and cracks in any exposed metal. The hydrogen atoms present in water molecules can combine with other elements to form acids, which will eventually cause more metal to be exposed. If chloride ions are present, as is the case with saltwater, the corrosion is likely to occur more quickly. Meanwhile, the oxygen atoms combine with metallic atoms to form the destructive oxide compound. As the atoms combine, they weaken the metal, making the structure brittle and crumbly.
When iron is in contact with water and oxygen it rusts.[5] If salt is present, for example in seawater or salt spray, the iron tends to rust more quickly, as a result of electrochemical reactions. Iron metal is relatively unaffected by pure water or by dry oxygen. As with other metals, like aluminium, a tightly adhering oxide coating, a passivation layer, protects the bulk iron from further oxidation. The conversion of the passivating ferrous oxide layer to rust results from the combined action of two agents, usually oxygen and water.
Other degrading solutions are sulfur dioxide in water and carbon dioxide in water. Under these corrosive conditions, iron hydroxide species are formed. Unlike ferrous oxides, the hydroxides do not adhere to the bulk metal. As they form and flake off from the surface, fresh iron is exposed, and the corrosion process continues until either all of the iron is consumed or all of the oxygen, water, carbon dioxide, or sulfur dioxide in the system are removed or consumed.[6]
When iron rusts, the oxides take up more volume than the original metal; this expansion can generate enormous forces, damaging structures made with iron.  See economic effect for more details.
The rusting of iron is an electrochemical process that begins with the transfer of electrons from iron to oxygen.[7] The iron is the reducing agent (gives up electrons) while the oxygen is the oxidising agent (gains electrons). The rate of corrosion is affected by water and accelerated by electrolytes, as illustrated by the effects of road salt on the corrosion of automobiles. The key reaction is the reduction of oxygen:
Because it forms hydroxide ions, this process is strongly affected by the presence of acid. Likewise, the corrosion of most metals by oxygen is accelerated at low pH. Providing the electrons for the above reaction is the oxidation of iron that may be described as follows:
The following redox reaction also occurs in the presence of water and is crucial to the formation of rust:
In addition, the following multistep acid–base reactions affect the course of rust formation:
as do the following dehydration equilibria:
From the above equations, it is also seen that the corrosion products are dictated by the availability of water and oxygen. With limited dissolved oxygen, iron(II)-containing materials are favoured, including FeO and black lodestone or magnetite (Fe3O4). High oxygen concentrations favour ferric materials with the nominal formulae Fe(OH)3−xO​x⁄2. The nature of rust changes with time, reflecting the slow rates of the reactions of solids.[5]
Furthermore, these complex processes are affected by the presence of other ions, such as Ca2+, which serve as electrolytes which accelerate rust formation, or combine with the hydroxides and oxides of iron to precipitate a variety of Ca, Fe, O, OH species.
The onset of rusting can also be detected in the laboratory with the use of ferroxyl indicator solution. The solution detects both Fe2+ ions and hydroxyl ions. Formation of Fe2+ ions and hydroxyl ions are indicated by blue and pink patches respectively.
Because of the widespread use and importance of iron and steel products, the prevention or slowing of rust is the basis of major economic activities in a number of specialized technologies. A brief overview of methods is presented here; for detailed coverage, see the cross-referenced articles.
Rust is permeable to air and water, therefore the interior metallic iron beneath a rust layer continues to corrode. Rust prevention thus requires coatings that preclude rust formation.
Stainless steel forms a passivation layer of chromium(III) oxide.[8][9] Similar passivation behavior occurs with magnesium, titanium, zinc, zinc oxides, aluminium, polyaniline, and other electroactive conductive polymers.[citation needed]
Special ""weathering steel"" alloys such as Cor-Ten rust at a much slower rate than normal, because the rust adheres to the surface of the metal in a protective layer. Designs using this material must include measures that avoid worst-case exposures, since the material still continues to rust slowly even under near-ideal conditions.[citation needed]
Galvanization consists of an application on the object to be protected of a layer of metallic zinc by either hot-dip galvanizing or electroplating. Zinc is traditionally used because it is cheap, adheres well to steel, and provides cathodic protection to the steel surface in case of damage of the zinc layer. In more corrosive environments (such as salt water), cadmium plating is preferred. Galvanization often fails at seams, holes, and joints where there are gaps in the coating. In these cases, the coating still provides some partial cathodic protection to iron, by acting as a galvanic anode and corroding itself instead of the underlying protected metal. The protective zinc layer is consumed by this action, and thus galvanization provides protection only for a limited period of time.
More modern coatings add aluminium to the coating as zinc-alume; aluminium will migrate to cover scratches and thus provide protection for a longer period. These approaches rely on the aluminium and zinc oxides reprotecting a once-scratched surface, rather than oxidizing as a sacrificial anode as in traditional galvanized coatings. In some cases, such as very aggressive environments or long design life, both zinc and a coating are applied to provide enhanced corrosion protection.
Typical galvanization of steel products which are to be subjected to normal day-to-day weathering in an outside environment consists of a hot-dipped 85 µm zinc coating. Under normal weather conditions, this will deteriorate at a rate of 1 µm per year, giving approximately 85 years of protection.[citation needed]
Cathodic protection is a technique used to inhibit corrosion on buried or immersed structures by supplying an electrical charge that suppresses the electrochemical reaction. If correctly applied, corrosion can be stopped completely. In its simplest form, it is achieved by attaching a sacrificial anode, thereby making the iron or steel the cathode in the cell formed. The sacrificial anode must be made from something with a more negative electrode potential than the iron or steel, commonly zinc, aluminium, or magnesium. The sacrificial anode will eventually corrode away, ceasing its protective action unless it is replaced in a timely manner.
Cathodic protection can also be provided by using a special-purpose electrical device to appropriately induce an electric charge.[10]
Rust formation can be controlled with coatings, such as paint, lacquer, varnish, or wax tapes[11] that isolate the iron from the environment. Large structures with enclosed box sections, such as ships and modern automobiles, often have a wax-based product (technically a ""slushing oil"") injected into these sections. Such treatments usually also contain rust inhibitors. Covering steel with concrete can provide some protection to steel because of the alkaline pH environment at the steel–concrete interface. However, rusting of steel in concrete can still be a problem, as expanding rust can fracture or slowly ""explode"" concrete from within.[citation needed]
As a closely related example, iron bars were used to reinforce stonework of the Parthenon in Athens, Greece, but caused extensive damage by rusting, swelling, and shattering the marble components of the building.[citation needed]
When only temporary protection is needed for storage or transport, a thin layer of oil, grease, or a special mixture such as Cosmoline can be applied to an iron surface. Such treatments are extensively used when ""mothballing"" a steel ship, automobile, or other equipment for long-term storage.
Special antiseize lubricant mixtures are available, and are applied to metallic threads and other precision machined surfaces to protect them from rust. These compounds usually contain grease mixed with copper, zinc, or aluminium powder, and other proprietary ingredients.[citation needed]
Bluing is a technique that can provide limited resistance to rusting for small steel items, such as firearms; for it to be successful, a water-displacing oil is rubbed onto the blued steel and other steel.
Corrosion inhibitors, such as gas-phase or volatile inhibitors, can be used to prevent corrosion inside sealed systems. They are not effective when air circulation disperses them, and brings in fresh oxygen and moisture.
Rust can be avoided by controlling the moisture in the atmosphere.[12] An example of this is the use of silica gel packets to control humidity in equipment shipped by sea.
Rust removal from small iron or steel objects by electrolysis can be done in a home workshop using simple materials such as a plastic bucket filled with an electrolyte consisting of washing soda dissolved in tap water, a length of rebar suspended vertically in the solution to act as an anode, another laid across the top of the bucket to act as a support for suspending the object, baling wire to suspend the object in the solution from the horizontal rebar, and a battery charger as a power source in which the positive terminal is clamped to the anode and the negative terminal is clamped to the object to be treated which becomes the cathode.[13]
Rust may be treated with commercial products known as rust converter which contain tannic acid or phosphoric acid which combines with rust; removed with organic acids like citric acid and vinegar or the stronger hydrochloric acid; or removed with chelating agents as in some commercial formulations or even a solution of molasses.[14]

Rust is associated with the degradation of iron-based tools and structures. As rust has a much higher volume than the originating mass of iron, its buildup can also cause failure by forcing apart adjacent parts — a phenomenon sometimes known as ""rust packing"". It was the cause of the collapse of the Mianus river bridge in 1983, when the bearings rusted internally and pushed one corner of the road slab off its support.
Rust was an important factor in the Silver Bridge disaster of 1967 in West Virginia, when a steel suspension bridge collapsed in less than a minute, killing 46 drivers and passengers on the bridge at the time. The Kinzua Bridge in Pennsylvania was blown down by a tornado in 2003, largely because the central base bolts holding the structure to the ground had rusted away, leaving the bridge anchored by gravity alone.
Reinforced concrete is also vulnerable to rust damage. Internal pressure caused by expanding corrosion of concrete-covered steel and iron can cause the concrete to spall, creating severe structural problems. It is one of the most common failure modes of reinforced concrete bridges and buildings.
The collapsed Silver Bridge, as seen from the Ohio side
The Kinzua Bridge after it collapsed
Rust is a commonly used metaphor for slow decay due to neglect, since it gradually converts robust iron and steel metal into a soft crumbling powder. A wide section of the industrialized American Midwest and American Northeast, once dominated by steel foundries, the automotive industry, and other manufacturers, has experienced harsh economic cutbacks that have caused the region to be dubbed the ""Rust Belt"".
In music, literature, and art, rust is associated with images of faded glory, neglect, decay, and ruin.
Rusted and pitted struts of the 70-year-old Nandu River Iron Bridge
Concentric rust patterns breaking through a painted surface
A rusted but otherwise intact Pineapple grenade that was previously buried in the ground near Opheusden, Netherlands
Colors and porous surface texture of rust
"
Kinzua Bridge - Wikipedia," 
The Kinzua Bridge or the Kinzua Viaduct (/ˈkɪnzuː/,[2] /-zuːə/) was a railroad trestle that spanned Kinzua Creek in McKean County in the U.S. state of Pennsylvania. The bridge was 301 feet (92 m) tall and 2,052 feet (625 m) long. Most of its structure collapsed during a tornado in July 2003.
The bridge was originally built from wrought iron in 1882 and was billed as the ""Eighth Wonder of the World"", holding the record as the tallest railroad bridge in the world for two years. In 1900, the bridge was dismantled and simultaneously rebuilt out of steel to allow it to accommodate heavier trains. It stayed in commercial service until 1959 and was sold to the Government of Pennsylvania in 1963, becoming the centerpiece of a state park.
Restoration of the bridge began in 2002, but before it was finished, a tornado struck the bridge in 2003, causing a large portion of the bridge to collapse. Corroded anchor bolts holding the bridge to its foundations failed, contributing to the collapse.
Before its collapse, the Kinzua Bridge was ranked as the fourth-tallest railway bridge in the United States.[3] It was listed on the National Register of Historic Places in 1977 and as a National Historic Civil Engineering Landmark by the American Society of Civil Engineers in 1982. The ruins of the Kinzua Bridge are in Kinzua Bridge State Park off U.S. Route 6 near the borough of Mount Jewett, Pennsylvania.
In 1882, Thomas L. Kane, president of the New York, Lake Erie and Western Railway (NYLE&W), was faced with the challenge of building a branch line off the main line in Pennsylvania, from Bradford south to the coal fields in Elk County.[4][5] The fastest way to do so was to build a bridge to cross the Kinzua Valley. The only other alternative to building a bridge would have been to lay an additional 8 miles (13 km) of track over rough terrain.[6] When built, the bridge was larger than any that had been attempted, and over twice as large as the largest similar structure at the time: the Portage Bridge over the Genesee River in western New York.[7]
The first Kinzua Bridge was built by a crew of 40 from 1,552 short tons (1,408 t) of wrought iron in just 94 working days, between May 10 and August 29, 1882.[1][8][9] The reason for the short construction time was that scaffolding was not used in the bridge's construction; instead a gin pole was used to build the first tower, then a traveling crane was built atop it and used in building the second tower.[8][10] The process was then repeated across all 20 towers.[4]
The bridge was designed by the engineer Octave Chanute and was built by the Phoenix Iron Works, which specialized in producing patented, hollow iron tubes called ""Phoenix columns"".[4] Because of the design of these columns, it was often mistakenly believed that the bridge had been built out of wooden poles.[10] The bridge's 110 sandstone masonry piers were quarried from the hillside used for the foundation of the bridge.[1][4] The tallest tower had a base that was 193 feet (59 m) wide.[1] The bridge was designed to support a load of 266 short tons (241 t),[5] and was estimated to cost between $167,000 and $275,000.[8][11][12]
On completion, the bridge was the tallest and longest railroad bridge in the world and was advertised as the ""Eighth Wonder of the World"".[3][10] Six of the bridge's 20 towers were taller than the Brooklyn Bridge.[1] Excursion trains from as far away as Buffalo, New York, and Pittsburgh would come just to cross the Kinzua Bridge,[10] which held the height record until the Garabit viaduct, 401 feet (122 m) tall, was completed in France in 1884.[13] Trains crossing the bridge were restricted to a speed of 5 miles per hour (8.0 km/h) because the locomotive, and sometimes the wind, caused the bridge to vibrate.[10] People sometimes visited the bridge in hopes of finding the loot of a bank robber, who supposedly hid $40,000 in gold and currency under or near it.[4][10]
By 1893, the NYLE&W had gone bankrupt and was merged with the Erie Railroad, which became the owner of the bridge.[13] By the start of the 20th century, locomotives were almost 85 percent heavier, and the iron bridge could no longer safely carry trains.[1] The last traffic crossed the old bridge on May 14, 1900, and removal of the old iron began on May 24.
The new bridge was designed by C.R. Grimm and was built by the Elmira Bridge Company out of 3,358 short tons (3,046 t) of steel, at a cost of $275,000.[14] Construction began on May 26, starting from both ends of the old bridge. A crew of between 100 and 150 worked 10-hour days for almost four months to complete the new steel frame.  Two Howe truss ""timber travelers"", each 180 feet (50 m) long and 16 feet (5 m) deep, were used to build the towers.[15] Each ""traveler"" was supported by a pair of the original wrought-iron towers, separated by the one that was to be replaced.  After the middle tower was demolished and a new steel one built in its place, the traveler was moved down the line by one tower and the process was repeated.  Construction of each new tower and the spans adjoining it took one week to complete.[15] The bolts used to hold the towers to the anchor blocks were reused from the first bridge, which would eventually play a major role in the bridge's demise.[16] Grimm, the designer of the bridge, later admitted that the bolts should have been replaced.[17]
The Kinzua Viaduct reopened to traffic on September 25, 1900. The new bridge was able to safely accommodate one of the largest steam locomotives in the world, the 511-short-ton (464 t) Big Boy.[18][1][13][14] The Erie Railroad maintained a station at the Kinzua Viaduct. Constructed between 1911 and 1916,[19] the station was not manned by an agent.[20] The station was closed sometime between 1923 and 1927.[21][22]
Train crews would sometimes play a trick on a brakeman on his first journey on the line. When the train was a short distance from the bridge, the crew would send the brakeman over the rooftops of the cars to check on a small supposed problem. As the train crossed the bridge, the rookie ""suddenly found himself terrified, staring down three hundred feet (90 m) from the roof of a rocking boxcar"".[13] Even after being reconstructed, the bridge still had a speed limit of 5 miles per hour (8 km/h). As the bridge aged, heavy trains pulled by two steam locomotives had to stop so the engines could cross the bridge one at a time. Diesel locomotives were lighter and did not face that limit; the bridge was last crossed by a steam locomotive on October 5, 1950.[13]
The Erie Railroad obtained trackage rights on the nearby Baltimore and Ohio Railroad (B&O) line in the late 1950s, allowing it to bypass the aging Kinzua Bridge. Regular commercial service ended on June 21, 1959, and the Erie sold the bridge to the Kovalchick Salvage Company of Indiana, Pennsylvania, for $76,000.[14] The bridge was reopened for one day in October 1959 when a wreck on the B&O line forced trains to be rerouted across the bridge.[13] According to the American Society of Civil Engineers, the Kinzua Bridge ""was a critical structure in facilitating the transport of coal from Northwestern Pennsylvania to the Eastern Great Lakes region, and is credited with causing an increase in coal mining that led to significant economic growth.""[1]
Nick Kovalchick, head of the Kovalchick Salvage Company, which then owned the bridge, was reluctant to dismantle it. On seeing it for the first time he is supposed to have said ""There will never be another bridge like this.""[23] Kovalchick worked with local groups who wanted to save the structure, and Pennsylvania Governor William Scranton signed a bill into law on August 12, 1963, to purchase the bridge and nearby land for $50,000 and create Kinzua Bridge State Park.[14][24] The deed for the park's 316 acres (128 ha) was recorded on January 20, 1965, and the park was opened to the public in 1970.[24]
An access road to the park was built in 1974, and new facilities there included a parking lot, drinking water and toilets, and installation of a fence on the bridge deck.[24] On July 5, 1975, there was an official ribbon cutting ceremony for the park, which ""was and is unique in the park system"" since ""its centerpiece is a man-made structure"".[23] The bridge was listed on the National Register of Historic Places on August 29, 1977,[25] and was named to the National Register of Historic Civil Engineering Landmarks by the American Society of Civil Engineers on June 26, 1982.[24]
The Knox and Kane Railroad (KKRR) operated sightseeing trips from Kane through the Allegheny National Forest and over the Kinzua Bridge from 1987 until the bridge was closed in 2002.[26] In 1988 it operated the longest steam train excursion in the United States, a 97-mile (156 km) round trip to the bridge from the village of Marienville in Forest County, with a stop in Kane. The New York Times described being on the bridge as ""more akin to ballooning than railroading"" and noted ""You stare straight out with nothing between you and an immense sea of verdure a hundred yards [91 m] below.""[27] The railroad still operated excursions through the forest and stopped at the bridge's western approach until October 2004.[26]
As of 2009, Kinzua Bridge State Park is a 329-acre (133 ha) Pennsylvania state park surrounding the bridge and the Kinzua Valley. The park is located off of U.S. Route 6 north of Mount Jewett in Hamlin and Keating Townships. A scenic overlook within the park allows views of the fallen bridge and of the valley, and is also a prime location to view the fall foliage in mid-October.[28] The park has a shaded picnic area with a centrally located modern restroom.[6] Before the bridge's collapse, visitors were allowed on or under the bridge and hiking was allowed in the valley around the bridge.[6] In September 2002 the bridge was closed even to pedestrian traffic.[9] About 100 acres (40 ha) of Kinzua Bridge State Park are open to hunting. Common game species are turkey, bear and deer.[6]
Since 2002, the Kinzua Bridge had been closed to all ""recreational pedestrian and railroad usage"" after it was determined that the structure was at risk to high winds.[6] Engineers had determined that during high winds, the bridge's center of gravity could shift, putting weight onto only one side of the bridge and causing it to fail.[6] An Ohio-based bridge construction and repair company had started work on restoring the Kinzua Bridge in February 2003.[6]
On July 21, 2003, construction workers had packed up and were starting to leave for the day when a storm arrived.[29]  A tornado within the storm struck the Kinzua Bridge, snapping and uprooting nearby trees, as well as causing 11 of the 20 bridge towers to collapse. There were no human deaths or injuries. The tornado was produced by a mesoscale convective system (MCS), a complex of strong thunderstorms, that had formed over an area that included eastern Ohio, western Pennsylvania, western New York, and southern Ontario.[30] The MCS traveled east at around 40 miles per hour (60 km/h). As the MCS crossed northwestern Pennsylvania, it formed into a distinctive comma shape. The northern portion of the MCS contained a long-lived mesocyclone, a thunderstorm with a rotating updraft that is often conducive to tornados.[31]
At approximately 15:20 EDT (19:20 UTC), the tornado touched down in Kinzua Bridge State Park, 1 mile (1.6 km) from the Kinzua Bridge. The tornado, classified as F1 on the Fujita scale, passed by the bridge and continued another 2.5 miles (4 km) before it lifted. It touched down again 2 miles (3 km) from Smethport and traveled another 3 miles (4.8 km) before finally dissipating. It was estimated to have been 1⁄3-mile (540 m) wide and it left a path 3.5 miles (5.6 km) long.[32] The same storm also spawned an F3 tornado in nearby Potter County.[33]
When the tornado touched down, the winds had increased to at least 94 miles per hour (151 km/h) and were coming from the east, perpendicular to the bridge, which ran north–south. An investigation determined that Towers 10 and 11 had collapsed first, in a westerly direction.[34] Meanwhile, Towers 12 through 14 had actually been picked up off their foundations, moved slightly to the northwest and set back down intact and upright, held together by only the railroad tracks on the bridge. Next towers four through nine collapsed to the west, twisting clockwise, as the tornado started to move northward.[34] As it moved north, inflow winds came in from the south and caused Towers 12, 13, and 14 to finally collapse towards the north, twisting counterclockwise.[34]
The failures were caused by the badly rusted base bolts holding the bases of the towers to concrete anchor blocks embedded into the ground.[6] An investigation determined that the tornado had a wind speed of at least 94 miles per hour (151 km/h), which applied an estimated 90 short tons-force (800 kN) of lateral force against the bridge.[35] The investigation also hypothesized that the whole structure oscillated laterally four to five times before fatigue started to cause the base bolts to fail. The towers fell intact in sections and suffered damage upon impact with the ground.[36] The century-old bridge was destroyed in less than 30 seconds.[37]
The state decided not to rebuild the Kinzua Bridge, which would have cost an estimated $45 million. Instead, it was proposed that the ruins be used as a visitor attraction to show the forces of nature at work.[37] Kinzua Bridge State Park had attracted 215,000 visitors annually before the bridge collapsed,[38] and was chosen by the Pennsylvania Bureau of Parks for its list of ""Twenty Must-See Pennsylvania State Parks"".[39] The viaduct and its collapse were featured in the History Channel's Life After People as an example of how corrosion and high winds would eventually lead to the collapse of any steel structure.[40] The bridge was removed from the National Register of Historic Places on July 21, 2004.[41]
The Knox and Kane Railroad was forced to suspend operations in October 2004 after a 75 percent decline in the number of passengers, possibly brought about by the collapse of the Kinzua Bridge.[42] The Kovalchick Corporation bought the Knox and Kane's tracks and all other property owned by the railroad, including the locomotives and rolling stock. The Kovalchick Corporation also owns the East Broad Top Railroad and was the company that owned the Kinzua Bridge before selling it to the state in 1963.[42] The company disclosed plans in 2008 to remove the tracks and sell them for scrap. The right-of-way would then be used to establish a rail trail.[42]
Pennsylvania released $700,000 to design repairs on the remaining towers and plan development of the new park facilities in June 2005.[43] In late 2005, the Pennsylvania Department of Conservation and Natural Resources (DCNR) put forward an $8 million proposal for a new observation deck and visitors' center, with plans to allow access to the bridge and a hiking trail giving views of the fallen towers.[37] The Kinzua Sky Walk was opened on September 15, 2011, in a ribbon-cutting ceremony.[44] The Sky Walk consists of a pedestrian walkway to an observation deck with a glass floor at the end of the bridge that allows views of the bridge and the valley directly below. The walkway cost $4.3 million to construct, but is estimated to bring in $11.5 million in tourism revenue for the region.[45]
Notes
Sources

"
Pennsylvania Department of Conservation and Natural Resources - Wikipedia," The Pennsylvania Department of Conservation and Natural  Resources (DCNR), established on July 1, 1995, is the agency in the U.S. State of Pennsylvania responsible for maintaining and preserving the state's 121 state parks and 20 state forests; providing information on the state's natural resources; and working with communities to benefit local recreation and natural areas.[1] The agency has its headquarters in the Rachel Carson State Office Building in Harrisburg.[2]
The department was formed when then-governor Tom Ridge split the Department of Environmental Resources (DER) into the DCNR and Department of Environmental Protection (DEP).
The DCNR is host to many different environmental education programs throughout the summer months. These range from topics such as ""Leave No Trace"" hiking/camping policy to the different wildlife and plant species of many of the state parks.
Pennsylvania DCNR rangers act much like National Park Rangers do. They routinely check on cabins and campsites, offer insightful answers to visitors questions, and help to maintain calmness throughout the parks.  They have full arrest powers while in park lands and carry side arms.  However, they do not have jurisdiction over Pennsylvania State Game Lands, which are patrolled by Wildlife Conservation Officers employed by the Pennsylvania Game Commission.  DCNR rangers enforce game laws as well as fishing and boating laws in state parks. However, the Pennsylvania Fish and Boat Commission is completely independent of the Pennsylvania Game Commission.  Both agencies are independent of DCNR, but work in cooperation with each other.
The DCNR comprises the following subunits:[4][5][6]
"
Valley Forge Pilgrimage - Wikipedia," The Valley Forge Pilgrimage and Encampment is the oldest annual scouting event in the world.[citation needed] It was first held on February 22, 1913 (George Washington's 181st birthday), and has been held every year since.[1] The event is hosted by the Cradle of Liberty Council and commemorates the soldiers of the Continental Army who braved the winter of 1777-78 at Valley Forge. Each year more than 2,000 brave the winter chill of the Delaware Valley to participate.
Boy Scouts of America (BSA) was only three years old, and was sweeping the country when approximately 300 Scouts from Philadelphia and suburban Delaware and Montgomery Counties took part in the first pilgrimage.[2]
Typical attendance ranged from 125 to 275 Scouts and leaders until the Great Depression. The event has been held every year, even during the Depression and World War II, though the event was then held at a reduced scale. In the early years, Scouts traveled to Valley Forge by train and foot, arriving at the Valley Forge Train Station.
The event was organized by an Episcopal priest, Rev. Dr. W. Herbert Burk of Norristown.[3] Burk had also been instrumental in building the Washington Memorial Chapel at Valley Forge. Burk continued to be involved in the planning for the annual pilgrimage for the next two decades. The term ""pilgrimage"" was chosen because every participating troop lined up in formation with their flags and paraded to the chapel for the memorial service for George Washington.
The first official overnight Valley Forge Encampment in connection with a pilgrimage was in 1948. Prior to that there were many unofficial campouts on the site, as the area was relatively rural and unpopulated. General John Pershing and Daniel Carter Beard have both been featured guests. The biggest crowd was at the 1932 pilgrimage, which marked Washington's 200th birthday. An estimated 9,000 Boy Scouts from Pennsylvania and neighboring states, plus 10,000 adults and children, were on hand.[4]
The event itself is divided into two parts: the Encampment, in which Scouts camp from Friday through Sunday on the Presidents' Day weekend, and the Pilgrimage which is all day Saturday. During the Pilgrimage, participants move around the Valley Forge NHP to hear volunteer interpreters from the National Park Service tell how the Continentals lived. Replicas of the original huts provide a glimpse of daily life. Conditions during the winter were poor, so the stories are also told how more than 2,000 soldiers died from typhus, dysentery, typhoid and pneumonia.
The Valley Forge Historical Trail is a 9-mile hiking trail through the Valley Forge National Park, the elevation gain of this trail is 833 feet. The hike starts and ends at the Valley Forge Visitor's Center. Hiking this trail as needed requires the use of a map, compass, along with the skills and abilities to use them, orienteering is also required. There is little access to water on the trail, although water can be retrieved about halfway through at the train station restrooms. It is normally advised that you bring your own water along as well. [5]
Coordinates: .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}40°05′49″N 75°26′29″W﻿ / ﻿40.09691°N 75.44139°W﻿ / 40.09691; -75.44139
"
George Washington - Wikipedia," 

George Washington (February 22, 1732[b] – December 14, 1799) was an American political leader, military general, statesman, and Founding Father who served as the first president of the United States from 1789 to 1797. Previously, he led Patriot forces to victory in the nation's War for Independence. He presided at the Constitutional Convention of 1787, which established the U.S. Constitution
and a federal government. Washington has been called the ""Father of His Country"" for his manifold leadership in the formative days of the new nation.
Washington received his initial military training and command with the Virginia Regiment during the French and Indian War. He was later elected to the Virginia House of Burgesses and was named a delegate to the Continental Congress, where he was appointed Commanding General of the Continental Army. He commanded American forces, allied with France, in the defeat and surrender of the British during the Siege of Yorktown. He resigned his commission after the Treaty of Paris in 1783.
Washington played a key role in adopting and ratifying the Constitution and was then twice elected president by the Electoral College. He implemented a strong, well-financed national government while remaining impartial in a fierce rivalry between cabinet members Thomas Jefferson and Alexander Hamilton. During the French Revolution, he proclaimed a policy of neutrality while sanctioning the Jay Treaty. He set enduring precedents for the office of president, including the title ""Mr. President"", and his Farewell Address is widely regarded as a pre-eminent statement on republicanism.
Washington owned slaves, and, in order to preserve national unity, he supported measures passed by Congress to protect slavery. He later became troubled with the institution of slavery and freed his slaves in a 1799 will. He endeavored to assimilate Native Americans into Anglo-American culture, but combated indigenous resistance during instances of violent conflict. He was a member of the Anglican Church and the Freemasons, and he urged broad religious freedom in his roles as general and president. Upon his death, he was eulogized as ""first in war, first in peace, and first in the hearts of his countrymen"". He has been memorialized by monuments, art, geographical locations, stamps, and currency, and many scholars and polls rank him among the greatest U.S. presidents.
The Washington family was a wealthy Virginia family which had made its fortune in land speculation.[10] Washington's great-grandfather John Washington immigrated in 1656 from Sulgrave, England, to the English colony of Virginia where he accumulated 5,000 acres (2,000 ha) of land, including Little Hunting Creek on the Potomac River. George Washington was born on February 22, 1732, at Popes Creek in Westmoreland County, Virginia,[11] and was the first of six children of Augustine and Mary Ball Washington.[12] His father was a justice of the peace and a prominent public figure who had three additional children from his first marriage to Jane Butler.[13] The family moved to Little Hunting Creek in 1735, then to Ferry Farm near Fredericksburg, Virginia, in 1738. When Augustine died in 1743, Washington inherited Ferry Farm and ten slaves; his older half-brother Lawrence inherited Little Hunting Creek and renamed it Mount Vernon.[14]
Washington did not have the formal education his elder brothers received at Appleby Grammar School in England, but he did learn mathematics, trigonometry, and land surveying. He was a talented draftsman and map-maker. By early adulthood he was writing with ""considerable force"" and ""precision"";[15] however, his writing displayed little wit or humor. In pursuit of admiration, status, and power, he tended to attribute his shortcomings and failures to someone else's ineffectuality.[16]
Washington often visited Mount Vernon and Belvoir, the plantation that belonged to Lawrence's father-in-law William Fairfax. Fairfax became Washington's patron and surrogate father, and Washington spent a month in 1748 with a team surveying Fairfax's Shenandoah Valley property.[17] He received a surveyor's license the following year from the College of William & Mary;[c] Fairfax appointed him surveyor of Culpeper County, Virginia, and he thus familiarized himself with the frontier region, resigning from the job in 1750. By 1752 he had bought almost 1,500 acres (600 ha) in the Valley and owned 2,315 acres (937 ha).[19]
In 1751, Washington made his only trip abroad when he accompanied Lawrence to Barbados, hoping the climate would cure his brother's tuberculosis.[20] Washington contracted smallpox during that trip, which immunized him but left his face slightly scarred.[21] Lawrence died in 1752, and Washington leased Mount Vernon from his widow; he inherited it outright after her death in 1761.[22]
Lawrence Washington's service as adjutant general of the Virginia militia inspired George to seek a commission. Virginia's Lieutenant Governor Robert Dinwiddie appointed him as a major and as commander of one of the four militia districts. The British and French were competing for control of the Ohio Valley at the time, the British constructing forts along the Ohio River and the French doing likewise between the river and Lake Erie.[23]
In October 1753, Dinwiddie appointed Washington as a special envoy to demand that the French vacate territory which the British had claimed.[d] Dinwiddie also appointed him to make peace with the Iroquois Confederacy and to gather intelligence about the French forces.[25] Washington met with Half-King Tanacharison and other Iroquois chiefs at Logstown to secure their promise of support against the French, and his party reached the Ohio River in November. They were intercepted by a French patrol and escorted to Fort Le Boeuf where Washington was received in a friendly manner. He delivered the British demand to vacate to French commander Saint-Pierre, but the French refused to leave. Saint-Pierre gave Washington his official answer in a sealed envelope after a few days' delay, and he gave Washington's party food and extra winter clothing for the trip back to Virginia.[26] Washington completed the precarious mission in 77 days in difficult winter conditions, achieving a measure of distinction when his report was published in Virginia and in London.[27]
In February 1754, Dinwiddie promoted Washington to lieutenant colonel and second-in-command of the 300-strong Virginia Regiment, with orders to confront French forces at the Forks of the Ohio.[28] Washington set out for the Forks with half the regiment in April but soon learned a French force of 1,000 had begun construction of Fort Duquesne there. In May, having set up a defensive position at Great Meadows, he learned that the French had made camp seven miles (11 km) away; he decided to take the offensive.[29]
The French detachment proved to be only about fifty men, so Washington advanced on May 28 with a small force of Virginians and Indian allies to ambush them.[30][e] What took place, known as the Battle of Jumonville Glen or the ""Jumonville affair"", was disputed, but French forces were killed outright with muskets and hatchets. French commander Joseph Coulon de Jumonville, who carried a diplomatic message for the British to evacuate, was killed. French forces found Jumonville and some of his men dead and scalped and assumed Washington was responsible.[32] Washington blamed his translator for not communicating the French intentions.[33] Dinwiddie congratulated Washington for his victory over the French.[34] This incident ignited the French and Indian War, which later became part of the larger Seven Years' War.[35]
The full Virginia Regiment joined Washington at Fort Necessity the following month with news that he had been promoted to command of the regiment and to colonel upon the death of the regimental commander. The regiment was reinforced by an independent company of a hundred South Carolinians led by Captain James Mackay, whose royal commission outranked that of Washington, and a conflict of command ensued. On July 3, a French force attacked with 900 men, and the ensuing battle ended in Washington's surrender.[36] In the aftermath, Colonel James Innes took command of intercolonial forces, the Virginia Regiment was divided, and Washington was offered a captaincy which he refused, with resignation of his commission.[37]
In 1755, Washington served voluntarily as an aide to General Edward Braddock, who led a British expedition to expel the French from Fort Duquesne and the Ohio Country.[38] On Washington's recommendation, Braddock split the army into one main column and a lightly equipped ""flying column"".[39] Suffering from a severe case of dysentery, Washington was left behind, and when he rejoined Braddock at Monongahela the French and their Indian allies ambushed the divided army. Two-thirds of the British force became casualties, including the mortally wounded Braddock. Under the command of Lieutenant Colonel Thomas Gage, Washington, still very ill, rallied the survivors and formed a rear guard, allowing the remnants of the force to disengage and retreat.[40] During the engagement he had two horses shot from under him, and his hat and coat were bullet-pierced.[41] His conduct under fire redeemed his reputation among critics of his command in the Battle of Fort Necessity,[42] but he was not included by the succeeding commander (Colonel Thomas Dunbar) in planning subsequent operations.[43]
The Virginia Regiment was reconstituted in August 1755, and Dinwiddie appointed Washington its commander, again with the rank of colonel. Washington clashed over seniority almost immediately, this time with John Dagworthy, another captain of superior royal rank, who commanded a detachment of Marylanders at the regiment's headquarters in Fort Cumberland.[44] Washington, impatient for an offensive against Fort Duquesne, was convinced Braddock would have granted him a royal commission and pressed his case in February 1756 with Braddock's successor, William Shirley, and again in January 1757 with Shirley's successor, Lord Loudoun. Shirley ruled in Washington's favor only in the matter of Dagworthy; Loudoun humiliated Washington, refused him a royal commission and agreed only to relieve him of the responsibility of manning Fort Cumberland.[45]
In 1758, the Virginia Regiment was assigned to the British Forbes Expedition to capture Fort Duquesne.[46][f] Washington disagreed with General John Forbes' tactics and chosen route.[48] Forbes nevertheless made Washington a brevet brigadier general and gave him command of one of the three brigades that would assault the fort. The French abandoned the fort and the valley before the assault was launched; Washington saw only a friendly-fire incident which left 14 dead and 26 injured. The war lasted another four years, but Washington resigned his commission and returned to Mount Vernon.[49]
Under Washington, the Virginia Regiment had defended 300 miles (480 km) of frontier against twenty Indian attacks in ten months.[50] He increased the professionalism of the regiment as it increased from 300 to 1,000 men, and Virginia's frontier population suffered less than other colonies. Some historians have said this was Washington's ""only unqualified success"" during the war.[51] Though he failed to realize a royal commission, he did gain self-confidence, leadership skills, and invaluable knowledge of British military tactics. The destructive competition Washington witnessed among colonial politicians fostered his later support of strong central government.[52]
On January 6, 1759, Washington, at age 26, married Martha Dandridge Custis, the 27-year-old widow of wealthy plantation owner Daniel Parke Custis. The marriage took place at Martha's estate; she was intelligent, gracious, and experienced in managing a planter's estate, and the couple created a happy marriage.[53] They raised John Parke Custis (Jacky) and Martha Parke (Patsy) Custis, children from her previous marriage, and later their grandchildren Eleanor Parke Custis (Nelly) and George Washington Parke Custis (Washy). Washington's 1751 bout with smallpox is thought to have rendered him sterile, though it is equally likely ""Martha may have sustained injury during the birth of Patsy, her final child, making additional births impossible.""[54] They lamented the fact that they had no children together.[55] They moved to Mount Vernon, near Alexandria, where he took up life as a planter of tobacco and wheat and emerged as a political figure.[56]
The marriage gave Washington control over Martha's one-third dower interest in the 18,000-acre (7,300 ha) Custis estate, and he managed the remaining two-thirds for Martha's children; the estate also included 84 slaves. He became one of Virginia's wealthiest men, which increased his social standing.[57]
At Washington's urging, Governor Lord Botetourt fulfilled Dinwiddie's 1754 promise of land bounties to all volunteer militia during the French and Indian War.[58] In late 1770, Washington inspected the lands in the Ohio and Great Kanawha regions, and he engaged surveyor William Crawford to subdivide it. Crawford allotted 23,200 acres (9,400 ha) to Washington; Washington told the veterans that their land was hilly and unsuitable for farming, and he agreed to purchase 20,147 acres (8,153 ha), leaving some feeling they had been duped.[59] He also doubled the size of Mount Vernon to 6,500 acres (2,600 ha) and increased its slave population to more than a hundred by 1775.[60]
Washington’s political activities included supporting the candidacy of his friend George William Fairfax in his 1755 bid to represent the region in the Virginia House of Burgess.  This support lead to a dispute which resulted in a physical altercation between Washington and another Virginia planter, William Payne. Washington defused the situation, including ordering officers from the Virginia Regiment to stand down. Washington apologized to Payne the following day at a tavern. Payne had been expecting to be challenged to a duel.[61][62][63]
As a respected military hero and large landowner, Washington held local offices and was elected to the Virginia provincial legislature, representing Frederick County in the House of Burgesses for seven years beginning in 1758.[60] He plied the voters with beer, brandy, and other beverages, although he was absent while serving on the Forbes Expedition.[64] He won election with roughly 40 percent of the vote, defeating three other candidates with the help of several local supporters. He rarely spoke in his early legislative career, but he became a prominent critic of both Britain's taxation policy and mercantilist policies towards the American colonies starting in the 1760s.[65]
By occupation, Washington was a planter, and he imported luxuries and other goods from England, paying for them by exporting tobacco.[66] His profligate spending combined with low tobacco prices left him £1,800 in debt by 1764, prompting him to diversify his holdings.[67] In 1765, because of erosion and other soil problems, he changed Mount Vernon's primary cash crop from tobacco to wheat and expanded operations to include corn flour milling and fishing.[68] Washington also took time for leisure with fox hunting, fishing, dances, theater, cards, backgammon, and billiards.[69]
Washington soon was counted among the political and social elite in Virginia. From 1768 to 1775, he invited some 2,000 guests to his Mount Vernon estate, mostly those whom he considered ""people of rank"". He became more politically active in 1769, presenting legislation in the Virginia Assembly to establish an embargo on goods from Great Britain.[70]
Washington's step-daughter Patsy Custis suffered from epileptic attacks from age 12, and she died in his arms in 1773. The following day, he wrote to Burwell Bassett: ""It is easier to conceive, than to describe, the distress of this Family"".[71] He canceled all business activity and remained with Martha every night for three months.[72]
Washington played a central role before and during the American Revolution. His disdain for the British military had begun when he was passed over for promotion into the Regular Army. Opposed to taxes imposed by the British Parliament on the Colonies without proper representation,[73] he and other colonists were also angered by the Royal Proclamation of 1763 which banned American settlement west of the Allegheny Mountains and protected the British fur trade.[74]
Washington believed the Stamp Act of 1765 was an ""Act of Oppression"", and he celebrated its repeal the following year.[g] In March 1766, Parliament passed the Declaratory Act asserting that Parliamentary law superseded colonial law.[76] Washington helped lead widespread protests against the Townshend Acts passed by Parliament in 1767, and he introduced a proposal in May 1769 drafted by George Mason which called Virginians to boycott British goods; the Acts were mostly repealed in 1770.[77]
Parliament sought to punish Massachusetts colonists for their role in the Boston Tea Party in 1774 by passing the Coercive Acts, which Washington referred to as ""an invasion of our rights and privileges"".[78] He said Americans must not submit to acts of tyranny since ""custom and use shall make us as tame and abject slaves, as the blacks we rule over with such arbitrary sway"".[79] That July, he and George Mason drafted a list of resolutions for the Fairfax County committee which Washington chaired, and the committee adopted the Fairfax Resolves calling for a Continental Congress.[80] On August 1, Washington attended the First Virginia Convention, where he was selected as a delegate to the First Continental Congress.[81] As tensions rose in 1774, he helped train county militias in Virginia and organized enforcement of the Continental Association boycott of British goods instituted by the Congress.[82]
The American Revolutionary War began on April 19, 1775, with the Battles of Lexington and Concord and the Siege of Boston.[83] The colonists were divided over breaking away from British rule and split into two factions: Patriots who rejected British rule, and Loyalists who desired to remain subject to the King.[84] General Thomas Gage was commander of British forces in America at the beginning of the war.[85] Upon hearing the shocking news of the onset of war, Washington was ""sobered and dismayed"",[86] and he hastily departed Mount Vernon on May 4, 1775, to join the Continental Congress in Philadelphia.[87]
Congress created the Continental Army on June 14, 1775, and Samuel and John Adams nominated Washington to become its commander-in-chief. Washington was chosen over John Hancock because of his military experience and the belief that a Virginian would better unite the colonies. He was considered an incisive leader who kept his ""ambition in check"".[88] He was unanimously elected commander in chief by Congress the next day.[89]
Washington appeared before Congress in uniform and gave an acceptance speech on June 16, declining a salary—though he was later reimbursed expenses. He was commissioned on June 19 and was roundly praised by Congressional delegates, including John Adams, who proclaimed that he was the man best suited to lead and unite the colonies.[90][91] Congress appointed Washington ""General & Commander in chief of the army of the United Colonies and of all the forces raised or to be raised by them"", and instructed him to take charge of the siege of Boston on June 22, 1775.[92]
Congress chose his primary staff officers, including Major General Artemas Ward, Adjutant General Horatio Gates, Major General Charles Lee, Major General Philip Schuyler, Major General Nathanael Greene, Colonel Henry Knox, and Colonel Alexander Hamilton.[93] Washington was impressed by Colonel Benedict Arnold and gave him responsibility for launching an invasion of Canada. He also engaged French and Indian War compatriot Brigadier General Daniel Morgan. Henry Knox impressed Adams with ordnance knowledge, and Washington promoted him to colonel and chief of artillery.[94]
Washington initially opposed enlistment of slaves into the Continental Army, but later he relented when the British issued proclamations such as Dunmore's Proclamation, which promised freedom to slaves of Patriot masters if they joined the British.[95] On January 16, 1776, Congress allowed free blacks to serve in the militia. By the end of the war one-tenth of Washington's army were blacks.[96]
Early in 1775, in response to the growing rebellious movement, London sent British troops, commanded by General Thomas Gage, to occupy Boston. They set up fortifications about the city, making it impervious to attack. Various local militias surrounded the city and effectively trapped the British, resulting in a standoff.[97]
As Washington headed for Boston, word of his march preceded him, and he was greeted everywhere; gradually he became a symbol of the Patriot cause.[98][h] Upon arrival on July 2, 1775, two weeks after the Patriot defeat at nearby Bunker Hill, he set up his Cambridge, Massachusetts headquarters and inspected the new army there, only to find an undisciplined and badly outfitted militia.[99] After consultation, he initiated Benjamin Franklin's suggested reforms—drilling the soldiers and imposing strict discipline, floggings, and incarceration.[100] Washington ordered his officers to identify the skills of recruits to ensure military effectiveness, while removing incompetent officers.[101] He petitioned Gage, his former superior, to release captured Patriot officers from prison and treat them humanely.[102] In October 1775, King George III declared that the colonies were in open rebellion and relieved General Gage of command for incompetence, replacing him with General William Howe.[103]
In June 1775, Congress ordered an invasion of Canada. It was led by Benedict Arnold, who, despite Washington's strong objection, drew volunteers from the latter's force during the Siege of Boston. The move on Quebec failed, with the American forces being reduced to less than half and forced to retreat.[104]
The Continental Army, further diminished by expiring short-term enlistments, and by January 1776 reduced by half to 9,600 men, had to be supplemented with militia, and was joined by Knox with heavy artillery captured from Fort Ticonderoga.[105] When the Charles River froze over Washington was eager to cross and storm Boston, but General Gates and others were opposed to untrained militia striking well-garrisoned fortifications. Washington reluctantly agreed to secure the Dorchester Heights, 100 feet above Boston, in an attempt to force the British out of the city.[106] On March 9, under cover of darkness, Washington's troops brought up Knox's big guns and bombarded British ships in Boston harbor. On March 17 9,000 British troops and Loyalists began a chaotic ten-day evacuation of Boston aboard 120 ships. Soon after, Washington entered the city with 500 men, with explicit orders not to plunder the city. He ordered vaccinations against smallpox to great effect, as he did later in Morristown, New Jersey.[107] He refrained from exerting military authority in Boston, leaving civilian matters in the hands of local authorities.[108][i]
Washington then proceeded to New York City, arriving on April 13, 1776, and began constructing fortifications there to thwart the expected British attack. He ordered his occupying forces to treat civilians and their property with respect, to avoid the abuses which were suffered by Bostonian citizens at the hands of British troops during their occupation.[110] A plot to assassinate or capture him was discovered but thwarted, though his bodyguard Thomas Hickey was hanged for mutiny and sedition.[111] General Howe transported his resupplied army, with the British fleet, from Halifax to New York, knowing the city was key to securing the continent. George Germain, who ran the British war effort in England, believed it could be won with one ""decisive blow"".[112] The British forces, including more than a hundred ships and thousands of troops, began arriving on Staten Island on July 2 to lay siege to the city.[113] After the Declaration of Independence was adopted on July 4, Washington informed his troops in his general orders of July 9 that Congress had declared the united colonies to be ""free and independent states"".[114]
Howe's troop strength totaled 32,000 regulars and Hessians auxiliaries, and Washington's consisted of 23,000, mostly raw recruits and militia.[115] In August, Howe landed 20,000 troops at Gravesend, Brooklyn, and approached Washington's fortifications, as George III proclaimed the rebellious American colonists to be traitors.[116] Washington, opposing his generals, chose to fight, based upon inaccurate information that Howe's army had only 8,000-plus troops.[117] In the Battle of Long Island, Howe assaulted Washington's flank and inflicted 1,500 Patriot casualties, the British suffering 400.[118] Washington retreated, instructing General William Heath to acquisition river craft in the area. On August 30, General William Alexander held off the British and gave cover while the army crossed the East River under darkness to Manhattan Island without loss of life or materiel, although Alexander was captured.[119]
Howe, emboldened by his Long Island victory, dispatched Washington as ""George Washington, Esq."", in futility to negotiate peace. Washington declined, demanding to be addressed with diplomatic protocol, as general and fellow belligerent, not as a ""rebel"", lest his men be hanged as such if captured.[120] The Royal Navy bombarded the unstable earthworks on lower Manhattan Island.[121] Washington, with misgivings, heeded the advice of Generals Greene and Putnam to defend Fort Washington. They were unable to hold it, and Washington abandoned it despite General Lee's objections, as his army retired north to the White Plains.[122] Howe's pursuit forced Washington to retreat across the Hudson River to Fort Lee to avoid encirclement. Howe landed his troops on Manhattan in November and captured Fort Washington, inflicting high casualties on the Americans. Washington was responsible for delaying the retreat, though he blamed Congress and General Greene. Loyalists in New York considered Howe a liberator and spread a rumor that Washington had set fire to the city.[123] Patriot morale reached its lowest when Lee was captured.[124] Now reduced to 5,400 troops, Washington's army retreated through New Jersey, and Howe broke off pursuit, delaying his advance on Philadelphia, and set up winter quarters in New York.[125]
Washington crossed the Delaware River into Pennsylvania, where Lee's replacement John Sullivan joined him with 2,000 more troops.[127] The future of the Continental Army was in doubt for lack of supplies, a harsh winter, expiring enlistments, and desertions. Washington was disappointed that many New Jersey residents were Loyalists or skeptical about the prospect of independence.[128]
Howe split up his British Army and posted a Hessian garrison at Trenton to hold western New Jersey and the east shore of the Delaware,[129] but the army appeared complacent, and Washington and his generals devised a surprise attack on the Hessians at Trenton, which he codenamed ""Victory or Death"".[130] The army was to cross the Delaware River to Trenton in three divisions: one led by Washington (2,400 troops), another by General James Ewing (700), and the third by Colonel John Cadwalader (1,500). The force was to then split, with Washington taking the Pennington Road and General Sullivan traveling south on the river's edge.[131]
Washington first ordered a 60-mile search for Durham boats, to transport his army, and he ordered the destruction of vessels that could be used by the British.[132] He crossed the Delaware River on the night of December 25–26, 1776, and risked capture staking out the Jersey shoreline. His men followed across the ice-obstructed river in sleet and snow from McConkey's Ferry, with 40 men per vessel. Wind churned up the waters, and they were pelted with hail, but by 3:00 a.m. on December 26, they made it across with no losses.[133] Henry Knox was delayed, managing frightened horses and about 18 field guns on flat-bottomed ferries. Cadwalader and Ewing failed to cross due to the ice and heavy currents, and a waiting Washington doubted his planned attack on Trenton. Once Knox arrived, Washington proceeded to Trenton, to take only his troops against the Hessians, rather than risk being spotted returning his army to Pennsylvania.[134]
The troops spotted Hessian positions a mile from Trenton, so Washington split his force into two columns, rallying his men: ""Soldiers keep by your officers. For God's sake, keep by your officers."" The two columns were separated at the Birmingham crossroads, with General Nathanael Greene's column taking the upper Ferry Road, led by Washington, and General John Sullivan's advancing on River Road. (See map.)[135] The Americans marched in sleet and snowfall. Many were shoeless with bloodied feet, and two died of exposure. At sunrise, Washington led them in a surprise attack on the Hessians, aided by Major General Knox and artillery. The Hessians had 22 killed (including Colonel Johann Rall), 83 wounded, and 850 captured with supplies.[136]
Washington retreated across the Delaware to Pennsylvania but returned to New Jersey on January 3, launching an attack on British regulars at Princeton, with 40 Americans killed or wounded and 273 British killed or captured.[137] American Generals Hugh Mercer and John Cadwalader were being driven back by the British when Mercer was mortally wounded, then Washington arrived and led the men in a counterattack which advanced to within 30 yards (27 m) of the British line.[138]
Some British troops retreated after a brief stand, while others took refuge in Nassau Hall, which became the target of Colonel Alexander Hamilton's cannons. Washington's troops charged, the British surrendered in less than an hour, and 194 soldiers laid down their arms.[139] Howe retreated to New York City where his army remained inactive until early the next year.[140] Washington's depleted Continental Army took up winter headquarters in Morristown, New Jersey while disrupting British supply lines and expelling them from parts of New Jersey. Washington later said the British could have successfully counterattacked his encampment before his troops were dug in.[141]
The British still controlled New York, and many Patriot soldiers did not re-enlist or deserted after the harsh winter campaign. Congress instituted greater rewards for re-enlisting and punishments for desertion in an effort to effect greater troop numbers.[142] Strategically, Washington's victories were pivotal for the Revolution and quashed the British strategy of showing overwhelming force followed by offering generous terms.[143] In February 1777, word reached London of the American victories at Trenton and Princeton, and the British realized the Patriots were in a position to demand unconditional independence.[144]
In July 1777, British General John Burgoyne led the Saratoga campaign south from Quebec through Lake Champlain and recaptured Fort Ticonderoga with the objective of dividing New England, including control of the Hudson River. But General Howe in British-occupied New York blundered, taking his army south to Philadelphia rather than up the Hudson River to join Burgoyne near Albany.[145] Meanwhile, Washington and Gilbert du Motier, Marquis de Lafayette rushed to Philadelphia to engage Howe and were shocked to learn of Burgoyne's progress in upstate New York, where the Patriots were led by General Philip Schuyler and successor Horatio Gates. Washington's army of less experienced men were defeated in the pitched battles at Philadelphia.[146]
Howe outmaneuvered Washington at the Battle of Brandywine on September 11, 1777, and marched unopposed into the nation's capital at Philadelphia. A Patriot attack failed against the British at Germantown in October. Major General Thomas Conway prompted some members of Congress (referred to as the Conway Cabal) to consider removing Washington from command because of the losses incurred at Philadelphia. Washington's supporters resisted and the matter was finally dropped after much deliberation.[147] Once the plot was exposed, Conway wrote an apology to Washington, resigned, and returned to France.[148]
Washington was concerned with Howe's movements during the Saratoga campaign to the north, and he was also aware that Burgoyne was moving south toward Saratoga from Quebec. Washington took some risks to support Gates' army, sending reinforcements north with Generals Benedict Arnold, his most aggressive field commander, and Benjamin Lincoln. On October 7, 1777, Burgoyne tried to take Bemis Heights but was isolated from support by Howe. He was forced to retreat to Saratoga and ultimately surrendered after the Battles of Saratoga. As Washington suspected, Gates' victory emboldened his critics.[149] Biographer John Alden maintains, ""It was inevitable that the defeats of Washington's forces and the concurrent victory of the forces in upper New York should be compared."" The admiration for Washington was waning, including little credit from John Adams.[150] British commander Howe resigned in May 1778, left America forever, and was replaced by Sir Henry Clinton.[151]
Washington's army of 11,000 went into winter quarters at Valley Forge north of Philadelphia in December 1777. They suffered between 2,000 and 3,000 deaths in the extreme cold over six months, mostly from disease and lack of food, clothing, and shelter.[152] Meanwhile, the British were comfortably quartered in Philadelphia, paying for supplies in pounds sterling, while Washington struggled with a devalued American paper currency. The woodlands were soon exhausted of game, and by February lowered morale and increased desertions ensued.[153]
Washington made repeated petitions to the Continental Congress for provisions. He received a congressional delegation to check the Army's conditions, and expressed the urgency of the situation, proclaiming: ""Something must be done. Important alterations must be made."" He recommended that Congress expedite supplies, and Congress agreed to strengthen and fund the army's supply lines by reorganizing the commissary department. By late February, supplies began arriving.[109]
Baron Friedrich Wilhelm von Steuben's incessant drilling soon transformed Washington's recruits into a disciplined fighting force,[154] and the revitalized army emerged from Valley Forge early the following year.[155] Washington promoted Von Steuben to Major General and made him chief of staff.[156]
In early 1778, the French responded to Burgoyne's defeat and entered into a Treaty of Alliance with the Americans. The Continental Congress ratified the treaty in May, which amounted to a French declaration of war against Britain.[157]
The British evacuated Philadelphia for New York that June and Washington summoned a war council of American and French Generals. He chose a partial attack on the retreating British at the Battle of Monmouth; the British were commanded by Howe's successor General Henry Clinton. Generals Charles Lee and Lafayette moved with 4,000 men, without Washington's knowledge, and bungled their first attack on June 28. Washington relieved Lee and achieved a draw after an expansive battle. At nightfall, the British continued their retreat to New York, and Washington moved his army outside the city.[158] Monmouth was Washington's last battle in the North; he valued the safety of his army more than towns with little value to the British.[159]
Washington became ""America's first spymaster"" by designing an espionage system against the British.[160] In 1778, Major Benjamin Tallmadge formed the Culper Ring at Washington's direction to covertly collect information about the British in New York.[161] Washington had disregarded incidents of disloyalty by Benedict Arnold, who had distinguished himself in many battles.[162]
During mid-1780, Arnold began supplying British spymaster John André with sensitive information intended to compromise Washington and capture West Point, a key American defensive position on the Hudson River.[163] Historians have noted several possible reasons for Arnold's treachery: his anger at losing promotions to junior officers, the repeated slights from Congress. He was also deeply in debt, had been profiteering from the war and was disappointed by Washington's lack of support during his resultant court-martial.[164]
Arnold repeatedly asked for command of West Point, and Washington finally agreed in August.[165] Arnold met André on September 21, giving him plans to take over the garrison.[166] Militia forces captured André and discovered the plans, but Arnold escaped to New York.[167] Washington recalled the commanders positioned under Arnold at key points around the fort to prevent any complicity, but he did not suspect Arnold's wife Peggy. Washington assumed personal command at West Point and reorganized its defenses.[168] André's trial for espionage ended in a death sentence, and Washington offered to return him to the British in exchange for Arnold, but Clinton refused. André was hanged on October 2, 1780, despite his last request being to face a firing squad, in order to deter other spies.[169]
In late 1778, General Clinton shipped 3,000 troops from New York to Georgia and launched a Southern invasion against Savannah, reinforced by 2,000 British and Loyalist troops. They repelled an attack by Patriots and French naval forces, which bolstered the British war effort.[170]
In mid-1779, Washington attacked Iroquois warriors of the Six Nations in order to force Britain's Indian allies out of New York, from which they had assaulted New England towns.[171] The Indian warriors joined with Loyalist rangers led by Walter Butler and viciously slew more than 200 frontiersmen in June, laying waste to the Wyoming Valley in Pennsylvania.[172] In response, Washington ordered General John Sullivan to lead an expedition to effect ""the total destruction and devastation"" of Iroquois villages and take their women and children hostage. Those who managed to escape fled to Canada.[173]
Washington's troops went into quarters at Morristown, New Jersey during the winter of 1779–1780 and suffered their worst winter of the war, with temperatures well below freezing. New York Harbor was frozen over, snow and ice covered the ground for weeks, and the troops again lacked provisions.[174]
Clinton assembled 12,500 troops and attacked Charlestown, South Carolina in January 1780, defeating General Benjamin Lincoln who had only 5,100 Continental troops.[175] The British went on to occupy the South Carolina Piedmont in June, with no Patriot resistance. Clinton returned to New York and left 8,000 troops commanded by General Charles Cornwallis.[176] Congress replaced Lincoln with Horatio Gates; he failed in South Carolina and was replaced by Washington's choice of Nathaniel Greene, but the British already had the South in their grasp. Washington was reinvigorated, however, when Lafayette returned from France with more ships, men, and supplies,[177] and 5,000 veteran French troops led by Marshal Rochambeau arrived at Newport, Rhode Island in July 1780.[178] French naval forces then landed, led by Admiral Grasse, and Washington encouraged Rochambeau to move his fleet south to launch a joint land and naval attack on Arnold's troops.[179]
Washington's army went into winter quarters at New Windsor, New York in December 1780, and Washington urged Congress and state officials to expedite provisions in hopes that the army would not ""continue to struggle under the same difficulties they have hitherto endured"".[180] On March 1, 1781, Congress ratified the Articles of Confederation, but the government that took effect on March 2 did not have the power to levy taxes, and it loosely held the states together.[181]
General Clinton sent Benedict Arnold, now a British Brigadier General with 1,700 troops, to Virginia to capture Portsmouth and to conduct raids on Patriot forces from there; Washington responded by sending Lafayette south to counter Arnold's efforts.[182] Washington initially hoped to bring the fight to New York, drawing off British forces from Virginia and ending the war there, but Rochambeau advised Grasse that Cornwallis in Virginia was the better target. Grasse's fleet arrived off the Virginia coast and Washington saw the advantage. He made a feint towards Clinton in New York, then headed south to Virginia.[183]
The Siege of Yorktown was a decisive allied victory by the combined forces of the Continental Army commanded by General Washington, the French Army commanded by the General Comte de Rochambeau, and the French Navy commanded by Admiral de Grasse, in the defeat of Cornwallis' British forces. On August 19, the march to Yorktown led by Washington and Rochambeau began, which is known now as the ""celebrated march"".[184] Washington was in command of an army of 7,800 Frenchmen, 3,100 militia, and 8,000 Continentals. Lacking in experience in siege warfare, Washington often deferred judgment to Rochambeau, effectively putting him in command; however, Rochambeau never challenged Washington's authority.[185]
By late September, Patriot-French forces completely surrounded Yorktown, trapped the British army, and prevented British reinforcements from Clinton in the North, while the French navy emerged victorious at the Battle of the Chesapeake. The final American offensive was begun with a shot fired by Washington.[186] The siege ended with a British surrender on October 19, 1781; over 7,000 British soldiers were made prisoners of war, in the last major land battle of the American Revolutionary War.[187] Washington negotiated the terms of surrender for two days, and the official signing ceremony took place on October 19; Cornwallis, in fact, claimed illness and was absent, sending General Charles O'Hara as his proxy.[188] As a gesture of goodwill, Washington held a dinner for the American, French, and British generals, all of whom fraternized on friendly terms and identified with one another as members of the same professional military caste.[189]
After the surrender at Yorktown, a situation developed that threatened relations between the newly independent America and Britain.[190] Following a series of retributive executions between Patriots and Loyalists, Washington, on May 18, 1782, wrote in a letter to General Moses Hazen[191] that a British captain would be executed in retaliation for the execution of Joshua Huddy, a popular Patriot leader, who was hanged at the direction of the Loyalist Richard Lippincott. Washington wanted Lippincott himself to be executed but was rebuffed.[192] Subsequently, Charles Asgill was chosen instead, by a drawing of lots from a hat. This was a violation of the 14th article of the Yorktown Articles of Capitulation, which protected prisoners of war from acts of retaliation.[191][193] Later, Washington's feelings on matters changed and in a letter of November 13, 1782, to Asgill, he acknowledged Asgill's letter and situation, expressing his desire not to see any harm come to him.[194] After much consideration between the Continental Congress, Alexander Hamilton, Washington, and appeals from the French Crown, Asgill was finally released,[195] where Washington issued Asgill a pass that allowed his passage to New York.[196][191]
As peace negotiations started, the British gradually evacuated troops from Savannah, Charlestown, and New York by 1783, and the French army and navy likewise departed.[197] The American treasury was empty, unpaid and mutinous soldiers forced the adjournment of Congress, and Washington dispelled unrest by suppressing the Newburgh Conspiracy in March 1783; Congress promised officers a five-year bonus.[198] Washington submitted an account of $450,000 in expenses which he had advanced to the army. The account was settled, though it was allegedly vague about large sums and included expenses his wife had incurred through visits to his headquarters.[199]
Washington resigned as commander-in-chief once the Treaty of Paris was signed, and he planned to retire to Mount Vernon. The treaty was ratified in April 1783, and Hamilton's Congressional committee adapted the army for peacetime. Washington gave the Army's perspective to the committee in his Sentiments on a Peace Establishment.[200] The Treaty was signed on September 3, 1783, and Great Britain officially recognized the independence of the United States. Washington then disbanded his army, giving an eloquent farewell address to his soldiers on November 2.[201] On November 25, the British evacuated New York City, and Washington and Governor George Clinton took possession.[202]
Washington advised Congress in August 1783 to keep a standing army, create a ""national militia"" of separate state units, and establish a navy and a national military academy. He circulated his ""Farewell"" orders that discharged his troops, whom he called ""one patriotic band of brothers"". Before his return to Mount Vernon, he oversaw the evacuation of British forces in New York and was greeted by parades and celebrations, where he announced that Colonel Henry Knox had been promoted commander-in-chief.[203]
After leading the Continental Army for 8½ years, Washington bade farewell to his officers at Fraunces Tavern in December 1783, and resigned his commission days later, refuting Loyalist predictions that he would not relinquish his military command.[204] In a final appearance in uniform, he gave a statement to the Congress: ""I consider it an indispensable duty to close this last solemn act of my official life, by commending the interests of our dearest country to the protection of Almighty God, and those who have the superintendence of them, to his holy keeping.""[205] Washington's resignation was acclaimed at home and abroad and showed a skeptical world that the new republic would not degenerate into chaos.[206][k]
The same month, Washington was appointed president-general of the Society of the Cincinnati, a hereditary fraternity, and he served for the remainder of his life.[208][l]
George WashingtonLetter to LafayetteFebruary 1, 1784[210]
Washington was longing to return home after spending just 10 days at Mount Vernon out of ​8.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;white-space:nowrap} 1⁄2 years of war. He arrived on Christmas Eve, delighted to be ""free of the bustle of a camp and the busy scenes of public life"".[211] He was a celebrity and was fêted during a visit to his mother at Fredericksburg in February 1784, and he received a constant stream of visitors wishing to pay their respects to him at Mount Vernon.[212]
Washington reactivated his interests in the Great Dismal Swamp and Potomac canal projects begun before the war, though neither paid him any dividends, and he undertook a 34-day, 680-mile (1090km) trip to check on his land holdings in the Ohio Country.[213] He oversaw the completion of the remodeling work at Mount Vernon which transformed his residence into the mansion that survives to this day—although his financial situation was not strong. Creditors paid him in depreciated wartime currency, and he owed significant amounts in taxes and wages. Mount Vernon had made no profit during his absence, and he saw persistently poor crop yields due to pestilence and poor weather. His estate recorded its eleventh year running at a deficit in 1787, and there was little prospect of improvement.[214] Washington undertook a new landscaping plan and succeeded in cultivating a range of fast-growing trees and shrubs that were native to North America.[215]
Before returning to private life in June 1783, Washington called for a strong union. Though he was concerned that he might be criticized for meddling in civil matters, he sent a circular letter to all the states maintaining that the Articles of Confederation was no more than ""a rope of sand"" linking the states. He believed the nation was on the verge of ""anarchy and confusion"", was vulnerable to foreign intervention and that a national constitution would unify the states under a strong central government.[216] When Shays' Rebellion erupted in Massachusetts on August 29, 1786, over taxation, Washington was further convinced that a national constitution was needed.[217] Some nationalists feared that the new republic had descended into lawlessness, and they met together on September 11, 1786, at Annapolis to ask Congress to revise the Articles of Confederation. One of their biggest efforts, however, was getting Washington to attend.[218] Congress agreed to a Constitutional Convention to be held in Philadelphia in Spring 1787, and each state was to send delegates.[219]
On December 4, 1786, Washington was chosen to lead the Virginia delegation, but he declined on December 21. He had concerns about the legality of the convention and consulted James Madison, Henry Knox, and others. They persuaded him to attend it, however, as his presence might induce reluctant states to send delegates and smooth the way for the ratification process.[220] On March 28, Washington told Governor Edmund Randolph that he would attend the convention, but made it clear that he was urged to attend.[221]
Washington arrived in Philadelphia on May 9, 1787, though a quorum was not attained until Friday, May 25. Benjamin Franklin nominated Washington to preside over the convention, and he was unanimously elected to serve as president general.[222] The convention's state-mandated purpose was to revise the Articles of Confederation with ""all such alterations and further provisions"" required to improve them, and the new government would be established when the resulting document was ""duly confirmed by the several states"".[223] Governor Edmund Randolph of Virginia introduced Madison's Virginia Plan on May 27, the third day of the convention. It called for an entirely new constitution and a sovereign national government, which Washington highly recommended.[224]
Washington wrote Alexander Hamilton on July 10: ""I almost despair of seeing a favorable issue to the proceedings of our convention and do therefore repent having had any agency in the business.""[225] Nevertheless, he lent his prestige to the goodwill and work of the other delegates. He unsuccessfully lobbied many to support ratification of the Constitution, such as anti-federalist Patrick Henry; Washington told him ""the adoption of it under the present circumstances of the Union is in my opinion desirable"" and declared the alternative would be anarchy.[226] Washington and Madison then spent four days at Mount Vernon evaluating the transition of the new government.[227]
The delegates to the Convention anticipated a Washington presidency and left it to him to define the office once elected.[228][m] The state electors under the Constitution voted for the president on February 4, 1789, and Washington suspected that most republicans had not voted for him.[230] The mandated March 4 date passed without a Congressional quorum to count the votes, but a quorum was reached on April 5. The votes were tallied the next day,[231] and Congressional Secretary Charles Thomson was sent to Mount Vernon to tell Washington he had been elected president. Washington won the majority of every state's electoral votes; John Adams received the next highest number of votes and therefore became vice president.[232] Washington had ""anxious and painful sensations"" about leaving the ""domestic felicity"" of Mount Vernon, but departed for New York City on April 16 to be inaugurated.[233]
Washington was inaugurated on April 30, 1789, taking the oath of office at Federal Hall in New York City.[234][n] His coach was led by militia and a marching band and followed by statesmen and foreign dignitaries in an inaugural parade, with a crowd of 10,000.[236] Chancellor Robert R. Livingston administered the oath, using a Bible provided by the Masons, after which the militia fired a 13-gun salute.[237] Washington read a speech in the Senate Chamber, asking ""that Almighty Being who rules over the universe, who presides in the councils of nations—and whose providential aids can supply every human defect, consecrate the liberties and happiness of the people of the United States"".[238] Though he wished to serve without a salary, Congress insisted adamantly that he accept it, later providing Washington $25,000 per year to defray costs of the presidency.[239]
Washington wrote to James Madison: ""As the first of everything in our situation will serve to establish a precedent, it is devoutly wished on my part that these precedents be fixed on true principles.""[240] To that end, he preferred the title ""Mr. President"" over more majestic names proposed by the Senate, including ""His Excellency"" and ""His Highness the President"".[241] His executive precedents included the inaugural address, messages to Congress, and the cabinet form of the executive branch.[242]
Washington had planned to resign after his first term, but the political strife in the nation convinced him he should remain in office.[243] He was an able administrator and a judge of talent and character, and he talked regularly with department heads to get their advice.[244] He tolerated opposing views, despite fears that a democratic system would lead to political violence, and he conducted a smooth transition of power to his successor.[245] He remained non-partisan throughout his presidency and opposed the divisiveness of political parties, but he favored a strong central government, was sympathetic to a Federalist form of government, and leery of the Republican opposition.[246]
Washington dealt with major problems. The old Confederation lacked the powers to handle its workload and had weak leadership, no executive, a small bureaucracy of clerks, a large debt, worthless paper money, and no power to establish taxes.[247] He had the task of assembling an executive department, and relied on Tobias Lear for advice selecting its officers.[248] Great Britain refused to relinquish its forts in the American West,[247] and Barbary pirates preyed on American merchant ships in the Mediterranean at a time when the United States did not even have a navy.[249]
Congress created executive departments in 1789, including the State Department in July, the Department of War in August, and the Treasury Department in September. Washington appointed fellow Virginian Edmund Randolph as Attorney General, Samuel Osgood as Postmaster General, Thomas Jefferson as Secretary of State, and Henry Knox as Secretary of War. Finally, he appointed Alexander Hamilton as Secretary of the Treasury. Washington's cabinet became a consulting and advisory body, not mandated by the Constitution.[250]
Washington's cabinet members formed rival parties with sharply opposing views, most fiercely illustrated between Hamilton and Jefferson.[251] Washington restricted cabinet discussions to topics of his choosing, without participating in the debate. He occasionally requested cabinet opinions in writing and expected department heads to agreeably carry out his decisions.[247]
Washington was apolitical and opposed the formation of parties, suspecting that conflict would undermine republicanism.[252] His closest advisors formed two factions, portending the First Party System. Secretary of the Treasury Alexander Hamilton formed the Federalist Party to promote the national credit and a financially powerful nation. Secretary of State Thomas Jefferson opposed Hamilton's agenda and founded the Jeffersonian Republicans. Washington favored Hamilton's agenda, however, and it ultimately went into effect—resulting in bitter controversy.[253]
Washington proclaimed November 26 as a day of Thanksgiving in order to encourage national unity. ""It is the duty of all nations to acknowledge the providence of Almighty God, to obey His will, to be grateful for His benefits, and humbly to implore His protection and favor."" He spent that day fasting and visiting debtors in prison to provide them with food and beer.[254]
In response to two antislavery petitions, Georgia and South Carolina objected and were threatening to ""blow the trumpet of civil war"". Washington and Congress responded with a series of pro-slavery measures: citizenship was denied to black immigrants; slaves were barred from serving in state militias; two more slave states (Kentucky in 1792, Tennessee in 1796) were admitted; and the continuation of slavery in federal territories south of the Ohio River was guaranteed. On February 12, 1793, Washington signed into law the Fugitive Slave Act, which overrode state laws and courts, allowing agents to cross state lines to capture and return escaped slaves.[255] Many in the north decried the law believing the act allowed bounty hunting and the kidnappings of blacks.[256] The Slave Trade Act of 1794, sharply limiting American involvement in the Atlantic slave trade, was also enacted.[257]
Washington's first term was largely devoted to economic concerns, in which Hamilton had devised various plans to address matters.[258] The establishment of public credit became a primary challenge for the federal government.[259] Hamilton submitted a report to a deadlocked Congress, and he, Madison, and Jefferson reached the Compromise of 1790 in which Jefferson agreed to Hamilton's debt proposals in exchange for moving the nation's capital temporarily to Philadelphia and then south near Georgetown on the Potomac River.[253] The terms were legislated in the Funding Act of 1790 and the Residence Act, both of which Washington signed into law. Congress authorized the assumption and payment of the nation's debts, with funding provided by customs duties and excise taxes.[260]
Hamilton created controversy among Cabinet members by advocating the establishment of the First Bank of the United States. Madison and Jefferson objected, but the bank easily passed Congress. Jefferson and Randolph insisted that the new bank was beyond the authority granted by the constitution, as Hamilton believed. Washington sided with Hamilton and signed the legislation on February 25, and the rift became openly hostile between Hamilton and Jefferson.[261]
The nation's first financial crisis occurred in March 1792. Hamilton's Federalists exploited large loans to gain control of U.S. debt securities, causing a run on the national bank;[262] the markets returned to normal by mid-April.[263] Jefferson believed Hamilton was part of the scheme, in spite of Hamilton's efforts to ameliorate, and Washington again found himself in the middle of a feud.[264]
Jefferson and Hamilton adopted diametrically opposed political principles. Hamilton believed in a strong national government requiring a national bank and foreign loans to function, while Jefferson believed the government should be primarily directed by the states and the farm element; he also resented the idea of banks and foreign loans. To Washington's dismay, the two men persistently entered into disputes and infighting.[265] Hamilton demanded that Jefferson resign if he could not support Washington, and Jefferson told Washington that Hamilton's fiscal system would lead to the overthrow of the Republic.[266] Washington urged them to call a truce for the nation's sake, but they ignored him.[267]
Washington reversed his decision to retire after his first term in order to minimize party strife, but the feud continued after his re-election.[266] Jefferson's political actions, his support of Freneau's National Gazette,[268] and his attempt to undermine Hamilton nearly led Washington to dismiss him from the cabinet; Jefferson ultimately resigned his position in December 1793, and Washington forsook him from that time on.[269]
The feud led to the well-defined Federalist and Republican parties, and party affiliation became necessary for election to Congress by 1794.[270] Washington remained aloof from congressional attacks on Hamilton, but he did not publicly protect him, either. The Hamilton–Reynolds sex scandal opened Hamilton to disgrace, but Washington continued to hold him in ""very high esteem"" as the dominant force in establishing federal law and government.[271]
In March 1791, at Hamilton's urging, with support from Madison, Congress imposed an excise tax on distilled spirits to help curtail the national debt, which took effect in July.[272] Grain farmers strongly protested in Pennsylvania's frontier districts; they argued that they were unrepresented and were shouldering too much of the debt, comparing their situation to excessive British taxation prior to the Revolutionary War. On August 2, Washington assembled his cabinet to discuss how to deal with the situation. Unlike Washington who had reservations about using force, Hamilton had long waited for such a situation and was eager to suppress the rebellion by use of Federal authority and force.[273] Not wanting to involve the federal government if possible, Washington called on Pennsylvania state officials to take the initiative, but they declined to take military action. On August 7, Washington issued his first proclamation for calling up state militias. After appealing for peace, he reminded the protestors that, unlike the rule of the British crown, the Federal law was issued by state-elected representatives.[274]
Threats and violence against tax collectors, however, escalated into defiance against federal authority in 1794 and gave rise to the Whiskey Rebellion. Washington issued a final proclamation on September 25, threatening the use of military force to no avail.[274] The federal army was not up to the task, so Washington invoked the Militia Act of 1792 to summon state militias.[275] Governors sent troops, initially commanded by Washington, who gave the command to Light-Horse Harry Lee to lead them into the rebellious districts. They took 150 prisoners, and the remaining rebels dispersed without further fighting. Two of the prisoners were condemned to death, but Washington exercised his Constitutional authority for the first time and pardoned them.[276]
Washington's forceful action demonstrated that the new government could protect itself and its tax collectors. This represented the first use of federal military force against the states and citizens,[277] and remains the only time an incumbent president has commanded troops in the field. Washington justified his action against ""certain self-created societies"" which he regarded as ""subversive organizations"" which threatened the national union. He did not dispute their right to protest, but he insisted that their dissent must not violate federal law. Congress agreed and extended their congratulations to him; only Madison and Jefferson expressed indifference.[278]
In April 1792, the French Revolutionary Wars began between Great Britain and France, and Washington declared America's neutrality. The revolutionary government of France sent diplomat Citizen Genêt to America, and he was welcomed with great enthusiasm. He created a network of new Democratic-Republican Societies promoting France's interests, but Washington denounced them and demanded that the French recall Genêt.[279] The National Assembly of France granted Washington honorary French citizenship on August 26, 1792, during the early stages of the French Revolution.[280] Hamilton formulated the Jay Treaty to normalize trade relations with Great Britain while removing them from western forts, and also to resolve financial debts remaining from the Revolution.[281] Chief Justice John Jay acted as Washington's negotiator and signed the treaty on November 19, 1794; critical Jeffersonians, however, supported France. Washington deliberated, then supported the treaty because it avoided war with Britain,[282] but was disappointed that its provisions favored Britain.[283] He mobilized public opinion and secured ratification in the Senate[284] but faced frequent public criticism.[285]
The British agreed to abandon their forts around the Great Lakes, and the United States modified the boundary with Canada. The government liquidated numerous pre-Revolutionary debts, and the British opened the British West Indies to American trade. The treaty secured peace with Britain and a decade of prosperous trade. Jefferson claimed that it angered France and ""invited rather than avoided"" war.[286] Relations with France deteriorated afterwards, leaving succeeding president John Adams with prospective war.[287] James Monroe was the American Minister to France, but Washington recalled him for his opposition to the Treaty. The French refused to accept his replacement Charles Cotesworth Pinckney, and the French Directory declared the authority to seize American ships two days before Washington's term ended. [288]
Ron Chernow describes Washington as always trying to be even-handed in dealing with the Natives. He states that Washington hoped they would abandon their itinerant hunting life and adapt to fixed agricultural communities in the manner of white settlers. He also maintains that Washington never advocated outright confiscation of tribal land or the forcible removal of tribes, and that he berated American settlers who abused natives, admitting that he held out no hope for pacific relations with the natives as long as ""frontier settlers entertain the opinion that there is not the same crime (or indeed no crime at all) in killing an native as in killing a white man.""[289]
By contrast, Colin G. Calloway writes that ""Washington had a lifelong obsession with getting Indian land, either for himself or for his nation, and initiated policies and campaigns that had devastating effects in Indian country.""[290] ""The growth of the nation,"" Galloway has stated, ""demanded the dispossession of Indian people. Washington hoped the process could be bloodless and that Indian people would give up their lands for a ""fair"" price and move away. But if Indians refused and resisted, as they often did, he felt he had no choice but to ""extirpate"" them and that the expeditions he sent to destroy Indian towns were therefore entirely justified.""[291]
During the Fall of 1789, Washington had to contend with the British refusing to evacuate their forts in the Northwest frontier and their concerted efforts to incite hostile Indian tribes to attack American settlers.[292][o] The Northwest tribes under Miami chief Little Turtle allied with the British Army to resist American expansion, and killed 1,500 settlers between 1783 and 1790.[293]
Washington decided that ""The Government of the United States are determined that their Administration of Indian Affairs shall be directed entirely by the great principles of Justice and humanity"",[294] and provided that their land interests should be negotiated by treaties.[294] The administration regarded powerful tribes as foreign nations, and Washington even smoked a peace pipe and drank wine with them at the Philadelphia presidential house.[295] He made numerous attempts to conciliate them;[296] he equated killing indigenous peoples with killing whites and sought to integrate them into European-American culture.[297] Secretary of War Henry Knox also attempted to encourage agriculture among the tribes.[296]
In the Southwest, negotiations failed between federal commissioners and raiding Indian tribes seeking retribution. Washington invited Creek Chief Alexander McGillivray and 24 leading chiefs to New York to negotiate a treaty and treated them like foreign dignitaries. Knox and McGillivray concluded the Treaty of New York on August 7, 1790 in Federal Hall, which provided the tribes with agricultural supplies and McGillivray with a rank of Brigadier General Army and a salary of $1,500.[298]
In 1790, Washington sent Brigadier General Josiah Harmar to pacify the Northwest tribes, but Little Turtle routed him twice and forced him to withdraw.[299] The Western Confederacy of tribes used guerrilla tactics and were an effective force against the sparsely manned American Army. Washington sent Major General Arthur St. Clair from Fort Washington on an expedition to restore peace in the territory in 1791. On November 4, St. Clair's forces were ambushed and soundly defeated by tribal forces with few survivors, despite Washington's warning of surprise attacks. Washington was outraged over what he viewed to be excessive Native American brutality and execution of captives, including women and children.[300]
St. Clair resigned his commission, and Washington replaced him with the Revolutionary War hero General Anthony Wayne. From 1792 to 1793, Wayne instructed his troops on Native American warfare tactics and instilled discipline which was lacking under St. Clair.[301] In August 1794, Washington sent Wayne into tribal territory with authority to drive them out by burning their villages and crops in the Maumee Valley.[302] On August 24, the American army under Wayne's leadership defeated the western confederacy at the Battle of Fallen Timbers, and the Treaty of Greenville in August 1795 opened up two-thirds of the Ohio Country for American settlement.[303]
Originally Washington had planned to retire after his first term, while many Americans could not imagine anyone else taking his place.[304] After nearly four years as president, and dealing with the infighting in his own cabinet and with partisan critics, Washington showed little enthusiasm in running for a second term, while Martha also wanted him not to run.[305] James Madison urged him not to retire, that his absence would only allow the dangerous political rift in his cabinet, and in the House, to worsen. Jefferson also pleaded with him not to retire and agreed to drop his attacks on Hamilton, or he would also retire if Washington did.[306] Hamilton maintained that Washington's absence would be ""deplored as the greatest evil"" to the country at this time.[307] Washington's close nephew George Augustine Washington, his manager at Mount Vernon, was critically ill and had to be replaced, further increasing Washington's desire to retire and return to Mount Vernon.[308]
When the election of 1792 neared, Washington did not publicly announce his presidential candidacy but silently consented to run, to prevent a further political-personal rift in his cabinet. The Electoral College unanimously elected him president on February 13, 1793, and John Adams as vice president by a vote of 77 to 50.[297] Washington, with nominal fanfare, arrived alone at his inauguration in his carriage. Sworn into office by Associate Justice William Cushing on March 4, 1793 in the Senate Chamber of Congress Hall in Philadelphia, Washington gave a brief address and then immediately retired to his Philadelphia presidential house, weary of office and in poor health.[309]
On April 22, 1793, during the French Revolution, Washington issued his famous Neutrality Proclamation and was resolved to pursue, ""a conduct friendly and impartial toward the belligerent Powers"" while he warned Americans not to intervene in the international conflict. [310] Although Washington recognized France's revolutionary government, he would eventually ask French minister to America Citizen Genêt be recalled over the Citizen Genêt Affair.[311] Genêt was a diplomatic troublemaker who was openly hostile toward Washington's neutrality policy. He procured four American ships as privateers to strike at Spanish forces (British allies) in Florida while organizing militias to strike at other British possessions. But his efforts failed to draw America into the foreign campaigns during Washington's presidency.[312] On July 31, 1793 Jefferson submitted his resignation from Washington's cabinet.[313] Washington signed the Naval Act of 1794 and commissioned the first six federal frigates to combat Barbary pirates.[314]
In January 1795, Hamilton, who desired more income for his family, resigned office and was replaced by Washington appointment Oliver Wolcott, Jr.. Washington and Hamilton remained friends. However, Washington's relationship with his Secretary of War Henry Knox deteriorated. Knox resigned office on the rumor he profited from construction contracts on U.S. Frigates.[315]
In the final months of his presidency, Washington was assailed by his political foes and a partisan press who accused him of being ambitious and greedy, while he argued that he had taken no salary during the war and had risked his life in battle. He regarded the press as a disuniting, ""diabolical"" force of falsehoods, sentiments that he expressed in his Farewell Address.[316] At the end of his second term, Washington retired for personal and political reasons, dismayed with personal attacks, and to ensure that a truly contested presidential election could be held. He did not feel bound to a two-term limit, but his retirement set a significant precedent. Washington is often credited with setting the principle of a two-term presidency, but it was Thomas Jefferson who first refused to run for a third term on political grounds.[317]
In 1796, Washington declined to run for a third term of office, believing his death in office would create an image of a lifetime appointment. The precedent of a two-term limit was created by his retirement from office.[318] In May 1792, in anticipation of his retirement, Washington instructed James Madison to prepare a ""valedictory address"", an initial draft of which was entitled the ""Farewell Address"".[319] In May 1796, Washington sent the manuscript to his Secretary of Treasury Alexander Hamilton who did an extensive rewrite, while Washington provided final edits.[320] On September 19, 1796, David Claypoole's American Daily Advertiser published the final version of the address.[321]
Washington stressed that national identity was paramount, while a united America would safeguard freedom and prosperity. He warned the nation of three eminent dangers: regionalism, partisanship, and foreign entanglements, and said the ""name of AMERICAN, which belongs to you, in your national capacity, must always exalt the just pride of patriotism, more than any appellation derived from local discriminations.""[322] Washington called for men to move beyond partisanship for the common good, stressing that the United States must concentrate on its own interests. He warned against foreign alliances and their influence in domestic affairs and against bitter partisanship and the dangers of political parties.[323] He counseled friendship and commerce with all nations, but advised against involvement in European wars.[324] He stressed the importance of religion, asserting that ""religion and morality are indispensable supports"" in a republic.[325] Washington's address favored Hamilton's Federalist ideology and economic policies.[326]
Washington closed the address by reflecting on his legacy:
Though in reviewing the incidents of my Administration I am unconscious of intentional error, I am nevertheless too sensible of my defects not to think it probable that I may have committed many errors. Whatever they may be, I fervently beseech the Almighty to avert or mitigate the evils to which they may tend. I shall also carry with me the hope that my country will never cease to view them with indulgence, and that, after forty-five years of my life dedicated to its service with an upright zeal, the faults of incompetent abilities will be consigned to oblivion, as myself must soon be to the mansions of rest.[327]After initial publication, many Republicans, including Madison, criticized the Address and believed it was an anti-French campaign document. Madison believed Washington was strongly pro-British. Madison also was suspicious of who authored the Address.[328]
In 1839, Washington biographer Jared Sparks maintained that Washington's ""... Farewell Address was printed and published with the laws, by order of the legislatures, as an evidence of the value they attached to its political precepts, and of their affection for its author.""[329] In 1972, Washington scholar James Flexner referred to the Farewell Address as receiving as much acclaim as Thomas Jefferson's Declaration of Independence and Abraham Lincoln's Gettysburg Address.[330] In 2010, historian Ron Chernow reported the Farewell Address proved to be one of the most influential statements on Republicanism.[331]
Washington retired to Mount Vernon in March 1797 and devoted time to his plantations and other business interests, including his distillery.[332] His plantation operations were only minimally profitable,[38] and his lands in the west (Piedmont) were under Indian attacks and yielded little income, with the squatters there refusing to pay rent. He attempted to sell these but without success.[333] He became an even more committed Federalist. He vocally supported the Alien and Sedition Acts and convinced Federalist John Marshall to run for Congress to weaken the Jeffersonian hold on Virginia.[334]
Washington grew restless in retirement, prompted by tensions with France, and he wrote to Secretary of War James McHenry offering to organize President Adams' army.[335] In a continuation of the French Revolutionary Wars, French privateers began seizing American ships in 1798, and relations deteriorated with France and led to the ""Quasi-War"". Without consulting Washington, Adams nominated him for a lieutenant general commission on July 4, 1798 and the position of commander-in-chief of the armies.[336] Washington chose to accept, replacing James Wilkinson,[337] and he served as the commanding general from July 13, 1798 until his death 17 months later. He participated in planning for a provisional army, but he avoided involvement in details. In advising McHenry of potential officers for the army, he appeared to make a complete break with Jefferson's Democratic-Republicans: ""you could as soon scrub the blackamoor white, as to change the principles of a profest Democrat; and that he will leave nothing unattempted to overturn the government of this country.""[338] Washington delegated the active leadership of the army to Hamilton, a major general. No army invaded the United States during this period, and Washington did not assume a field command.[339]
Washington was thought to be rich because of the well-known ""glorified façade of wealth and grandeur"" at Mount Vernon,[340] but nearly all his wealth was in the form of land and slaves rather than ready cash. To supplement his income he erected a distillery for substantial whiskey production.[341] Historians estimate that the estate was worth about $1 million in 1799 dollars,[342] equivalent to $15,065,000 in 2019. He bought land parcels to spur development around the new Federal City that was named in his honor, and he sold individual lots to middle-income investors rather than multiple lots to large investors, believing they would more likely commit to making improvements.[343]
On December 12, 1799, Washington inspected his farms on horseback in snow and sleet. He returned home late for dinner but refused to change out of his wet clothes, not wanting to keep his guests waiting. He had a sore throat the following day but again went out in freezing, snowy weather to mark trees for cutting. That evening, he complained of chest congestion, but was still cheerful. On Saturday, he awoke to an inflamed throat and difficulty breathing, so he ordered estate overseer George Rawlins to remove nearly a pint of his blood, bloodletting being a common practice of the time. His family summoned Doctors James Craik, Gustavus Richard Brown, and Elisha C. Dick.[344] (Dr. William Thornton arrived some hours after Washington died.)[345]
Dr. Brown thought Washington had quinsy; Dr. Dick thought the condition was a more serious ""violent inflammation of the throat"".[346] They continued the process of bloodletting to approximately five pints, and Washington's condition deteriorated further. Dr. Dick proposed a tracheotomy, but the others were not familiar with that procedure and therefore disapproved.[347] Washington instructed Brown and Dick to leave the room, while he assured Craik, ""Doctor, I die hard, but I am not afraid to go.""[348]
Washington's death came more swiftly than expected.[349] On his deathbed, he instructed his private secretary Tobias Lear to wait three days before his burial, out of fear of being entombed alive.[350] According to Lear, he died peacefully between 10 and 11 p.m. on December 14, 1799, with Martha seated at the foot of his bed. His last words were ""'Tis well"", from his conversation with Lear about his burial. He was 67.[351]
Congress immediately adjourned for the day upon news of Washington's death, and the Speaker's chair was shrouded in black the next morning.[352] The funeral was held four days after his death on December 18, 1799, at Mount Vernon, where his body was interred. Cavalry and foot soldiers led the procession, and six colonels served as the pallbearers. The Mount Vernon funeral service was restricted mostly to family and friends.[353] Reverend Thomas Davis read the funeral service by the vault with a brief address, followed by a ceremony performed by various members of Washington's Masonic lodge in Alexandria, Virginia.[354] Congress chose Light-Horse Harry Lee to deliver the eulogy. Word of his death traveled slowly; church bells rang in the cities, and many places of business closed.[355] People worldwide admired Washington and were saddened by his death, and memorial processions were held in major cities of the United States. Martha wore a black mourning cape for one year, and she burned their correspondence to protect their privacy. Only five letters between the couple are known to have survived: two from Martha to George and three from him to her.[356]
The diagnosis of Washington's illness and the immediate cause of his death have been subjects of debate since the day he died. The published account of Drs. Craik and Brown[p] stated that his symptoms had been consistent with cynanche trachealis (tracheal inflammation), a term of that period used to describe severe inflammation of the upper windpipe, including quinsy. Accusations have persisted since Washington's death concerning medical malpractice, with some believing he had been bled to death.[347] Various modern medical authors have speculated that he died from a severe case of epiglottitis complicated by the given treatments, most notably the massive blood loss which almost certainly caused hypovolemic shock.[358][q]
Washington was buried in the old Washington family vault at Mount Vernon, situated on a grassy slope overspread with willow, juniper, cypress, and chestnut trees. It contained the remains of his brother Lawrence and other family members, but the decrepit brick vault was in need of repair, prompting Washington to leave instructions in his will for the construction of a new vault.[355] Washington's estate at the time of his death was worth an estimated $780,000 in 1799, approximately equivalent to $14.3 million in 2010.[362] Washington's peak net worth was $587.0 million, including his 300 slaves.[363]
In 1830, a disgruntled ex-employee of the estate attempted to steal what he thought was Washington's skull, prompting the construction of a more secure vault.[364] The next year, the new vault was constructed at Mount Vernon to receive the remains of George and Martha and other relatives.[365] In 1832, a joint Congressional committee debated moving his body from Mount Vernon to a crypt in the Capitol. The crypt had been built by architect Charles Bulfinch in the 1820s during the reconstruction of the burned-out capital, after the Burning of Washington by the British during the War of 1812. Southern opposition was intense, antagonized by an ever-growing rift between North and South; many were concerned that Washington's remains could end up on ""a shore foreign to his native soil"" if the country became divided, and Washington's remains stayed in Mount Vernon.[366]
On October 7, 1837, Washington's remains were placed, still in the original lead coffin, within a marble sarcophagus designed by William Strickland and constructed by John Struthers earlier that year.[367] The sarcophagus was sealed and encased with planks, and an outer vault was constructed around it.[368] The outer vault has the sarcophagi of both George and Martha Washington; the inner vault has the remains of other Washington family members and relatives.[365]
Washington was somewhat reserved in personality, but he generally had a strong presence among others. He made speeches and announcements when required, but he was not a noted orator or debater.[370] He was taller than most of his contemporaries;[371] accounts of his height vary from 6 ft (1.83 m) to 6 ft 3.5 in (1.92 m) tall, he weighed between 210–220 pounds (95–100 kg) as an adult,[372] and he was known for his great strength.[373] He had grey-blue eyes and reddish-brown hair which he wore powdered in the fashion of the day.[374] He had a rugged and dominating presence, which garnered respect from his male peers.
Washington suffered frequently from severe tooth decay and ultimately lost all his teeth but one. He had several sets of false teeth made which he wore during his presidency—none of which were made of wood, contrary to common lore. These dental problems left him in constant pain, for which he took laudanum.[375] As a public figure, he relied upon the strict confidence of his dentist.[376]
Washington was a talented equestrian early in life. He collected thoroughbreds at Mount Vernon, and his two favorite horses were Blueskin and Nelson.[377] Fellow Virginian Thomas Jefferson said Washington was ""the best horseman of his age and the most graceful figure that could be seen on horseback"";[378] he also hunted foxes, deer, ducks, and other game.[379] He was an excellent dancer and attended the theater frequently. He drank in moderation but was morally opposed to excessive drinking, smoking tobacco, gambling, and profanity.[380]
Washington was descended from Anglican minister Lawrence Washington (his great-great-grandfather), whose troubles with the Church of England may have prompted his heirs to emigrate to America.[381] Washington was baptized as an infant in April 1732 and became a devoted member of the Church of England (the Anglican Church).[382] He served more than 20 years as a vestryman and churchwarden for Fairfax Parish and Truro Parish, Virginia.[383] He privately prayed and read the Bible daily, and he publicly encouraged people and the nation to pray.[384] He may have taken communion on a regular basis prior to the Revolutionary War, but he did not do so following the war, for which he was admonished by Pastor James Abercrombie.[385]
Washington believed in a ""wise, inscrutable, and irresistible"" Creator God who was active in the Universe, contrary to deistic thought.[381] He referred to God by the Enlightenment terms Providence, the Creator, or the Almighty, and also as the Divine Author or the Supreme Being.[386] He believed in a divine power who watched over battlefields, was involved in the outcome of war, was protecting his life, and was involved in American politics—and specifically in the creation of the United States.[387][r] Modern historian Ron Chernow has posited that Washington avoided evangelistic Christianity or hellfire-and-brimstone speech along with communion and anything inclined to ""flaunt his religiosity"". Chernow has also said Washington ""never used his religion as a device for partisan purposes or in official undertakings"".[389] No mention of Jesus Christ appears in his private correspondence, and such references are rare in his public writings.[390] He frequently quoted from the Bible or paraphrased it, and often referred to the Anglican Book of Common Prayer.[391] There is debate on whether he is best classed as a Christian or a theistic rationalist—or both.[392]
Washington emphasized religious toleration in a nation with numerous denominations and religions. He publicly attended services of different Christian denominations and prohibited anti-Catholic celebrations in the Army.[393] He engaged workers at Mount Vernon without regard for religious belief or affiliation. While president, he acknowledged major religious sects and gave speeches on religious toleration.[394] He was distinctly rooted in the ideas, values, and modes of thinking of the Enlightenment,[395] but he harbored no contempt of organized Christianity and its clergy, ""being no bigot myself to any mode of worship"".[395] In 1793, speaking to members of the New Church in Baltimore, Washington proclaimed, ""We have abundant reason to rejoice that in this Land the light of truth and reason has triumphed over the power of bigotry and superstition.""[396]
Freemasonry was a widely accepted institution in the late 18th century, known for advocating moral teachings.[397] Washington was attracted to the Masons' dedication to the Enlightenment principles of rationality, reason, and brotherhood. The American Masonic lodges did not share the anti-clerical perspective of the controversial European lodges.[398] A Masonic lodge was established in Fredericksburg in September 1752, and Washington was initiated two months later at the age of 20 as one of its first Entered Apprentices. Within a year, he progressed through its ranks to become a Master Mason.[399] Washington had a high regard for the Masonic Order, but his personal lodge attendance was sporadic. In 1777, a convention of Virginia lodges asked him to be the Grand Master of the newly established Grand Lodge of Virginia, but he declined due to his commitments leading the Continental Army. After 1782, he corresponded frequently with Masonic lodges and members,[400] and he was listed as Master in the Virginia charter of Alexandria Lodge No. 22 in 1788.[401]
In Washington's lifetime, slavery was deeply ingrained in the economic and social fabric of Virginia.[402] Washington owned and worked African slaves his entire adult life.[403] He acquired them through inheritance, gained control of eighty-four dower slaves on his marriage to Martha and purchased at least seventy-one slaves between 1752 and 1773.[404] His early views on slavery were no different from any Virginia planter of the time.[405] He demonstrated no moral qualms about the institution and referred to his slaves as ""a Species of Property"".[406] From the 1760s his attitudes underwent a slow evolution. The first doubts were prompted by his transition from tobacco to grain crops which left him with a costly surplus of slaves, causing him to question the economic efficiency of the system.[407] His growing disillusionment with the institution was spurred by the principles of the American Revolution and revolutionary friends such as Lafayette and Hamilton.[408] Most historians agree the Revolution was central to the evolution of Washington's attitudes on slavery;[409] ""After 1783"", Kenneth Morgan writes, ""...[Washington] began to express inner tensions about the problem of slavery more frequently, though always in private...""[410]
The many contemporary reports of slave treatment at Mount Vernon are varied and conflicting.[411] Historian Kenneth Morgan (2000) maintains that Washington was frugal on spending for clothes and bedding for his slaves, and only provided them with just enough food, and that he maintained strict control over his slaves, instructing his overseers to keep them working hard from dawn to dusk year round. [412] However, historian Dorothy Twohig (2001) said: ""Food, clothing, and housing seem to have been at least adequate"".[413] Washington faced growing debts involved with the costs of supporting slaves. He held an ""ingrained sense of racial superiority"" over African Americans, but harbored no ill feelings toward them.[414]
Some slave families worked at different locations on the plantation but were allowed to visit one another on their days off.[415] Washington's slaves received two hours off for meals during the workday, and given time off on Sundays and religious holidays.[416] Washington frequently cared for ill or injured slaves personally, and he provided physicians and midwives and had his slaves inoculated for smallpox.[417][failed verification – see discussion] In May 1796, Martha's personal and favorite slave Ona Judge escaped to Portsmouth. At Martha's behest Washington attempted to capture Ona, using a Treasury agent, but this effort failed. In February 1797, Washington's personal slave Hercules escaped to Philadelphia and was never found.[418]
Some accounts report that Washington opposed flogging, but at times sanctioned its use, generally as a last resort, on both male and female slaves.[419] Washington used both reward and punishment to encourage discipline and productivity in his slaves. He tried appealing to an individual's sense of pride, gave better blankets and clothing to the ""most deserving"", and motivated his slaves with cash rewards. He believed ""watchfulness and admonition"" to be often better deterrents against transgressions, but would punish those who ""will not do their duty by fair means"". Punishment ranged in severity from demotion back to fieldwork, through whipping and beatings, to permanent separation from friends and family by sale. Historian Ron Chernow maintains that overseers were required to warn slaves before resorting to the lash and required Washington's written permission before whipping, though his extended absences did not always permit this.[420] Washington remained dependent on slave labor to work his farms and negotiated the purchase of more slaves in 1786 and 1787.[421]
In February 1786, Washington took a census of Mount Vernon and recorded 224 slaves.[422]
By 1799, slaves at Mount Vernon totaled 317, including 143 children.[423] Washington owned 124 slaves, leased 40, and held 153 for his wife's dower interest.[424] Washington supported many slaves who were too young or too old to work, greatly increasing Mount Vernon's slave population and causing the plantation to operate at a loss.[425]
Based on his letters, diary, documents, accounts from colleagues, employees, friends and visitors, Washington slowly developed a cautious sympathy toward abolitionism that eventually ended with the emancipation of his own slaves.[426] As president, he kept publicly silent on slavery, believing it was a nationally divisive issue that could destroy the union.[427]
In a 1778 letter to Lund Washington, he made clear his desire ""to get quit of Negroes"" when discussing the exchange of slaves for land he wanted to buy.[428] The next year, he stated his intention not to separate families as a result of ""a change of masters"".[429] During the 1780s Washington privately expressed his support for gradual emancipation of slaves.[430] Between 1783 and 1786 he gave moral support to a plan proposed by Lafayette to purchase land and free slaves to work on it, but declined to participate in the experiment.[413] Washington privately expressed support for emancipation to prominent Methodists Thomas Coke and Francis Asbury in 1785, but declined to sign their petition.[431] In personal correspondence the next year, he made clear his desire to see the institution of slavery ended by a gradual legislative process, a view that correlated with the mainstream antislavery literature published in the 1780s that Washington possessed.[432] He significantly reduced his purchases of slaves after the war, but continued to acquire them in small numbers.[433]
In 1788, Washington declined a suggestion from a leading French abolitionist, Jacques Brissot, to establish an abolitionist society in Virginia, stating that although he supported the idea, the time was not yet right to confront the issue.[434] The historian Henry Wiencek (2003) believes, based on a remark that appears in the notebook of his biographer David Humphreys, that Washington considered making a public statement by freeing his slaves on the eve of his presidency in 1789.[435] The historian Philip D. Morgan (2005) disagrees, believing the remark was a ""private expression of remorse"" at his inability to free his slaves.[436] Other historians agree with Morgan that Washington was determined not to risk national unity over an issue as divisive as slavery.[437] Washington never responded to any of the antislavery petitions he received, and the subject was not mentioned in either his last address to Congress or his Farewell Address.[438]
The first clear indication that Washington was seriously intending to free his own slaves appears in a letter written to his secretary, Tobias Lear, in 1794.[439] Washington instructed Lear to find buyers for his land in western Virginia, explaining in a private coda that he was doing so ""to liberate a certain species of property which I possess, very repugnantly to my own feelings"".[440] The plan, along with others Washington considered in 1795 and 1796, could not be realized because of his failure to find buyers for his land, his reluctance to break up slave families and the refusal of the Custis heirs to help prevent such separations by freeing their dower slaves at the same time.[441]
On July 9, 1799, Washington finished making his last will; the longest provision concerned slavery. All his slaves were to be freed after the death of his wife Martha. Washington said he did not free them immediately because his slaves intermarried with his wife's dower slaves. He forbade their sale or transportation out of Virginia. His will provided that old and young freed people be taken care of indefinitely; younger ones were to be taught to read and write and placed in suitable occupations.[442] Washington freed more than 160 slaves, including 25 he had acquired from his wife's brother in payment of a debt freed by graduation.[443] He was among the few large slave-holding Virginians during the Revolutionary Era who emancipated their slaves.[444]
On January 1, 1801, one year after George Washington's death, Martha Washington signed an order freeing his slaves. Many of them, having never strayed far from Mount Vernon, were naturally reluctant to try their luck elsewhere; others refused to abandon spouses or children still held as dower slaves (the Custis estate)[445] and also stayed with or near Martha. Following George Washington's instructions in his will, funds were used to feed and clothe the young, aged, and sickly slaves until the early 1830s.[446]
Washington's legacy endures as one of the most influential in American history, since he served as commander-in-chief of the Continental Army, a hero of the Revolution, and the first president of the United States. Various historians maintain that he also was a dominant factor in America's founding, the Revolutionary War, and the Constitutional Convention.[447] Revolutionary War comrade Light-Horse Harry Lee eulogized him as ""First in war—first in peace—and first in the hearts of his countrymen"".[448] Lee's words became the hallmark by which Washington's reputation was impressed upon the American memory, with some biographers regarding him as the great exemplar of republicanism. He set many precedents for the national government and the presidency in particular, and he was called the ""Father of His Country"" as early as 1778.[449][s]
In 1885, Congress proclaimed Washington's birthday to be a federal holiday.[451] Twentieth-century biographer Douglas Southall Freeman concluded, ""The great big thing stamped across that man is character."" Modern historian David Hackett Fischer has expanded upon Freeman's assessment, defining Washington's character as ""integrity, self-discipline, courage, absolute honesty, resolve, and decision, but also forbearance, decency, and respect for others"".[452]
Washington became an international symbol for liberation and nationalism, as the leader of the first successful revolution against a colonial empire. The Federalists made him the symbol of their party, but the Jeffersonians continued to distrust his influence for many years and delayed building the Washington Monument.[453] Washington was elected a member of the American Academy of Arts and Sciences on January 31, 1781, before he had even begun his presidency.[454] He was posthumously appointed to the grade of General of the Armies of the United States during the United States Bicentennial to ensure he would never be outranked; this was accomplished by the congressional joint resolution Public Law 94-479 passed on January 19, 1976, with an effective appointment date of July 4, 1976.[455][t]
Parson Weems wrote a hagiographic biography in 1809 to honor Washington.[458] Historian Ron Chernow maintains that Weems attempted to humanize Washington, making him look less stern, and to inspire ""patriotism and morality"" and to foster ""enduring myths"", such as Washington's refusal to lie about damaging his father's cherry tree.[459] Weems' accounts have never been proven or disproven.[460] Historian John Ferling, however, maintains that Washington remains the only founder and president ever to be referred to as ""godlike"", and points out that his character has been the most scrutinized by historians, past and present.[461] Historian Gordon S. Wood concludes that ""the greatest act of his life, the one that gave him his greatest fame, was his resignation as commander-in-chief of the American forces.""[462] Chernow suggests that Washington was ""burdened by public life"" and divided by ""unacknowledged ambition mingled with self-doubt"".[463] A 1993 review of presidential polls and surveys consistently ranked Washington number 4, 3, or 2 among presidents.[464] A 2018 Siena College Research Institute survey ranked him number 1 among presidents.[465]
Jared Sparks began collecting and publishing Washington's documentary record in the 1830s in Life and Writings of George Washington (12 vols., 1834–1837).[466] The Writings of George Washington from the Original Manuscript Sources, 1745–1799 (1931–1944) is a 39-volume set edited by John Clement Fitzpatrick, who was commissioned by the George Washington Bicentennial Commission. It contains more than 17,000 letters and documents and is available online from the University of Virginia.[467]
Numerous universities, including George Washington University and Washington University in St. Louis, were named in honor of Washington.[468][469]
Many places and monuments have been named in honor of Washington, most notably the nation's capital Washington, D.C. The state of Washington is the only state to be named after a president.[470]
George Washington appears on contemporary U.S. currency, including the one-dollar bill and the quarter-dollar coin (the Washington quarter). Washington and Benjamin Franklin appeared on the nation's first postage stamps in 1847. Washington has since appeared on many postage issues, more than any other person.[471]
Washington issue of 1862
Washington–Franklin issue of 1917
Washington quarter dollar
Washington on the 1928 dollar bill
"
File:GeorgeWashingtonByRobertField.jpg - Wikipedia," Original file ‎(1,643 × 1,903 pixels, file size: 1.8 MB, MIME type: image/jpeg)

Click on a date/time to view the file as it appeared at that time.
The following other wikis use this file:
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.

If the file has been modified from its original state, some details may not fully reflect the modified file."
Phase One (company) - Wikipedia," Phase One is a Danish company specializing in high-end digital photography equipment and software. They manufacture open platform based medium format camera systems and solutions. Their own RAW processing software, Capture One, supports many DSLRs besides their backs.
PODAS workshops (Phase One Digital Artist Series) is a series of worldwide photography workshops designed for digital photographers interested in working with medium format, high-resolution cameras. PODAS is a part of the Phase One educational division. Each attendee receives a Phase One digital camera system for the duration of the workshop.[2]
On 18 February 2014, it was announced that UK-based private equity firm Silverfleet Capital would acquire a 60% majority stake in the company.[3]
On June 17, 2019, Phase One A/S was once again sold, this time to the Danish investment company Axcel.
[4]
In 2009, Phase One purchased a major stake in Japanese Mamiya and the two companies develop products together. The following cameras are currently produced and sold by Phase One:
The Phase One 645DF+ and 645DF cameras are medium format cameras which support both focal plane and leaf shutter lenses with shutter speeds ranging from 1/4000s to 60 minutes and flash synchronization up to 1/1600 sec.[5][6] Among the new features on the 645DF+ are:[5]
The Phase One V-Grip Air vertical grip is compatible with the 645DF+/645DF. The V-Grip Air supports a Profoto Air flash trigger for wireless flash synchronization.[7]
The 645DF+/645DF supports digital back interfaces including the IQ and P+ series digital backs as well as 3rd party digital backs from Hasselblad, Leaf and others.
In 2012, Phase One released two specialty cameras: iXR[8] which is made specifically for reproduction and iXA[9] which is made specifically for aerial photography. Both uses the 645 lenses as the normal 645 cameras. Main difference on this camera is they have no viewfinder and very few mechanical moving parts.
In 2013, Phase One signed a collaborative distribution agreement with Digital Transitions (DT) to deliver advanced digitization solutions for cultural heritage preservation imaging projects worldwide (Repro camera solutions). The range of Digital Transitions digitization equipment includes a multitude of reprographic benches, purpose-built reprographic cameras, specialized book copy stations, film scanning kits, and accessories that are designed to host the line of Phase One digital capture hardware and Capture One software.
In 2014, Phase One launched a medium format digital back with a CMOS/active pixel sensor: The IQ250. All Phase One digital backs launched prior to the IQ250 have sensors based on the CCD (Charge-coupled device) technology.
In 2015, Phase One introduced the XF camera system. It is a new digital camera platform, medium format system built with the intention to upgrade it over a series of updates throughout the product lifetime. At the same time, the IQ3 series digital backs were introduced. This included the IQ3 50MP based on the previous IQ250, the IQ3 60MP based on the previous IQ260 and the newly introduced IQ3 80MP which includes a new CCD (Charge-coupled device) sensor exclusive to Phase One.
The 645DF / DF+ / XF is compatible with the following lenses:
Phase One XT compatible lenses
*In addition, any medium or large format lens which has sufficient image circle and resolution with the Cambo lens mount
RS Lenses compatible with iXM-RS
RSM lenses compatible with iXM
Lenses for iXM-MV
NOTE: iXM-MV is also compatible with Schneider-Kreuznach medium format lenses
The IQ series Phase One backs included many industry-first innovations. It was the first camera series to utilize a USB 3 connection. At the time of the release, this was not very widespread, but did allow for backwards compatibility with USB 2.0. It was also the first camera series to include a high resolution multi-touch display, similar to the ""Retina"" screen used in the iPhone 4.[11]
The P+ series are similar to the normal P series but have higher capture speeds, better response to long exposure times, and add Live Preview, which allows the user to focus and compose on a monitor while tethered. Also, a new high resolution LCD screen was implemented with better resolution and luminance.
The P series are fully untethered backs available for many different camera mounts.
The H series are tethered backs available for many different camera mounts. Camera back connects through standard 6pin IEEE 1394. Originally this type of camera back was released as the ""Lightphase"", a continuation of Phase One's previous tradition of using the name ""phase"" in the name of the product. This changed with the release of the ""H20"", which was originally called ""Lightphase H20"" but the name was changed to ""Phase One H20"" for better brand recognition.
The Scan backs are tethered digital scan backs. All use SCSI connection except for PowerPhase FX, which uses IEEE1394 ""Firewire"". The very early models, which were known as the CB6x (StudioKit) and FC70 (PhotoPhase), were made in plastic and had an external control unit that connected to a computer through NuBus.
The XF Camera System is upgradable and received various planned new features and functionality over its product lifetime.
Phase One reprographic camera systems and book capture / scanners are purpose-built to provide preservation-level rapid capture of rare books, circulation materials, manuscripts, documents, photographic slides, photographic negatives and photographic glass-plates.
Phase One repro camera solutions include the following products:
The above camera systems are purpose built for the following solutions but compatible with standard Reproduction systems.
The Capture One software comes in several flavours but is still a single binary. License key and option selected determine which version is active:
Capture One companion software:
"
File:JacquelineMegaw.jpg - Wikipedia," Original file ‎(10,328 × 7,760 pixels, file size: 29.75 MB, MIME type: image/jpeg)
Event: Introducing - Phase One IQ 180, Capture One 6.1
Model: Jacqueline Megaw
(Note: The personality rights of this photo is still owned by Jacqueline Megaw)
(Note2: Color-profile not handled correctly by non-Firefox browsers)
http://creativecommons.org/publicdomain/zero/1.0/deed.enCC0Creative Commons Zero, Public Domain Dedicationfalsefalse
Click on a date/time to view the file as it appeared at that time.
The following other wikis use this file:
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.

If the file has been modified from its original state, some details may not fully reflect the modified file."
Shutter speed - Wikipedia," 
In photography, shutter speed or exposure time is the length of time when the film or digital sensor inside the camera is exposed to light, also when a camera's shutter is open when taking a photograph.[1]
The amount of light that reaches the film or image sensor is proportional to the exposure time. ​1⁄500 of a second will let half as much light in as ​1⁄250.
The camera's shutter speed, the lens's aperture or f-stop, and the scene's luminance together determine the amount of light that reaches the film or sensor (the exposure). Exposure value (EV) is a quantity that accounts for the shutter speed and the f-number. Once the sensitivity to light of the recording surface (either film or sensor) is set in numbers expressed in ""ISOs"" (ex: 200 ISO, 400 ISO), the light emitted by the scene photographed can be controlled through aperture and shutter-speed to match the film or sensor sensitivity to light. This will achieve a good exposure when all the details of the scene are legible on the photograph. Too much light let into the camera results in an overly pale image (or ""over-exposure"") while too little light will result in an overly dark image (or ""under-exposure"").
Multiple combinations of shutter speed and f-number can give the same exposure value (E.V.). According to exposure value formula, doubling the exposure time doubles the amount of light (subtracts 1 EV).  Reducing the aperture size at multiples of one over the square root of two lets half as much light into the camera, usually at a predefined scale of f/1, f/1.4, f/2, f/2.8, f/4, f/5.6, f/8, f/11, f/16, f/22, and so on. For example, f/8 lets 4 times more light into the camera as f/16 does. A shutter speed of ​1⁄50 s with an f/4 aperture gives the same exposure value as a ​1⁄100 s shutter speed with an f/2.8 aperture, and also the same exposure value as a ​1⁄200 s shutter speed with an f/2 aperture, or ​1⁄25 s at f/5.6.
In addition to its effect on exposure, the shutter speed changes the way movement appears in photographs. Very short shutter speeds can be used to freeze fast-moving subjects, for example at sporting events. Very long shutter speeds are used to intentionally blur a moving subject for effect.[2] Short exposure times are sometimes called ""fast"", and long exposure times ""slow"".
Adjustments to the aperture need to be compensated by changes of the shutter speed to keep the same (right) exposure.
In early days of photography, available shutter speeds were not standardized, though a typical sequence might have been ​1⁄10 s, ​1⁄25 s, ​1⁄50 s, ​1⁄100 s, ​1⁄200 s and ​1⁄500 s; neither were apertures or film sensitivity (at least 3 different national standards existed). Soon this problem resulted in a solution consisting in the adoption of a standardized way of choosing aperture so that each major step exactly doubled or halved the amount of light entering the camera (f/2.8, f/4, f/5.6, f/8, f/11, f/16, etc.), a standardized 2:1 scale was adopted for shutter speed so that opening one aperture stop and reducing the amount of time of the shutter speed by one step resulted in the identical exposure. The agreed standards for shutter speeds are:[3]
With this scale, each increment roughly doubles the amount of light (longer time) or halves it (shorter time).
Camera shutters often include one or two other settings for making very long exposures:
The ability of the photographer to take images without noticeable blurring by camera movement is an important parameter in the choice of the slowest possible shutter speed for a handheld camera. The rough guide used by most 35 mm photographers is that the slowest shutter speed that can be used easily without much blur due to camera shake is the shutter speed numerically closest to the lens focal length. For example, for handheld use of a 35 mm camera with a 50 mm normal lens, the closest shutter speed is ​1⁄60 s (closest to ""50""), while for a 200 mm lens it is recommended not to choose shutter speeds below ​1⁄200 of a second. This rule can be augmented with knowledge of the intended application for the photograph, an image intended for significant enlargement and closeup viewing would require faster shutter speeds to avoid obvious blur. Through practice and special techniques such as bracing the camera, arms, or body to minimize camera movement, using a monopod or a tripod, slower shutter speeds can be used without blur. If a shutter speed is too slow for hand holding, a camera support, usually a tripod, must be used. Image stabilization on digital cameras or lenses can often permit the use of shutter speeds 3–4 stops slower (exposures 8–16 times longer).
Shutter priority refers to a shooting mode used in cameras. It allows the photographer to choose a shutter speed setting and allow the camera to decide the correct aperture. This is sometimes referred to as Shutter Speed Priority Auto Exposure, or TV (time value on Canon cameras) mode, S mode on Nikons and most other brands.
Shutter speed is one of several methods used to control the amount of light recorded by the camera's digital sensor or film. It is also used to manipulate the visual effects of the final image.
Slower shutter speeds are often selected to suggest the movement of an object in a still photograph.
Excessively fast shutter speeds can cause a moving subject to appear unnaturally frozen. For instance, a running person may be caught with both feet in the air with all indication of movement lost in the frozen moment.
When a slower shutter speed is selected, a longer time passes from the moment the shutter opens till the moment it closes. More time is available for movement in the subject to be recorded by the camera as a blur.
A slightly slower shutter speed will allow the photographer to introduce an element of blur, either in the subject, where, in our example, the feet, which are the fastest moving element in the frame, might be blurred while the rest remains sharp; or if the camera is panned to follow a moving subject, the background is blurred while the subject remains relatively sharp.
The exact point at which the background or subject will start to blur depends on the speed at which the object is moving, the angle that the object is moving in relation to the camera, the distance it is from the camera and the focal length of the lens in relation to the size of the digital sensor or film.
When slower shutter-speeds, in excess of about half a second, are used on running water, the water in the photo will have a ghostly white appearance reminiscent of fog. This effect can be used in landscape photography.
Zoom burst is a technique which entails the variation of the focal length of a zoom lens during a longer exposure. In the moment that the shutter is opened, the lens is zoomed in, changing the focal length during the exposure. The center of the image remains sharp, while the details away from the center form a radial blur, which causes a strong visual effect, forcing the eye into the center of the image.[4]
The following list provides an overview of common photographic uses for standard shutter speeds.
Star trails like these are created by using a long exposure to capture the apparent motion of the stars.[14]
The Whirligig ride during night at SFGAm at an exposure time of 0.8 seconds.
Light streaks outside London Waterloo station.
Effect of different shutter speeds on photograph.
A 30-second exposure of the rotating New Technology Telescope
Motion picture cameras used in traditional film cinematography employ a mechanical rotating shutter. The shutter rotation is synchronized with film being pulled through the gate, hence shutter speed is a function of the frame rate and shutter angle.
Where E = shutter speed (reciprocal of exposure time in seconds), F = frames per second, and S = shutter angle:[15]
With a traditional shutter angle of 180°, film is exposed for ​1⁄48 second at 24 frame/s.[15] To avoid effect of light interference when shooting under artificial lights or when shooting television screens and computer monitors, ​1⁄50 s (172.8°) or ​1⁄60 s (144°) shutter is often used.[16]
Electronic video cameras do not have mechanical shutters and allow setting shutter speed directly in time units. Professional video cameras often allow selecting shutter speed in terms of shutter angle instead of time units, especially those that are capable of overcranking or undercranking.
"
List of street photographers - Wikipedia," This is a list of notable street photographers. Street photography is photography conducted for art or enquiry that presents unmediated chance encounters and random incidents[1] within public places. Street photography does not need the backdrop of a street or even an urban environment. Though people are usually present, street photography may lack people and can be of an object or environment where the image projects a decidedly human character in facsimile or aesthetic.[2]
"
Martin Elkort - Wikipedia," Martin Edward Elkort (April 18, 1929 - November 19, 2016) was an American photographer, illustrator and writer known primarily for his street photography.  Prints of his work are held and displayed by several prominent art museums in the United States.  His photographs have regularly appeared in galleries and major publications.  Early black and white photographs by Elkort feature the fabled Lower East Side in Manhattan, New York City, showing its ethnic diversity, myriad streets and cluttered alleys.  The Coney Island amusement park in Brooklyn was another favorite site during that period.  His later work depicts street scenes from downtown Los Angeles and Tijuana, Mexico. Throughout Martin Elkort's long career as a photographer, he always showed the positive, joyful side of life in his candid images.[1]
Born in the Bronx, New York City, Martin Elkort grew up during the Great Depression.  At the age of 15, he suffered a bout with polio and spent four months in the hospital.[2]  When he returned home, his parents gave him his first Ciroflex, a twin-lens reflex camera, that cost them about a week’s salary.[3]
Elkort took his first professional photograph at the age of 10 while on a car trip with his parents to Baltimore. During the trip, he took photographs of flooded streets. The Baltimore Sun purchased his photographs of flood scenes and featured one of them on its front page.[3][4]  After his recovery from polio, he set out around Manhattan taking pictures of whatever interested him.[3]
Elkort was a member of New York Photo League from 1948–1951; an editorial associate and contributor to New Mexico Magazine in 1957; a founding member in 2002 of Los Angeles League of Photographers (LALOP); a contributing editor and contributed photographs to Rangefinder Magazine in 2006; and a member of the Photography Arts Council at Los Angeles County Museum of Art.[citation needed]
While studying at New York City's Cooper Union School of Art, Elkort joined the New York Photo League, an organization of photographers that served as the epicenter of the documentary movement in American photography.[4]   There he studied under masters like Paul Strand, Aaron Siskind, Sid Grossman, Lou Stoumen, Imogen Cunningham, Weegee and many other luminaries, learning to become adept at what he refers to as ‘stealth photography’. With a more refined Rolleiflex twin-lens reflex camera strapped around his neck, he would roam the streets peering down into the 2×2 inch ground glass. He developed the skill of walking right up to a person and taking their photo without them even realizing it. His goal was to capture this post-war period's general optimism and innocence.[2]  During this period he worked at the Wildenstein & Company Gallery and later, the Stephen Michael Studio in Manhattan where he further enhanced his photographic knowledge and technique.
In 1948, Elkort showed his pictures of Hasidic Jewish boys playing in the streets to Edward Steichen, who was curator of photography at New York's Museum of Modern Art and probably America's most famous photographer at the time.[5]  Steichen rejected his photos, describing Martin's skills as ""no better than the other 35 million amateur photographers in the country."" Dejected but determined, Elkort worked tirelessly to improve his craft and two years later, he met with Steichen again. This time the famous curator bought three of his images for the museum's collection: ""Soda Fountain Girl"",[6] ""Puppy Love"",[7] and ""The Girl With Black Cat"",[8] all uplifting images of children at Coney Island.[4]
Elkort's photographs (c. 1951) of recently liberated Jewish immigrants learning new work skills at the Bramson ORT (Organization for Rehabilitation and Training) School in Brooklyn offer a rare and intimate glimpse into of their optimistic struggle to integrate into a new society after World War II. Some of his pictures show Jewish workers bearing tattoos evidencing their incarceration in Nazi concentration camps during The Holocaust. In 1951, more than 20,000 Jews received vocational training at the Bramson ORT School.[9]    Seamstresses, tailors, pattern makers, pressers; here they learned a trade that was much needed in New York’s growing fashion and garment district. In 2008, Elkort donated 33 of his vintage ORT photographs to the United States Holocaust Memorial Museum in Washington, D.C..
After receiving a digital camera for his 70th birthday, Martin's photographic career re-ignited. He began to show his current and older work in galleries around the country. He also found a renewed interest in the New York Photo League.[2]
In 2002, he co-founded the Los Angeles League of Photographers[10]  along with David Schulman and David Stork. Modeled after the New York Photo League, its mission is to expose the wider public to photography's essential social, political and aesthetic values.  He also writes articles for magazines dealing with photography including Rangefinder and Black & White Magazine.
As of March 2014[update], Elkort's work is widely exhibited and can be found in the permanent collections of the United States Holocaust Memorial Museum in Washington, D.C.; the Museum of Modern Art in New York City; The Jewish Museum (Manhattan) in New York City; the Columbus Museum of Art; The Museum of Fine Arts, Houston; The J. Paul Getty Museum in Los Angeles; as well as many corporate and private collections.[3]
Following his retirement from the travel industry in 1996, Elkort authored two books, Getting from Fired to Hired and The Secret Life of Food.  He also wrote numerous magazine articles for Rangefinder and Black & White magazines.
In the 1970s, Martin and his wife Edythe bought and ran a travel agency in Beverly Hills, California, catering to a clientele that included many Hollywood stars.[2]
In 1976, Martin and his longtime friend Murray Vidockler founded the Society for Accessible Travel & Hospitality (SATH) [11]  to promote better wheelchair access on buses and at airports, hotels and major destinations.
Elkort's work is held in the following collections:
"
Lower East Side - Wikipedia," 
The Lower East Side, sometimes abbreviated as LES and sometimes referred to as Loisaida, is a neighborhood in the southeastern part of the New York City borough of Manhattan, roughly between the Bowery and the East River from Canal to Houston streets. Traditionally an immigrant, working-class neighborhood, it began rapid gentrification in the mid-2000s, prompting the National Trust for Historic Preservation to place the neighborhood on their list of America's Most Endangered Places.[6][7]
The Lower East Side is part of Manhattan Community District 3, and its primary ZIP Code is 10002.[1] It is patrolled by the 7th Precinct of the New York City Police Department.
The Lower East Side is roughly bounded by the Bowery to the west, East Houston Street to the north, the FDR Drive and East River to the east, and Canal Street to the south. The western boundary below Grand Street veers east off of the Bowery to approximately Essex Street.
The neighborhood is bordered in the south and west by Chinatown, which extends north to roughly Grand Street, in the west by Nolita and in the north by the East Village.[8][9]
Historically, the ""Lower East Side"" referred to the area alongside the East River from about the Manhattan Bridge and Canal Street up to 14th Street, and roughly bounded on the west by Broadway. It included areas known today as East Village, Alphabet City, Chinatown, Bowery, Little Italy, and NoLIta. Parts of the East Village are still known as Loisaida, a Latino pronunciation of ""Lower East Side"".

Politically, the neighborhood is in New York's 7th[10] and 12th[11] congressional districts.[12] It is in the New York State Assembly's 65th district and 74th district;[13][14] the New York State Senate's 26th district;[15] and New York City Council's 1st and 2nd districts.[16]As was all of Manhattan Island, the area now known as the Lower East Side was occupied by members of the Lenape tribe, who were organized in bands which moved from place to place according to the seasons, fishing on the rivers in the summer, and moving inland in the fall and winter to gather crops and hunt for food. Their main trail took approximately the route of Broadway. One encampment in the Lower East Side area, near Corlears Hook was called Rechtauck or Naghtogack.[17]
The population of the Dutch colony of New Amsterdam was located primarily below the current Fulton Street, while north of it were a number of small plantations and large farms called bouwerij (bowery) at the time (equivalent to ""boerderij"" in present-day Dutch). Around these farms were a number of enclaves of free or ""half-free"" Africans, which served as a buffer between the Dutch and the Native Americans. One of the largest of these was located along the modern Bowery between Prince Street and Astor Place, as well as the ""only separate enclave"" of this type within Manhattan.[18] These black farmers were some of the earliest settlers of the area.[19]
Gradually, during the 17th century, there was an overall consolidation of the boweries and farms into larger parcels, and much of the Lower East Side was then part of the Delancy farm.[19]
James Delancey's pre-Revolutionary farm east of post road leading from the city (Bowery) survives in the names Delancey Street and Orchard Street. On the modern map of Manhattan, the Delancey farm[20] is represented in the grid of streets from Division Street north to Houston Street.[21] In response to the pressures of a growing city, Delancey began to survey streets in the southern part of the ""West Farm""[22] in the 1760s. A spacious projected Delancey Square—intended to cover the area within today's Eldridge, Essex, Hester and Broome Streets—was eliminated when the loyalist Delancey family's property was confiscated after the American Revolution. The city Commissioners of Forfeiture eliminated the aristocratic planned square for a grid, effacing Delancey's vision of a New York laid out like the West End of London.
The point of land on the East River now called Corlears Hook was also called Corlaers Hook under Dutch and British rule, and briefly Crown Point during British occupation in the Revolution. It was named after the schoolmaster Jacobus van Corlaer, who settled on this ""plantation"" that in 1638 was called by a Europeanized version of its Lenape name, Nechtans[23] or Nechtanc.[24] Corlaer sold the plantation to Wilhelmus Hendrickse Beekman (1623–1707), founder of the Beekman family of New York; his son Gerardus Beekman was christened at the plantation, on August 17, 1653.
On February 25, 1643, volunteers from the New Amsterdam colony killed thirty[25] Wiechquaesgecks at their encampment at Corlears Hook, as part of Kieft's War, in retaliation for ongoing conflicts between the colonists and the natives of the area, including their unwillingness to pay tribute, and their refusal to turn over the killer of a colonist.[26]
The projection into the East River that retained Corlaer's name was an important landmark for navigators for 300 years. On older maps and documents it is usually spelled Corlaers Hook, but since the early 19th century the spelling has been anglicized to Corlears. The rough unplanned settlement that developed at Corlaer's Hook under the British occupation of New York during the Revolution was separated from the densely populated city by rough hills of glacial till: ""this region lay beyond the city proper, from which it was separated by high, uncultivated, and rough hills"", observers recalled in 1843.[27]
As early as 1816, Corlears Hook was notorious for streetwalkers, ""a resort for the lewd and abandoned of both sexes"", and in 1821 its ""streets abounding every night with preconcerted groups of thieves and prostitutes"" were noted by the ""Christian Herald"".[28] In the course of the 19th century they came to be called hookers.[29] In the summer of cholera in New York, 1832, a two-storey wooden workshop was commandeered to serve as a makeshift cholera hospital; between July 18 and September 15 when the hospital was closed, as the cholera wound down, 281 patients were admitted, both black and white, of whom 93 died.[30]
In 1833, Corlear's Hook was the location of some of the first tenements built in New York City.[19]
Corlears Hook is mentioned in the opening page of Herman Melville's Moby Dick, first published in 1851: ""Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears Hook to Coenties Slip, and from thence, by Whitehall, northward. What do you see? ...""
The original location of Corlears Hook is now obscured by shoreline landfill.[31] It was near the east end of the present pedestrian bridge over the FDR Drive near Cherry Street. The name is preserved in Corlears Hook Park at the intersection of Jackson and Cherry Streets along the East River Drive.[32]
The bulk of immigrants who came to New York City in the late 19th and early 20th centuries came to the Lower East Side, moving into crowded tenements there.[33] By the 1840s, large numbers of German immigrants settled in the area, and a large part of it became known as ""Little Germany"" or ""Kleindeutschland"".[19][34] This was followed by groups of Italians and Eastern European Jews, as well as Greeks, Hungarians, Poles, Romanians, Russians, Slovaks and Ukrainians, each of whom settled in relatively homogeneous enclaves.  By 1920, the Jewish neighborhood was one of the largest of these ethnic groupings, with 400,000 people, pushcart vendors prominent on Orchard and Grand Streets, and numerous Yiddish theatres along Second Avenue between Houston and 14th Streets.[19]
Living conditions in these ""slum"" areas were far from ideal, although some improvement came from a change in the zoning laws which required ""new law"" tenements to be built with air shafts between them, so that fresh air and some light could reach each apartment. Still, reform movements, such as the one started by Jacob A. Riis' book How the Other Half Lives continued to attempt to alleviate the problems of the area through settlement houses, such as the Henry Street Settlement, and other welfare and service agencies.  The city itself moved to address the problem when it built First Houses, the first such public housing project in the United States, in 1935-1936. The development, located on the south side of East 3rd Street between First Avenue and Avenue A, and on the west side of Avenue A between East 2nd and East 3rd Streets, is now considered to be located within the East Village.[19]
By the turn of the twentieth century, the neighborhood had become closely associated with radical politics, such as anarchism, socialism and communism, and was also known as a place where many popular performers had grown up, such as the Marx Brothers, Eddie Cantor, Al Jolson, George and Ira Gershwin, Jimmy Durante, and Irving Berlin.  Later, more radical artists such as the Beat poets and writers were drawn to the neighborhood – especially the parts which later became the East Village – by the inexpensive housing and cheap food.[19]
The German population decreased in the early twentieth century as a result of the General Slocum disaster and due to anti-German sentiment prompted by World War I. After World War II, the Lower East Side became New York City's first racially integrated neighborhood with the influx of African Americans and Puerto Ricans. Areas where Spanish speaking was predominant began to be called Loisaida.[19]
By the 1960s, the influence of the Jewish and eastern European groups declined as many of these residents had left the area, while other ethnic groups had coalesced into separate neighborhood, such as Little Italy.  The Lower East Side then experienced a period of ""persistent poverty, crime, drugs, and abandoned housing"".[19] A substantial portion of the neighborhood was slated for demolition under the Cooper Square Urban Renewal Plan of 1956, which was to redevelop the area from Ninth to Delancey Streets from the Bowery/Third Avenue to Chrystie Street/Second Avenue with new privately owned cooperative housing.[33]:38[35] The United Housing Foundation was selected as the sponsor for the project, which faced great opposition from the community.[36] Neither the original large-scale development nor a 1961 revised proposal were implemented,[33]:39 and it was not until 1991 that an agreement was made to redevelop a small portion of the proposed renewal site.[37]
The East Village was once considered the Lower East Side's northwest corner. However, in the 1960s, the demographics of the area above Houston Street began to change, as hipsters, musicians, and artists moved in. Newcomers and real estate brokers popularized the East Village name, and the term was adopted by the popular media by the mid-1960s. As the East Village developed a culture separate from the rest of the Lower East Side, the two areas came to be seen as two separate neighborhoods rather than the former being part of the latter.[38][39]
By the 1980s, the Lower East Side had begun to stabilize after its period of decline, and once again began to attract students, artists and adventurous members of the middle-class, as well as immigrants from countries such as Bangladesh, China, the Dominican Republic, India, Japan, Korea, the Philippines and Poland.[19]
In the early 2000s, the gentrification of the East Village spread to the Lower East Side proper, making it one of the trendiest neighborhoods in Manhattan. Orchard Street, despite its ""Bargain District"" moniker, is now lined with upscale boutiques. Similarly, trendy restaurants, including Clinton St. Baking Company & Restaurant, Cube 63, and Falai are found on a stretch of tree-lined Clinton Street that New York Magazine described as the ""hippest restaurant row"" on the Lower East Side.[40][41]
In November 2007, the Blue Condominium, a 32-unit, 16 story luxury condominium tower was completed at 105 Norfolk Street just north of Delancey Street, the pixellated, faceted blue design of which starkly contrasts with the surrounding neighborhood. Following the construction of the Hotel on Rivington one block away, several luxury condominiums around Houston, and the New Museum on Bowery, this new wave of construction is another sign that the gentrification cycle is entering a high-luxury phase similar to in SoHo and Nolita in the previous decade.
More recently, the gentrification that was previously confined to north of Delancey Street continued south. Several restaurants, bars, and galleries opened below Delancey Street after 2005, especially around the intersection of Broome and Orchard Streets. The neighborhood's second boutique hotel, Blue Moon Hotel, opened on Orchard Street just south of Delancey Street in early 2006. However, unlike The Hotel on Rivington, the Blue Moon used an existing tenement building, and its exterior is almost identical to neighboring buildings. In September 2013, it was announced that the Essex Crossing redevelopment project was to be built in the area, centered around the intersection of Essex and Delancey Streets, but mostly utilizing land south of Delancey Street.[42]
The census tabulation area for the Lower East Side is bounded to the north by 14th Street and to the west by Avenue B, Norfolk Street, Essex Street, and Pike Street. Based on data from the 2010 United States Census, the population of Lower East Side was 72,957, an increase of 699 (1.0%) from the 72,258 counted in 2000. Covering an area of 535.91 acres (216.88 ha), the neighborhood had a population density of 136.1 inhabitants per acre (87,100/sq mi; 33,600/km2).[2] The racial makeup of the neighborhood was 22.6% (16,453) White, 10.9% (7,931) African American, 0.2% (142) Native American, 24.9% (18,166) Asian, 0.0% (13) Pacific Islander, 0.3% (191) from other races, and 1.6% (1,191) from two or more races. Hispanic or Latino of any race were 39.6% (28,870) of the population.[3]
The racial composition of the Lower East Side changed moderately from 2000 to 2010, with the most significant changes being the White population's increase by 18% (2,514), the Asian population's increase by 10% (1,673), and the Hispanic / Latino population's decrease by 10% (3,219). The minority Black population experienced a slight increase by 1% (41), while the very small population of all other races decreased by 17% (310).[43]
The Lower East Side lies in Manhattan Community District 3, which encompasses the Lower East Side, the East Village and Chinatown. Community District 3 had 171,103 inhabitants as of NYC Health's 2018 Community Health Profile, with an average life expectancy of 82.2 years.[44]:2, 20 This is higher than the median life expectancy of 81.2 for all New York City neighborhoods.[45]:53 (PDF p. 84) Most inhabitants are adults: a plurality (35%) are between the ages of 25–44, while 25% are between 45–64, and 16% are 65 or older. The ratio of youth and college-aged residents was lower, at 13% and 11% respectively.[44]:2
As of 2017, the median household income in Community District 3 was $39,584,[46] though the median income in the Lower East Side individually was $51,649.[4] In 2018, an estimated 18% of Community District 3 residents lived in poverty, compared to 14% in all of Manhattan and 20% in all of New York City. One in twelve residents (8%) were unemployed, compared to 7% in Manhattan and 9% in New York City. Rent burden, or the percentage of residents who have difficulty paying their rent, is 48% in Community District 3, compared to the boroughwide and citywide rates of 45% and 51% respectively.  Based on this calculation, as of 2018[update], Community District 3 is considered to be gentrifying: according to the Community Health Profile, the district was low-income in 1990 and has seen above-median rent growth up to 2010.[44]:7
One of the oldest neighborhoods of the city, the Lower East Side has long been a lower-class worker neighborhood and often a poor and ethnically diverse section of New York. As well as Irish, Italians, Poles, Ukrainians, and other ethnic groups, it once had a sizeable German population and was known as Little Germany (Kleindeutschland). Today it is a predominantly Puerto Rican and Dominican community, and in the process of gentrification (as documented by the portraits of its residents in the Clinton+Rivington chapter of The Corners Project.)[47]
Since the immigration waves from eastern Europe in the late 19th and early 20th century, the Lower East Side became known as having been a center of Jewish immigrant culture. In her 2000 book Lower East Side Memories: A Jewish Place in America, Hasia Diner explains that the Lower East Side is especially remembered as a place of Jewish beginnings for Ashkenazi American Jewish culture.[48] Vestiges of the area's Jewish heritage exist in shops on Hester and Essex Streets, and on Grand Street near Allen Street. An Orthodox Jewish community is based in the area, operating  yeshiva day schools and a mikvah. A few Judaica shops can be found along Essex Street and a few Jewish scribes and variety stores. Some kosher delis and bakeries, as well as a few ""kosher style"" delis, including the famous Katz's Deli, are located in the neighborhood. Second Avenue in the Lower East Side was home to many Yiddish theatre productions in the Yiddish Theater District during the early part of the 20th century, and Second Avenue came to be known as ""Yiddish Broadway,"" though most of the theaters are gone. Songwriter Irving Berlin, actor John Garfield, and singer Eddie Cantor grew up here.
Since the mid-20th century, the area has been settled primarily by immigrants, primarily from Latin America, especially Central America and Puerto Rico. They have established their own groceries and shops, marketing goods from their culture and cuisine. Bodegas have replaced Jewish shops. They are mostly Roman Catholic.
In what is now the East Village, the earlier populations of Poles and Ukrainians have moved on and been largely supplanted by newer immigrants. The immigration of numerous Japanese people over the last fifteen years or so has led to the proliferation of Japanese restaurants and specialty food markets. There is also a notable population of Bangladeshis and other immigrants from Muslim countries, many of whom are congregants of the small Madina Masjid (Mosque), located on First Avenue and 11th Street.
The neighborhood still has many historic synagogues, such as the Bialystoker Synagogue,[49] Beth Hamedrash Hagadol, the Eldridge Street Synagogue,[50] Kehila Kedosha Janina (the only Greek synagogue in the Western Hemisphere),[51] the Angel Orensanz Center (the fourth oldest synagogue building in the United States), and various smaller synagogues along East Broadway. Another landmark, the First Roumanian-American congregation (the Rivington Street synagogue) partially collapsed in 2006, and was subsequently demolished. In addition, there is a major Hare Krishna temple and several Buddhist houses of worship.
Chinese residents have also been moving into Lower East Side, and since the late 20th century, they have comprised a large immigrant group in the area. The part of the neighborhood south of Delancey Street and west of Allen Street has, in large measure, become part of Chinatown. Grand Street is one of the major business and shopping streets of Chinatown. Also contained within the neighborhood are strips of lighting and restaurant supply shops on the Bowery.
While the Lower East Side has been a place of successive immigrant populations, many American Jews relate to the neighborhood in a strong manner, and Chinatown holds a special place in the imagination of Chinese Americans,[52][53] just as Astoria in Queens holds a place in the hearts of Greek Americans. It was a center for the ancestors of many people in the metropolitan area, and it was written about and portrayed in fiction and films.
In the late twentieth century, Jewish communities have worked to preserve a number of buildings associated with the Jewish immigrant community.[54][55][56]
Landmarks include:
Synagogues include:
Little Fuzhou (Chinese: 小福州; pinyin: Xiǎo Fúzhōu; Foochow Romanized: Siēu-hók-ciŭ), or Fuzhou Town (Chinese: 福州埠; pinyin: Fúzhōu Bù; Foochow Romanized: Hók-ciŭ-pú) is a neighborhood within the eastern sliver of Chinatown, in the Two Bridges and Lower East Side areas of Manhattan. Starting in the 1980s and especially in the 1990s, the neighborhood became a prime destination for immigrants from Fuzhou, Fujian, China. Manhattan's Little Fuzhou is centered on East Broadway. However, since the 2000s, Chinatown, Brooklyn became New York City's new primary destination for the Fuzhou immigrants evolving a second Little Fuzhou of the city and has now far surpassed as being the largest Fuzhou cultural center of the New York metropolitan area and still rapidly growing in contrast to Manhattan's Little Fuzhou, which is now undergoing gentrification.
Since the 2010s, the Fuzhou immigrant population and businesses have been declining throughout the whole eastern portion of Manhattan's Chinatown due to the gentrification. There is a rapidly increasing influx of high income professionals moving into this area, often non-Chinese, including high end hipster-owned businesses.[62][63]
The neighborhood has become home to numerous contemporary art galleries. One of the first was ABC No Rio.[64] Begun by a group of Colab no wave artists (some living on Ludlow Street), ABC No Rio opened an outsider gallery space that invited community participation and encouraged the widespread production of art. Taking an activist approach to art that grew out of The Real Estate Show (the take over of an abandoned building by artists to open an outsider gallery only to have it chained closed by the police) ABC No Rio kept its sense of activism, community, and outsiderness. The product of this open, expansive approach to art was a space for creating new works that did not have links to the art market place and that were able to explore new artistic possibilities.
Other outsider galleries sprung up throughout the Lower East Side and East Village—some 200 at the height of the scene in the 1980s, including the 124 Ridge Street Gallery among others. In December 2007, the New Museum relocated to a brand-new, critically acclaimed building on Bowery at Prince. A growing number of galleries are opening in the Bowery neighborhood to be in close proximity to the museum. The Museum of Reclaimed Urban Space, which opened in 2012, exhibits photography featuring the neighborhood in addition to chronicling its history of activism.
Social service agencies like Henry Street Settlement and Educational Alliance have visual and performing arts programs, the former at Abrons Arts Center, a home for contemporary interdisciplinary arts.
The neighborhood is also home to several graffiti artists, such as Chico and Jean-Michel Basquiat.
As the neighborhood gentrified and has become safer at night, it has become a popular late night destination.  Orchard, Ludlow and Essex between Rivington Street and Stanton Street have become especially packed at night, and the resulting noise is a cause of tension between bar owners and longtime residents.[65][66] However, as gentrification continues, many established landmarks and venues have been lost.[67]
The Lower East Side is also home to many live music venues. Punk bands played at C-Squat[citation needed] and alternative rock bands play at Bowery Ballroom on Delancey Street and Mercury Lounge on East Houston Street. Punk bands play at Otto's Shrunken Head and R-Bar. Punk and alternative bands play at Bowery Electric just north of the old CBGB's location.[68] There are also bars that offer performance space, such as Pianos on Ludlow Street and Arlene's Grocery on Stanton Street.

The Lower East Side is the location of the Slipper Room a burlesque, variety and vaudeville theatre on Orchard and Stanton.  Lady Gaga, Leonard Cohen and U2 have all appeared there, while popular downtown performers Dirty Martini, Murray Hill and Matt Fraser often appear. Variety shows are regularly hosted by comedians James Habacker, Bradford Scobie, Matthew Holtzclaw and Matt Roper under the guise of various characters.The Lower East Side is patrolled by the 7th Precinct of the NYPD, located at ​19.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px;white-space:nowrap} 1⁄2 Pitt Street.[69] The 7th Precinct, along with the neighboring 5th Precinct, ranked 48th safest out of 69 patrol areas for per-capita crime in 2010.[70] As of 2018[update], with a non-fatal assault rate of 42 per 100,000 people, the Lower East Side and East Village's rate of violent crimes per capita is less than that of the city as a whole. The incarceration rate of 449 per 100,000 people is higher than that of the city as a whole.[44]:8
The 7th Precinct has a lower crime rate than in the 1990s, with crimes across all categories having decreased by 64.8% between 1990 and 2019. The precinct reported 0 murders, 7 rapes, 149 robberies, 187 felony assaults, 94 burglaries, 507 grand larcenies, and 18 grand larcenies auto in 2019.[71]
The Lower East Side is served by two New York City Fire Department (FDNY) fire stations:[72]
As of 2018[update], preterm births and births to teenage mothers are less common in the Lower East Side and East Village than in other places citywide. In the Lower East Side and East Village, there were 82 preterm births per 1,000 live births (compared to 87 per 1,000 citywide), and 10.1 births to teenage mothers per 1,000 live births (compared to 19.3 per 1,000 citywide).[44]:11 The Lower East Side and East Village have a low population of residents who are uninsured. In 2018, this population of uninsured residents was estimated to be 11%, slightly less than the citywide rate of 12%.[44]:14
The concentration of fine particulate matter, the deadliest type of air pollutant, in the Lower East Side and East Village is 0.0089 milligrams per cubic metre (8.9×10−9 oz/cu ft), more than the city average.[44]:9 Twenty percent of Lower East Side and East Village residents are smokers, which is more than the city average of 14% of residents being smokers.[44]:13 In the Lower East Side and East Village, 10% of residents are obese, 11% are diabetic, and 22% have high blood pressure—compared to the citywide averages of 24%, 11%, and 28% respectively.[44]:16 In addition, 16% of children are obese, compared to the citywide average of 20%.[44]:12
Eighty-eight percent of residents eat some fruits and vegetables every day, which is about the same as the city's average of 87%. In 2018, 70% of residents described their health as ""good,"" ""very good,"" or ""excellent,"" less than the city's average of 78%.[44]:13 For every supermarket in the Lower East Side and East Village, there are 18 bodegas.[44]:10
The nearest major hospitals are Beth Israel Medical Center in Stuyvesant Town, as well as the Bellevue Hospital Center and NYU Langone Medical Center in Kips Bay, and NewYork-Presbyterian Lower Manhattan Hospital in the Civic Center area.[75][76] In addition, FDNY EMS Division 1/Station 4 is located on Pier 39.
The Lower East Side is located within the ZIP Code 10002.[77] The United States Postal Service operates two post offices in the Lower East Side:
The Lower East Side and East Village generally have a higher rate of college-educated residents than the rest of the city as of 2018[update]. A plurality of residents age 25 and older (48%) have a college education or higher, while 24% have less than a high school education and 28% are high school graduates or have some college education. By contrast, 64% of Manhattan residents and 43% of city residents have a college education or higher.[44]:6 The percentage of Lower East Side and East Village students excelling in math rose from 61% in 2000 to 80% in 2011, and reading achievement increased from 66% to 68% during the same time period.[80]
The Lower East Side and East Village's rate of elementary school student absenteeism is lower than the rest of New York City. In the Lower East Side and East Village, 16% of elementary school students missed twenty or more days per school year, less than the citywide average of 20%.[45]:24 (PDF p. 55)[44]:6 Additionally, 77% of high school students in the Lower East Side and East Village graduate on time, more than the citywide average of 75%.[44]:6
The New York City Department of Education operates public schools in the Lower East Side as part of Community School District 1.[81] District 1 does not contain any zoned schools, which means that students living in District 1 can apply to any school in the district, including those in the East Village.[82][83]
The following public elementary schools are located in the Lower East Side, serving grades PK-5 unless otherwise indicated:[81]
The following public elementary/middle schools are located in the Lower East Side, serving grades PK-8 unless otherwise indicated:[81]
The following public middle and high schools are located in the Lower East Side:[81]
The Lower East Side Preparatory High School (LESPH) and Emma Lazarus High School (ELHS) are second-chance schools that enable students, aged 17–21, to obtain their high school diplomas. LESPH is a bilingual Chinese-English school with a high proportion of Asian students. ELHS' instructional model is English-immersion with an ethnically diverse student body.
The Seward Park Campus comprises five schools with an average graduation rate of about 80%. The original school in the building was opened 1929 and closed 2006.[99]
The New York Public Library (NYPL) operates two branches in the Lower East Side. The Seward Park branch is located at 4192 East Broadway. It was founded by the Aguilar Free Library Society in 1886, and the current three-story Carnegie library building was opened in 1909 and renovated in 2004.[100] The Hamilton Fish Park branch is located at 415 East Houston Street. It was originally built as a Carnegie library in 1909, but was torn down when Houston Street was expanded; the current one-story structure was completed in 1960.[101]
The Lower East Side is home to many private parks, such as La Plaza Cultural.[102] There are several public parks in the area, including Sara D. Roosevelt Park between Chrystie and Forsyth Streets from Houston to Canal Streets,[103] as well as Seward Park on Essex Street between Hester Street and East Broadway.[104]
The East River shorefront contains the John V. Lindsay East River Park, a public park running between East 12th Street in the East Village and Montgomery Street in the Lower East Side.[105] Planned for the waterfront is Pier 42, the first section of which is scheduled to open in 2021.[106]
There are multiple New York City Subway stations in the neighborhood, including Grand Street (B and ​D trains), Bowery (J and ​Z trains), Second Avenue (F and <F>​ trains), Delancey Street–Essex Street (F, <F>​​, J, M, and Z​ trains), and East Broadway (F and <F>​ trains).[107] New York City Bus routes include M9, M14A SBS, M14D SBS, M15, M15 SBS, M21, M22, M103 and B39.[108]
The Williamsburg Bridge and Manhattan Bridge connect the Lower East Side to Brooklyn. The FDR Drive is on the neighborhood's south and east ends.[109]
As of 2018[update], thirty-seven percent of roads in the Lower East Side have bike lanes.[44]:10 Bike lanes are present on Allen, Chrystie, Clinton, Delancey, Grand, Houston, Montgomery, Madison, Rivington, Stanton, and Suffolk Streets; Bowery, East Broadway, and FDR Drive; the Williamsburg and Manhattan bridges; and the East River Greenway.[110]
The Lower East Side is served by NYC Ferry's Lower East Side route, which stops at Corlears Hook in the East River Park.[111] The service started operating on August 29, 2018.[112][113]
Children's literature
Novels
Songs
Plays
Films
Television
Video games
Music videos
Notes
Bibliography

"
Buddhism - Wikipedia," 

Buddhism (/ˈbʊdɪzəm/, US: /ˈbuːd-/)[1][2] is the world's fourth-largest religion[3][4] with over 520 million followers, or over 7% of the global population, known as Buddhists.[5][6] Buddhism encompasses a variety of traditions, beliefs and spiritual practices largely based on original teachings attributed to the Buddha (born Siddhārtha Gautama in the 5th or 4th century BCE) and resulting interpreted philosophies. It originated in ancient India as a Sramana tradition sometime between the 6th and 4th centuries BCE, spreading through much of Asia. Two major extant branches of Buddhism are generally recognized by scholars: Theravāda (Pali: ""The School of the Elders"") and Mahāyāna (Sanskrit: ""The Great Vehicle"").
As expressed in the Buddha's Four Noble Truths, the goal of Buddhism is to overcome suffering (duḥkha) caused by desire, attachment to a static self, and ignorance of the true nature of reality (avidya).[7] Most Buddhist traditions emphasize transcending the individual self through the attainment of Nirvana or by following the path of Buddhahood, ending the cycle of death and rebirth.[8][9][10] Buddhist schools vary in their interpretation of the path to liberation, the relative importance and canonicity assigned to the various Buddhist texts, and their specific teachings and practices.[11][12] Widely observed practices include taking refuge in the Buddha, the Dharma and the Sangha, observance of moral precepts, Buddhist monasticism, Buddhist meditation, and the cultivation of the Paramitas (perfections, or virtues).
Theravada Buddhism has a widespread following in Sri Lanka and Southeast Asia such as Cambodia, Laos, Myanmar and Thailand. Mahayana, which includes the traditions of Pure Land, Zen, Nichiren Buddhism, Shingon and Tiantai (Tendai), is found throughout East Asia such as China, Japan, Korea, Vietnam, Taiwan and Singapore. Vajrayana, a body of teachings attributed to Indian adepts, may be viewed as a separate branch or as an aspect of Mahayana Buddhism.[13] Tibetan Buddhism, which preserves the Vajrayana teachings of eighth-century India, is practised in the countries of the Himalayan region, Mongolia,[14] and Kalmykia.[15]
Buddhism is an Indian religion[16] founded on the teachings of a mendicant and spiritual teacher called ""the Buddha"" (""the Awakened One"", c. 5th to 4th century BCE).[17][18] Early texts have the Buddha's family name as ""Gautama"" (Pali: Gotama). The details of Buddha's life are mentioned in many Early Buddhist Texts but are inconsistent. His social background and life details are difficult to prove, and the precise dates are uncertain.[19][note 1]
The evidence of the early texts suggests that Siddharta Gautama was born in Lumbini and grew up in Kapilavastu,[note 2] a town in the Ganges Plain, near the modern Nepal–India border, and that he spent his life in what is now modern Bihar[note 3] and Uttar Pradesh.[27][19] Some hagiographic legends state that his father was a king named Suddhodana, his mother was Queen Maya, and he was born in Lumbini.[28] However, scholars such as Richard Gombrich consider this a dubious claim because a combination of evidence suggests he was born in the Shakya community, which was governed by a small oligarchy or republic-like council where there were no ranks but where seniority mattered instead.[29][note 4] Some of the stories about Buddha, his life, his teachings, and claims about the society he grew up in may have been invented and interpolated at a later time into the Buddhist texts.[32][33]
According to early texts such as the Pali Ariyapariyesanā-sutta (""The discourse on the noble quest,"" MN 26) and its Chinese parallel at MĀ 204, Gautama was moved by the suffering (dukkha) of life and death, and its endless repetition due to rebirth.[34] He thus set out on a quest to find liberation from suffering (also known as ""nirvana"").[35] Early texts and biographies state that Gautama first studied under two teachers of meditation, namely Alara Kalama (Sanskrit: Arada Kalama) and Uddaka Ramaputta (Sanskrit: Udraka Ramaputra), learning meditation and philosophy, particularly the meditative attainment of ""the sphere of nothingness"" from the former, and ""the sphere of neither perception nor non-perception"" from the latter.[36][37][note 5]
Finding these teachings to be insufficient to attain his goal, he turned to the practice of severe asceticism, which included a strict fasting regime and various forms of breath control.[40] This too fell short of attaining his goal, and then he turned to the meditative practice of dhyana. He famously sat in meditation under a Ficus religiosa tree now called the Bodhi Tree in the town of Bodh Gaya and attained ""Awakening"" (Bodhi).[citation needed]
According to various early texts like the Mahāsaccaka-sutta, and the Samaññaphala Sutta, on awakening, the Buddha gained insight into the workings of karma and his former lives, as well as achieving the ending of the mental defilements (asavas), the ending of suffering, and the end of rebirth in saṃsāra.[40] This event also brought certainty about the Middle Way as the right path of spiritual practice to end suffering.[41][42] As a fully enlightened Buddha, he attracted followers and founded a Sangha (monastic order).[43] He spent the rest of his life teaching the Dharma he had discovered, and then died, achieving ""final nirvana,"" at the age of 80 in Kushinagar, India.[44][22]
Buddha's teachings were propagated by his followers, which in the last centuries of the 1st millennium BCE became various Buddhist schools of thought, each with its own basket of texts containing different interpretations and authentic teachings of the Buddha;[45][46][47] these over time evolved into many traditions of which the more well known and widespread in the modern era are Theravada, Mahayana and Vajrayana Buddhism.[48][49][note 6]
The term ""Buddhism"" is an occidental neologism, commonly (and ""rather roughly"" according to Donald S. Lopez Jr.) used as a translation for the Dharma of the Buddha, fójiào in Chinese, bukkyō in Japanese, nang pa sangs rgyas pa'i chos in Tibetan, buddhadharma in Sanskrit, buddhaśāsana in Pali.[52]
The Four Truths express the basic orientation of Buddhism: we crave and cling to impermanent states and things, which is dukkha, ""incapable of satisfying"" and painful.[53][54] This keeps us caught in saṃsāra, the endless cycle of repeated rebirth, dukkha and dying again.[note 7]
But there is a way to liberation from this endless cycle[60] to the state of nirvana, namely following the Noble Eightfold Path.[note 8]
The truth of dukkha is the basic insight that life in this mundane world, with its clinging and craving to impermanent states and things[53] is dukkha, and unsatisfactory.[55][66][web 1] Dukkha can be translated as ""incapable of satisfying,""[web 5] ""the unsatisfactory nature and the general insecurity of all conditioned phenomena""; or ""painful.""[53][54] Dukkha is most commonly translated as ""suffering,"" but this is inaccurate, since it refers not to episodic suffering, but to the intrinsically unsatisfactory nature of temporary states and things, including pleasant but temporary experiences.[note 9] We expect happiness from states and things which are impermanent, and therefore cannot attain real happiness.
In Buddhism, dukkha is one of the three marks of existence, along with impermanence and anattā (non-self).[72] Buddhism, like other major Indian religions, asserts that everything is impermanent (anicca), but, unlike them, also asserts that there is no permanent self or soul in living beings (anattā).[73][74][75] The ignorance or misperception (avijjā) that anything is permanent or that there is self in any being is considered a wrong understanding, and the primary source of clinging and dukkha.[76][77][78]
Dukkha arises when we crave (Pali: taṇhā) and cling to these changing phenomena. The clinging and craving produces karma, which ties us to samsara, the cycle of death and rebirth.[79][web 6][note 10] Craving includes kama-tanha, craving for sense-pleasures; bhava-tanha, craving to continue the cycle of life and death, including rebirth; and vibhava-tanha, craving to not experience the world and painful feelings.[79][80][81]
Dukkha ceases, or can be confined,[82] when craving and clinging cease or are confined. This also means that no more karma is being produced, and rebirth ends.[note 11] Cessation is nirvana, ""blowing out,"" and peace of mind.[84][85]
By following the Buddhist path to moksha, liberation,[62] one starts to disengage from craving and clinging to impermanent states and things. The term ""path"" is usually taken to mean the Noble Eightfold Path, but other versions of ""the path"" can also be found in the Nikayas.[86] The Theravada tradition regards insight into the four truths as liberating in itself.[68]
Saṃsāra means ""wandering"" or ""world"", with the connotation of cyclic, circuitous change.[87][88] It refers to the theory of rebirth and ""cyclicality of all life, matter, existence"", a fundamental assumption of Buddhism, as with all major Indian religions.[88][89] Samsara in Buddhism is considered to be dukkha, unsatisfactory and painful,[90] perpetuated by desire and avidya (ignorance), and the resulting karma.[88][91][92]
The theory of rebirths, and realms in which these rebirths can occur, is extensively developed in Buddhism, in particular Tibetan Buddhism with its wheel of existence (Bhavacakra) doctrine.[90] Liberation from this cycle of existence, nirvana, has been the foundation and the most important historical justification of Buddhism.[93][94]
The later Buddhist texts assert that rebirth can occur in six realms of existence, namely three good realms (heavenly, demi-god, human) and three evil realms (animal, hungry ghosts, hellish).[note 12] Samsara ends if a person attains nirvana, the ""blowing out"" of the desires and the gaining of true insight into impermanence and non-self reality.[96][97][98]
Rebirth refers to a process whereby beings go through a succession of lifetimes as one of many possible forms of sentient life, each running from conception to death.[99] In Buddhist thought, this rebirth does not involve any soul, because of its doctrine of anattā (Sanskrit: anātman, no-self doctrine) which rejects the concepts of a permanent self or an unchanging, eternal soul, as it is called in Hinduism and Christianity.[100] According to Buddhism there ultimately is no such thing as a self in any being or any essence in any thing.[101]
The Buddhist traditions have traditionally disagreed on what it is in a person that is reborn, as well as how quickly the rebirth occurs after each death.[102][103] Some Buddhist traditions assert that ""no self"" doctrine means that there is no perduring self, but there is avacya (inexpressible) self which migrates from one life to another.[102] The majority of Buddhist traditions, in contrast, assert that vijñāna (a person's consciousness) though evolving, exists as a continuum and is the mechanistic basis of what undergoes rebirth, rebecoming and redeath.[55][102] The rebirth depends on the merit or demerit gained by one's karma, as well as that accrued on one's behalf by a family member.[note 13]
Each rebirth takes place within one of five realms according to Theravadins, or six according to other schools – heavenly, demi-gods, humans, animals, hungry ghosts and hellish.[105][106][note 14]
In East Asian and Tibetan Buddhism, rebirth is not instantaneous, and there is an intermediate state (Tibetan ""bardo"") between one life and the next.[116][117] The orthodox Theravada position rejects the wait, and asserts that rebirth of a being is immediate.[116] However there are passages in the Samyutta Nikaya of the Pali Canon that seem to lend support to the idea that the Buddha taught about an intermediate stage between one life and the next.[118][119]
In Buddhism, karma (from Sanskrit: ""action, work"") drives saṃsāra – the endless cycle of suffering and rebirth for each being. Good, skilful deeds (Pāli: kusala) and bad, unskilful deeds (Pāli: akusala) produce ""seeds"" in the unconscious receptacle (ālaya) that mature later either in this life or in a subsequent rebirth.[120][121] The existence of karma is a core belief in Buddhism, as with all major Indian religions, and it implies neither fatalism nor that everything that happens to a person is caused by karma.[122][note 15]
A central aspect of Buddhist theory of karma is that intent (cetanā) matters and is essential to bring about a consequence or phala ""fruit"" or vipāka ""result"".[123][note 16] However, good or bad karma accumulates even if there is no physical action, and just having ill or good thoughts creates karmic seeds; thus, actions of body, speech or mind all lead to karmic seeds.[122] In the Buddhist traditions, life aspects affected by the law of karma in past and current births of a being include the form of rebirth, realm of rebirth, social class, character and major circumstances of a lifetime.[122][127][128] It operates like the laws of physics, without external intervention, on every being in all six realms of existence including human beings and gods.[122][129]
A notable aspect of the karma theory in Buddhism is merit transfer.[130][131] A person accumulates merit not only through intentions and ethical living, but also is able to gain merit from others by exchanging goods and services, such as through dāna (charity to monks or nuns).[132] Further, a person can transfer one's own good karma to living family members and ancestors.[131][note 17]
The cessation of the kleshas and the attainment of nirvana (nibbāna), with which the cycle of rebirth ends, has been the primary and the soteriological goal of the Buddhist path for monastic life since the time of the Buddha.[62][135][136] The term ""path"" is usually taken to mean the Noble Eightfold Path, but other versions of ""the path"" can also be found in the Nikayas.[note 18] In some passages in the Pali Canon, a distinction is being made between right knowledge or insight (sammā-ñāṇa), and right liberation or release (sammā-vimutti), as the means to attain cessation and liberation.[137][138]
Nirvana literally means ""blowing out, quenching, becoming extinguished"".[139][140] In early Buddhist texts, it is the state of restraint and self-control that leads to the ""blowing out"" and the ending of the cycles of sufferings associated with rebirths and redeaths.[141][142][143] Many later Buddhist texts describe nirvana as identical with anatta with complete ""emptiness, nothingness"".[144][145][146][note 19] In some texts, the state is described with greater detail, such as passing through the gate of emptiness (sunyata) – realising that there is no soul or self in any living being, then passing through the gate of signlessness (animitta) – realising that nirvana cannot be perceived, and finally passing through the gate of wishlessness (apranihita) – realising that nirvana is the state of not even wishing for nirvana.[135][148][note 20]
The nirvana state has been described in Buddhist texts partly in a manner similar to other Indian religions, as the state of complete liberation, enlightenment, highest happiness, bliss, fearlessness, freedom, permanence, non-dependent origination, unfathomable, and indescribable.[150][151] It has also been described in part differently, as a state of spiritual release marked by ""emptiness"" and realisation of non-self.[152][153][154][note 21]
While Buddhism considers the liberation from saṃsāra as the ultimate spiritual goal, in traditional practice, the primary focus of a vast majority of lay Buddhists has been to seek and accumulate merit through good deeds, donations to monks and various Buddhist rituals in order to gain better rebirths rather than nirvana.[157][111][note 22]
Pratityasamutpada, also called ""dependent arising, or dependent origination"", is the Buddhist theory to explain the nature and relations of being, becoming, existence and ultimate reality. Buddhism asserts that there is nothing independent, except the state of nirvana.[158] All physical and mental states depend on and arise from other pre-existing states, and in turn from them arise other dependent states while they cease.[159]
The 'dependent arisings' have a causal conditioning, and thus Pratityasamutpada is the Buddhist belief that causality is the basis of ontology, not a creator God nor the ontological Vedic concept called universal Self (Brahman) nor any other 'transcendent creative principle'.[160][161] However, the Buddhist thought does not understand causality in terms of Newtonian mechanics, rather it understands it as conditioned arising.[162][163] In Buddhism, dependent arising is referring to conditions created by a plurality of causes that necessarily co-originate a phenomenon within and across lifetimes, such as karma in one life creating conditions that lead to rebirth in one of the realms of existence for another lifetime.[164][165][166]
Buddhism applies the dependent arising theory to explain origination of endless cycles of dukkha and rebirth, through its Twelve Nidānas or ""twelve links"" doctrine. It states that because Avidyā (ignorance) exists Saṃskāras (karmic formations) exists, because Saṃskāras exists therefore Vijñāna (consciousness) exists, and in a similar manner it links Nāmarūpa (sentient body), Ṣaḍāyatana (six senses), Sparśa (sensory stimulation), Vedanā (feeling), Taṇhā (craving), Upādāna (grasping), Bhava (becoming), Jāti (birth), and Jarāmaraṇa (old age, death, sorrow, pain).[167][168]
By breaking the circuitous links of the Twelve Nidanas, Buddhism asserts that liberation from these endless cycles of rebirth and dukkha can be attained.[169]
A related doctrine in Buddhism is that of anattā (Pali) or anātman (Sanskrit). It is the view that there is no unchanging, permanent self, soul or essence in phenomena.[170] The Buddha and Buddhist philosophers who follow him such as Vasubandhu and Buddhaghosa, generally argue for this view through by analyzing the person through the schema of the five aggregates, and then attempting to show that none of these five components of personality can be permanent or absolute.[171] This can be seen in Buddhist discourses such as the Anattalakkhana Sutta.
""Emptiness"" or ""voidness"" (Skt: Śūnyatā, Pali: Suññatā), is a related concept with many different interpretations throughout the various Buddhisms. In early Buddhism, it was commonly stated that all five aggregates are void (rittaka), hollow (tucchaka), coreless (asāraka), for example as in the Pheṇapiṇḍūpama Sutta (SN 22:95).[172] Similarly, in Theravada Buddhism, it often simply means that the five aggregates are empty of a Self.[173]
Emptiness is a central concept in Mahāyāna Buddhism, especially in Nagarjuna's Madhyamaka school, and in the Prajñāpāramitā sutras. In Madhyamaka philosophy, emptiness is the view which holds that all phenomena (dharmas) are without any svabhava (literally ""own-nature"" or ""self-nature""), and are thus without any underlying essence, and so are ""empty"" of being independent. This doctrine sought to refute the heterodox theories of svabhava circulating at the time.[174]
All forms of Buddhism revere and take spiritual refuge in the ""three jewels"" (triratna): Buddha, Dharma and Sangha.[175]
While all varieties of Buddhism revere ""Buddha"" and ""buddhahood"", they have different views on what these are. Whatever that may be, ""Buddha"" is still central to all forms of Buddhism.
In Theravada Buddhism, a Buddha is someone who has become awake through their own efforts and insight. They have put an end to their cycle of rebirths and have ended all unwholesome mental states which lead to bad action and thus are morally perfected.[176] While subject to the limitations of the human body in certain ways (for example, in the early texts, the Buddha suffers from backaches), a Buddha is said to be ""deep, immeasurable, hard-to-fathom as is the great ocean,"" and also has immense psychic powers (abhijñā).[177]
Theravada generally sees Gautama Buddha (the historical Buddha Sakyamuni) as the only Buddha of the current era. While he is no longer in this world, he has left us the Dharma (Teaching), the Vinaya (Discipline) and the Sangha (Community).[178] There are also said to be two types of Buddhas, a sammasambuddha is also said to teach the Dharma to others, while a paccekabuddha (solitary buddha) does not teach.[176]
Mahāyāna Buddhism meanwhile, has a vastly expanded cosmology, with various Buddhas and other holy beings (aryas) residing in different realms. Mahāyāna texts not only revere numerous Buddhas besides Sakyamuni, such as Amitabha and Vairocana, but also see them as transcendental or supramundane (lokuttara) beings.[179] Mahāyāna Buddhism holds that these other Buddhas in other realms can be contacted and are able to benefit beings in this world.[180] In Mahāyāna, a Buddha is a kind of ""spiritual king"", a ""protector of all creatures"" with a lifetime that is countless of eons long, rather than just a human teacher who has transcended the world after death.[181] Buddha Sakyamuni's life and death on earth is then usually understood as a ""mere appearance"" or ""a manifestation skilfully projected into earthly life by a long-enlightened transcendent being, who is still available to teach the faithful through visionary experiences.""[181][182]
""Dharma"" (Pali: Dhamma) in Buddhism refers to the Buddha's teaching, which includes all of the main ideas outlined above. While this teaching reflects the true nature of reality, it is not a belief to be clung to, but a pragmatic teaching to be put into practice. It is likened to a raft which is ""for crossing over"" (to nirvana) not for holding on to.[183]
It also refers to the universal law and cosmic order which that teaching both reveals and relies upon.[184] It is an everlasting principle which applies to all beings and worlds. In that sense it is also the ultimate truth and reality about the universe, it is thus ""the way that things really are.""
The Dharma is the second of the three jewels which all Buddhists take refuge in. All Buddhas in all worlds, in the past, present and in the future, are believed by Buddhists to understand and teach the Dharma. Indeed, it is part of what makes them a Buddha that they do so.
The third ""jewel"" which Buddhists take refuge in is the ""Sangha"", which refers to the monastic community of monks and nuns who follow Gautama Buddha's monastic discipline which was ""designed to shape the Sangha as an ideal community, with the optimum conditions for spiritual growth.""[185] The Sangha consists of those who have chosen to follow the Buddha's ideal way of life, which is one of celibate monastic renunciation with minimal material possessions (such as an alms bowl and robes).[186]
The Sangha is seen as important because they preserve and pass down Buddha Dharma. As Gethin states ""the Sangha lives the teaching, preserves the teaching as Scriptures and teaches the wider community. Without the Sangha there is no Buddhism.""[187]
The Sangha also acts as a ""field of merit"" for laypersons, allowing them to make spiritual merit or goodness by donating to the Sangha and supporting them. In return, they keep their duty to preserve and spread the Dharma everywhere for the good of the world.[188]
The Sangha is also supposed to follow the Vinaya (monastic rule) of the Buddha, thereby serving as an spiritual example for the world and future generations. The Vinaya rules also force the Sangha to live in dependence on the rest of the lay community (they must beg for food etc) and thus draw the Sangha into a relationship with the lay community.[189]
There is also a separate definition of Sangha, referring to those who have attained any stage of awakening, whether or not they are monastics. This sangha is called the āryasaṅgha ""noble Sangha"".[190] All forms of Buddhism generally reveres these āryas (Pali: ariya, ""noble ones"" or ""holy ones"") who are spiritually attained beings. Aryas have attained the fruits of the Buddhist path.[191] Becoming an arya is a goal in most forms of Buddhism. The āryasaṅgha includes holy beings such as bodhisattvas, arhats and stream-enterers. 
In early Buddhism and in Theravada Buddhism, an arhat (literally meaning ""worthy"") is someone who reached the same awakening (bodhi) of a Buddha by following the teaching of a Buddha.[192] They are seen as having ended rebirth and all the mental defilements. A bodhisattva (""a being bound for awakening"") meanwhile, is simply a name for someone who is working towards awakening (bodhi) as a Buddha. According to all the early buddhist schools as well as Theravada, to be considered a bodhisattva one has to have made a vow in front of a living Buddha and also has to have received a confirmation of one's future Buddhahood.[193] In Theravada, the future Buddha is called Metteyya (Maitreya) and he is revered as a bodhisatta currently working for future Buddhahood.[193]
Mahāyāna Buddhism generally sees the attainment of the arhat as an inferior one, since it is seen as being done only for the sake of individual liberation. It thus promotes the bodhisattva path as the highest and most worthwhile.[194] While in Mahāyāna, anyone who has given rise to bodhicitta (the wish to become a Buddha that arises from a sense of compassion for all beings) is considered a bodhisattva,[195] some of these holy beings (such as Maitreya and Avalokiteshvara) have reached very high levels of spiritual attainment and are seen as being very powerful supramundane beings who provide aid to countless beings through their advanced powers.[196]
Mahāyāna Buddhism also differs from Theravada and the other schools of early Buddhism in promoting several unique doctrines which are contained in Mahāyāna sutras and philosophical treatises.
One of these is the unique interpretation of emptiness and dependent origination found in the Madhyamaka school. Another very influential doctrine for Mahāyāna is the main philosophical view of the Yogācāra school variously, termed Vijñaptimātratā-vāda (""the doctrine that there are only ideas"" or ""mental impressions"") or Vijñānavāda (""the doctrine of consciousness""). According to Mark Siderits, what classical Yogācāra thinkers like Vasubandhu had in mind is that we are only ever aware of mental images or impressions, which may appear as external objects, but ""there is actually no such thing outside the mind.""[197] There are several interpretations of this main theory, many scholars see it as a type of Idealism, others as a kind of phenomenology.[198]
Another very influential concept unique to Mahāyāna is that of ""Buddha-nature"" (buddhadhātu) or ""Tathagata-womb"" (tathāgatagarbha). Buddha-nature is a concept found in some 1st-millennium CE Buddhist texts, such as the Tathāgatagarbha sūtras. According to Paul Williams these Sutras suggest that 'all sentient beings contain a Tathagata' as their 'essence, core inner nature, Self'.[199][note 23] According to Karl Brunnholzl ""the earliest mahayana sutras that are based on and discuss the notion of tathāgatagarbha as the buddha potential that is innate in all sentient beings began to appear in written form in the late second and early third century.""[201] For some, the doctrine seems to conflict with the Buddhist anatta doctrine (non-Self), leading scholars to posit that the Tathāgatagarbha Sutras were written to promote Buddhism to non-Buddhists.[202][203] This can be seen in texts like the Laṅkāvatāra Sūtra, which state that Buddha-nature is taught to help those who have fear when they listen to the teaching of anatta.[204] Buddhist texts like the Ratnagotravibhāga clarify that the ""Self"" implied in Tathagatagarbha doctrine is actually ""not-Self"".[205][206] Various interpretations of the concept have been advanced by Buddhist thinkers throughout the history of Buddhist thought and most attempt to avoid anything like the Hindu Atman doctrine.
These Indian Buddhist ideas, in various synthetic ways, form the basis of subsequent Mahāyāna philosophy in Tibetan Buddhism and East Asian Buddhism.
While the Noble Eightfold Path is best-known in the West, a wide variety of paths and models of progress have been used and described in the different Buddhist traditions. However, they generally share basic practices such as sila (ethics), samadhi (meditation, dhyana) and prajña (wisdom), which are known as the three trainings. An important additional practice is a kind and compassionate attitude toward every living being and the world. Devotion is also important in some Buddhist traditions, and in the Tibetan traditions visualisations of deities and mandalas are important. The value of textual study is regarded differently in the various Buddhist traditions. It is central to Theravada and highly important to Tibetan Buddhism, while the Zen tradition takes an ambiguous stance.
An important guiding principle of Buddhist practice is the Middle Way (madhyamapratipad). It was a part of Buddha's first sermon, where he presented the Noble Eightfold Path that was a 'middle way' between the extremes of asceticism and hedonistic sense pleasures.[207][208] In Buddhism, states Harvey, the doctrine of ""dependent arising"" (conditioned arising, pratītyasamutpāda) to explain rebirth is viewed as the 'middle way' between the doctrines that a being has a ""permanent soul"" involved in rebirth (eternalism) and ""death is final and there is no rebirth"" (annihilationism).[209][210]
A common presentation style of the path (mārga) to liberation in the Early Buddhist Texts is the ""graduated talk"", in which the Buddha lays out a step by step training.[211]
In the early texts, numerous different sequences of the gradual path can be found.[212] One of the most important and widely used presentations among the various Buddhist schools is The Noble Eightfold Path, or ""Eightfold Path of the Noble Ones"" (Skt. 'āryāṣṭāṅgamārga'). This can be found in various discourses, most famously in the Dhammacakkappavattana Sutta (The discourse on the turning of the Dharma wheel).
Other suttas such as the Tevijja Sutta, and the Cula-Hatthipadopama-sutta give a different outline of the path, though with many similar elements such as ethics and meditation.[212]
According to Rupert Gethin, the path to awakening is also frequently summarized by another a short formula: ""abandoning the hindrances, practice of the four establishings of mindfulness, and development of the awakening factors.""[213]
The Eightfold Path consists of a set of eight interconnected factors or conditions, that when developed together, lead to the cessation of dukkha.[214] These eight factors are: Right View (or Right Understanding), Right Intention (or Right Thought), Right Speech, Right Action, Right Livelihood, Right Effort, Right Mindfulness, and Right Concentration.
This Eightfold Path is the fourth of the Four Noble Truths, and asserts the path to the cessation of dukkha (suffering, pain, unsatisfactoriness).[215][216] The path teaches that the way of the enlightened ones stopped their craving, clinging and karmic accumulations, and thus ended their endless cycles of rebirth and suffering.[217][218][219]
The Noble Eightfold Path is grouped into three basic divisions, as follows:[220][221][222]
Theravada Buddhism is a diverse tradition and thus includes different explanations of the path to awakening. However, the teachings of the Buddha are often encapsulated by Theravadins in the basic framework of the Four Noble Truths and the Eighthfold Path.[232][233]
Some Theravada Buddhists also follow the presentation of the path laid out in Buddhaghosa's Visuddhimagga. This presentation is known as the ""Seven Purifications"" (satta-visuddhi).[234] This schema and its accompanying outline of ""insight knowledges"" (vipassanā-ñāṇa) is used by modern influential Theravadin scholars, such Mahasi Sayadaw (in his ""The Progress of Insight"") and Nyanatiloka Thera (in ""The Buddha's Path to Deliverance"").[235][236]
Mahāyāna Buddhism is based principally upon the path of a Bodhisattva.[237] A Bodhisattva refers to one who is on the path to buddhahood.[238] The term Mahāyāna was originally a synonym for Bodhisattvayāna or ""Bodhisattva Vehicle.""[239][240][241]
In the earliest texts of Mahāyāna Buddhism, the path of a bodhisattva was to awaken the bodhicitta.[242] Between the 1st and 3rd century CE, this tradition introduced the Ten Bhumi doctrine, which means ten levels or stages of awakening.[242] This development was followed by the acceptance that it is impossible to achieve Buddhahood in one (current) lifetime, and the best goal is not nirvana for oneself, but Buddhahood after climbing through the ten levels during multiple rebirths.[243] Mahāyāna scholars then outlined an elaborate path, for monks and laypeople, and the path includes the vow to help teach Buddhist knowledge to other beings, so as to help them cross samsara and liberate themselves, once one reaches the Buddhahood in a future rebirth.[237] One part of this path are the pāramitā (perfections, to cross over), derived from the Jatakas tales of Buddha's numerous rebirths.[244][245]
The doctrine of the bodhisattva bhūmis was also eventually merged with the Sarvāstivāda Vaibhāṣika schema of the ""five paths"" by the Yogacara school.[246] This Mahāyāna ""five paths"" presentation can be seen in Asanga's Mahāyānasaṃgraha.[246]
The Mahāyāna texts are inconsistent in their discussion of the pāramitās, and some texts include lists of two, others four, six, ten and fifty-two.[247][248][249] The six paramitas have been most studied, and these are:[244][249][250]
In Mahāyāna Sutras that include ten pāramitā, the additional four perfections are ""skillful means, vow, power and knowledge"".[248] The most discussed pāramitā and the highest rated perfection in Mahayana texts is the ""Prajna-paramita"", or the ""perfection of insight"".[248] This insight in the Mahāyāna tradition, states Shōhei Ichimura, has been the ""insight of non-duality or the absence of reality in all things"".[254][255]
East Asian Buddhism in influenced by both the classic Indian Buddhist presentations of the path such as the eighth-fold path as well as classic Indian Mahāyāna presentations such as that found in the Da zhidu lun.[256]
There many different presentations of soteriology, including numerous paths and vehicles (yanas) in the different traditions of East Asian Buddhism.[257] There is no single dominant presentation. In Zen Buddhism for example, one can find outlines of the path such as the Two Entrances and Four Practices, The Five ranks, The Ten Ox-Herding Pictures and The Three mysterious Gates of Linji.
In Indo-Tibetan Buddhism, the path to liberation is outlined in the genre known as Lamrim (""Stages of the Path""). All the various Tibetan schools have their own Lamrim presentations. This genre can be traced to Atiśa's 11th-century A Lamp for the Path to Enlightenment (Bodhipathapradīpa).[258]
In various suttas which present the graduated path taught by the Buddha, such as the Samaññaphala Sutta and the Cula-Hatthipadopama Sutta, the first step on the path is hearing the Buddha teach the Dharma.[212] This then said to lead to the acquiring of confidence or faith in the Buddha's teachings.[212]
Mahayana Buddhist teachers such as Yin Shun also state that hearing the Dharma and study of the Buddhist discourses is necessary ""if one wants to learn and practice the Buddha Dharma.""[259] Likewise, in Indo-Tibetan Buddhism, the ""Stages of the Path"" (Lamrim) texts generally place the activity of listening to the Buddhist teachings as an important early practice.[260]
Traditionally, the first step in most Buddhist schools requires taking of the ""Three Refuges"", also called the Three Jewels (Sanskrit: triratna, Pali: tiratana) as the foundation of one's religious practice.[261] This practice may have been influenced by the Brahmanical motif of the triple refuge, found in the Rigveda 9.97.47, Rigveda 6.46.9 and Chandogya Upanishad 2.22.3–4.[262] Tibetan Buddhism sometimes adds a fourth refuge, in the lama. The three refuges are believed by Buddhists to be protective and a form of reverence.[261]
The ancient formula which is repeated for taking refuge affirms that ""I go to the Buddha as refuge, I go to the Dhamma as refuge, I go to the Sangha as refuge.""[263] Reciting the three refuges, according to Harvey, is considered not as a place to hide, rather a thought that ""purifies, uplifts and strengthens the heart"".[175]
Śīla (Sanskrit) or sīla (Pāli) is the concept of ""moral virtues"", that is the second group and an integral part of the Noble Eightfold Path.[223] It generally consists of right speech, right action and right livelihood.[223]
One of the most basic forms of ethics in Buddhism is the taking of ""precepts"". This includes the Five Precepts for laypeople, Eight or Ten Precepts for monastic life, as well as rules of Dhamma (Vinaya or Patimokkha) adopted by a monastery.[264][265]
Other important elements of Buddhist ethics include giving or charity (dāna), Mettā (Good-Will), Heedfulness (Appamada), ‘self-respect’ (Hri) and 'regard for consequences' (Apatrapya).
Buddhist scriptures explain the five precepts (Pali: pañcasīla; Sanskrit: pañcaśīla) as the minimal standard of Buddhist morality.[224] It is the most important system of morality in Buddhism, together with the monastic rules.[266]
The five precepts are seen as a basic training applicable to all Buddhists. They are:[264][267][268]
Undertaking and upholding the five precepts is based on the principle of non-harming (Pāli and Sanskrit: ahiṃsa).[275] The Pali Canon recommends one to compare oneself with others, and on the basis of that, not to hurt others.[276] Compassion and a belief in karmic retribution form the foundation of the precepts.[277][278] Undertaking the five precepts is part of regular lay devotional practice, both at home and at the local temple.[279][280] However, the extent to which people keep them differs per region and time.[281][280] They are sometimes referred to as the śrāvakayāna precepts in the Mahāyāna tradition, contrasting them with the bodhisattva precepts.[282]
The five precepts are not commandments and transgressions do not invite religious sanctions, but their power has been based on the Buddhist belief in karmic consequences and their impact in the afterlife. Killing in Buddhist belief leads to rebirth in the hell realms, and for a longer time in more severe conditions if the murder victim was a monk. Adultery, similarly, invites a rebirth as prostitute or in hell, depending on whether the partner was unmarried or married.[283] These moral precepts have been voluntarily self-enforced in lay Buddhist culture through the associated belief in karma and rebirth.[284] Within the Buddhist doctrine, the precepts are meant to develop mind and character to make progress on the path to enlightenment.[285]
The monastic life in Buddhism has additional precepts as part of patimokkha, and unlike lay people, transgressions by monks do invite sanctions. Full expulsion from sangha follows any instance of killing, engaging in sexual intercourse, theft or false claims about one's knowledge. Temporary expulsion follows a lesser offence.[286] The sanctions vary per monastic fraternity (nikaya).[287]
Lay people and novices in many Buddhist fraternities also uphold eight (asta shila) or ten (das shila) from time to time. Four of these are same as for the lay devotee: no killing, no stealing, no lying, and no intoxicants.[288] The other four precepts are:[289][288]
All eight precepts are sometimes observed by lay people on uposatha days: full moon, new moon, the first and last quarter following the lunar calendar.[288] The ten precepts also include to abstain from accepting money.[288]
In addition to these precepts, Buddhist monasteries have hundreds of rules of conduct, which are a part of its patimokkha.[290][note 24]
Vinaya is the specific code of conduct for a sangha of monks or nuns. It includes the Patimokkha, a set of 227 offences including 75 rules of decorum for monks, along with penalties for transgression, in the Theravadin tradition.[292] The precise content of the Vinaya Pitaka (scriptures on the Vinaya) differs in different schools and tradition, and different monasteries set their own standards on its implementation. The list of pattimokkha is recited every fortnight in a ritual gathering of all monks.[292] Buddhist text with vinaya rules for monasteries have been traced in all Buddhist traditions, with the oldest surviving being the ancient Chinese translations.[293]
Monastic communities in the Buddhist tradition cut normal social ties to family and community, and live as ""islands unto themselves"".[294] Within a monastic fraternity, a sangha has its own rules.[294] A monk abides by these institutionalised rules, and living life as the vinaya prescribes it is not merely a means, but very nearly the end in itself.[294] Transgressions by a monk on Sangha vinaya rules invites enforcement, which can include temporary or permanent expulsion.[295]
Another important practice taught by the Buddha is the restraint of the senses (indriyasamvara). In the various graduated paths, this is usually presented as a practice which is taught prior to formal sitting meditation, and which supports meditation by weakening sense desires that are a hindrance to meditation.[296] According to Anālayo, sense restraint is when one ""guards the sense doors in order to prevent sense impressions from leading to desires and discontent.""[296] This is not an avoidance of sense impression, but a kind of mindful attention towards the sense impressions which does not dwell on their main features or signs (nimitta). This is said to prevent harmful influences from entering the mind.[297] This practice is said to give rise to an inner peace and happiness which forms a basis for concentration and insight.[297]
A related Buddhist virtue and practice is renunciation, or the intent for desirelessness (nekkhamma).[298] Generally, renunciation is the giving up of actions and desires that are seen as unwholesome on the path, such as lust for sensuality and worldly things.[299] Renunciation can be cultivated in different ways. The practice of giving for example, is one form of cultivating renunciation. Another one is the giving up of lay life and becoming a monastic (bhiksu o bhiksuni).[300] Practicing celibacy (whether for life as a monk, or temporarily) is also a form of renunciation.[301]</ref> Many Jataka stories such as the focus on how the Buddha practiced renunciation in past lives.[302]
One way of cultivating renunciation taught by the Buddha is the contemplation (anupassana) of the ""dangers"" (or ""negative consequences"") of sensual pleasure (kāmānaṃ ādīnava). As part of the graduated discourse, this contemplation is taught after the practice of giving and morality.[303]
Another related practice to renunciation and sense restraint taught by the Buddha is ""restraint in eating"" or moderation with food, which for monks generally means not eating after noon. Devout laypersons also follow this rule during special days of religious observance (uposatha).[304] Observing the Uposatha also includes other practices dealing with renunciation, mainly the eight precepts.
For Buddhist monastics, renunciation can also be trained through several optional ascetic practices called dhutaṅga.
In different Buddhist traditions, other related practices which focus on fasting are followed.
The training of the faculty called ""mindfulness"" (Pali: sati, Sanskrit: smṛti, literally meaning ""recollection, remembering"") is central in Buddhism. According to Analayo, mindfulness is a full awareness of the present moment which enhances and strengthens memory.[305] The Indian Buddhist philosopher Asanga defined mindfulness thus: ""It is non-forgetting by the mind with regard to the object experienced. Its function is non-distraction.""[306] According to Rupert Gethin, sati is also ""an awareness of things in relation to things, and hence an awareness of their relative value.""[307]
There are different practices and exercises for training mindfulness in the early discourses, such as the four Satipaṭṭhānas (Sanskrit: smṛtyupasthāna, ""establishments of mindfulness"") and Ānāpānasati (Sanskrit: ānāpānasmṛti, ""mindfulness of breathing"").
A closely related mental faculty, which is often mentioned side by side with mindfulness, is sampajañña (""clear comprehension""). This faculty is the ability to comprehend what one is doing and is happening in the mind, and whether it is being influenced by unwholesome states or wholesome ones.[308]
A wide range of meditation practices has developed in the Buddhist traditions, but ""meditation"" primarily refers to the attainment of samādhi and the practice of dhyāna (Pali: jhāna). Samādhi is a calm, undistracted, unified and concentrated state of consciousness. It is defined by Asanga as ""one-pointedness of mind on the object to be investigated. Its function consists of giving a basis to knowledge (jñāna).""[306]
Dhyāna is ""state of perfect equanimity and awareness (upekkhā-sati-parisuddhi),"" reached through focused mental training.[309]
The practice of dhyāna aids in maintaining a calm mind, and avoiding disturbance of this calm mind by mindfulness of disturbing thoughts and feelings.[310][note 25]
The earliest evidence of yogis and their meditative tradition, states Karel Werner, is found in the Keśin hymn 10.136 of the Rigveda.[311] While evidence suggests meditation was practised in the centuries preceding the Buddha,[312] the meditative methodologies described in the Buddhist texts are some of the earliest among texts that have survived into the modern era.[313][314] These methodologies likely incorporate what existed before the Buddha as well as those first developed within Buddhism.[315][note 26]
There is no scholarly agreement on the origin and source of the practice of dhyāna. Some scholars, like Bronkhorst, see the four dhyānas as a Buddhist invention.[319] Alexander Wynne argues that the Buddha learned dhyāna from brahmanical teachers.[320]
Whatever the case, the Buddha taught meditation with a new focus and interpretation, particularly through the four dhyānas methodology,[321] in which mindfulness is maintained.[322][323] Further, the focus of meditation and the underlying theory of liberation guiding the meditation has been different in Buddhism.[312][324][325] For example, states Bronkhorst, the verse 4.4.23 of the Brihadaranyaka Upanishad with its ""become calm, subdued, quiet, patiently enduring, concentrated, one sees soul in oneself"" is most probably a meditative state.[326] The Buddhist discussion of meditation is without the concept of soul and the discussion criticises both the ascetic meditation of Jainism and the ""real self, soul"" meditation of Hinduism.[327]
Buddhist texts teach various meditation schemas. One of the most prominent is that of the four rupa-jhānas (four meditations in the realm of form), which are ""stages of progressively deepening concentration"".[328] According to Gethin, they are states of ""perfect mindfulness, stillness and lucidity.""[329] They are described in the Pali Canon as trance-like states without desire.[330] In the early texts, the Buddha is depicted as entering jhāna both before his awakening under the bodhi tree and also before his final nirvana (see: the Mahāsaccaka-sutta and the Mahāparinibbāṇa Sutta).[331][332]
The four rupa-jhānas are:[328][333]
There is a wide variety of scholarly opinions (both from modern scholars and from traditional Buddhists) on the interpretation of these meditative states as well as varying opinions on how to practice them.[328][334]
Often grouped into the jhāna-scheme are four other meditative states, referred to in the early texts as arupa samāpattis (formless attainments). These are also referred to in commentarial literature as immaterial/formless jhānas (arūpajhānas). The first formless attainment is a place or realm of infinite space (ākāsānañcāyatana) without form or colour or shape. The second is termed the realm of infinite consciousness (viññāṇañcāyatana); the third is the realm of nothingness (ākiñcaññāyatana), while the fourth is the realm of ""neither perception nor non-perception"".[335] The four rupa-jhānas in Buddhist practice lead to rebirth in successfully better rupa Brahma heavenly realms, while arupa-jhānas lead into arupa heavens.[336][337]
In the Pali canon, the Buddha outlines two meditative qualities which are mutually supportive: samatha (Pāli; Sanskrit: śamatha; ""calm"") and vipassanā (Sanskrit: vipaśyanā, insight).[338] The Buddha compares these mental qualities to a ""swift pair of messengers"" who together help deliver the message of nibbana (SN 35.245).[339]
The various Buddhist traditions generally see Buddhist meditation as being divided into those two main types.[340][341] Samatha is also called ""calming meditation"", and focuses on stilling and concentrating the mind i.e. developing samadhi and the four dhyānas. According to Damien Keown, vipassanā meanwhile, focuses on ""the generation of penetrating and critical insight (paññā)"".[342]
There are numerous doctrinal positions and disagreements within the different Buddhist traditions regarding these qualities or forms of meditation. For example, in the Pali Four Ways to Arahantship Sutta (AN 4.170), it is said that one can develop calm and then insight, or insight and then calm, or both at the same time.[343] Meanwhile, in Vasubandhu's Abhidharmakośakārikā, vipaśyanā is said to be practiced once one has reached samadhi by cultivating the four foundations of mindfulness (smṛtyupasthānas).[344]
Beginning with comments by La Vallee Poussin, a series of scholars have argued that these two meditation types reflect a tension between two different ancient Buddhist traditions regarding the use of dhyāna, one which focused on insight based practice and the other which focused purely on dhyāna.[345][346] However, other scholars such as Analayo and Rupert Gethin have disagreed with this ""two paths"" thesis, instead seeing both of these practices as complementary.[346][347]
The four immeasurables or four abodes, also called Brahma-viharas, are virtues or directions for meditation in Buddhist traditions, which helps a person be reborn in the heavenly (Brahma) realm.[348][349][350] These are traditionally believed to be a characteristic of the deity Brahma and the heavenly abode he resides in.[351]
The four Brahma-vihara are:
According to Peter Harvey, the Buddhist scriptures acknowledge that the four Brahmavihara meditation practices ""did not originate within the Buddhist tradition"".[353][note 27] The Brahmavihara (sometimes as Brahmaloka), along with the tradition of meditation and the above four immeasurables are found in pre-Buddha and post-Buddha Vedic and Sramanic literature.[355][356] Aspects of the Brahmavihara practice for rebirths into the heavenly realm have been an important part of Buddhist meditation tradition.[357][358]
According to Gombrich, the Buddhist usage of the brahma-vihāra originally referred to an awakened state of mind, and a concrete attitude toward other beings which was equal to ""living with Brahman"" here and now. The later tradition took those descriptions too literally, linking them to cosmology and understanding them as ""living with Brahman"" by rebirth in the Brahma-world.[359] According to Gombrich, ""the Buddha taught that kindness – what Christians tend to call love – was a way to salvation.""[360]
Some Buddhist traditions, especially those associated with Tantric Buddhism (also known as Vajrayana and Secret Mantra) use images and symbols of deities and Buddhas in meditation. This is generally done by mentally visualizing a Buddha image (or some other mental image, like a symbol, a mandala, a syllable, etc.), and using that image to cultivate calm and insight. One may also visualize and identify oneself with the imagined deity.[361][362] While visualization practices have been particularly popular in Vajrayana, they may also found in Mahayana and Theravada traditions.[363]
In Tibetan Buddhism, unique tantric techniques which include visualization (but also mantra recitation, mandalas, and other elements) are considered to be much more effective than non-tantric meditations and they are one of the most popular meditation methods.[364] The methods of Unsurpassable Yoga Tantra, (anuttarayogatantra) are in turn seen as the highest and most advanced. Anuttarayoga practice is divided into two stages, the Generation Stage and the Completion Stage. In the Generation Stage, one meditates on emptiness and visualizes oneself as a deity as well as visualizing its mandala. The focus is on developing clear appearance and divine pride (the understanding that oneself and the deity are one).[365] This method is also known as deity yoga (devata yoga). There are numerous meditation deities (yidam) used, each with a mandala, a circular symbolic map used in meditation.[366]
In the Completion Stage, one meditates on ultimate reality based on the image that has been generated. Completion Stage practices also include techniques such as tummo and phowa. These are said to work with subtle body elements, like the energy channels (nadi), vital essences (bindu), ""vital winds"" (vayu), and chakras.[367] The subtle body energies are seen as influencing consciousness in powerful ways, and are thus used in order to generate the 'great bliss' (maha-sukha) which is used to attain the luminous nature of the mind and realization of the empty and illusory nature of all phenomena (""the illusory body""), which leads to enlightenment.[368][369]
Completion practices are often grouped into different systems, such as the six dharmas of Naropa, and the six yogas of Kalachakra. In Tibetan Buddhism, there are also practices and methods which are sometimes seen as being outside of the two tantric stages, mainly Mahamudra and Dzogchen (Atiyoga).
According to Peter Harvey, whenever Buddhism has been healthy, not only ordained but also more committed lay people have practised formal meditation.[370] Loud devotional chanting however, adds Harvey, has been the most prevalent Buddhist practice and considered a form of meditation that produces ""energy, joy, lovingkindness and calm"", purifies mind and benefits the chanter.[371]
Throughout most of Buddhist history, meditation has been primarily practised in Buddhist monastic tradition, and historical evidence suggests that serious meditation by lay people has been an exception.[372][373][374] In recent history, sustained meditation has been pursued by a minority of monks in Buddhist monasteries.[375] Western interest in meditation has led to a revival where ancient Buddhist ideas and precepts are adapted to Western mores and interpreted liberally, presenting Buddhism as a meditation-based form of spirituality.[375]
Prajñā (Sanskrit) or paññā (Pāli) is wisdom, or knowledge of the true nature of existence. Another term which is associated with prajñā and sometimes is equivalent to it is vipassanā (Pāli) or vipaśyanā (Sanskrit), which is often translated as ""insight"". In Buddhist texts, the faculty of insight is often said to be cultivated through the four establishments of mindfulness.[376]
In the early texts, Paññā is included as one of the ""five faculties"" (indriya) which are commonly listed as important spiritual elements to be cultivated (see for example: AN I 16). Paññā along with samadhi, is also listed as one of the ""trainings in the higher states of mind"" (adhicittasikkha).[376]
The Buddhist tradition regards ignorance (avidyā), a fundamental ignorance, misunderstanding or mis-perception of the nature of reality, as one of the basic causes of dukkha and samsara. Overcoming this ignorance is part of the path to awakening. This overcoming includes the contemplation of impermanence and the non-self nature of reality,[377][378] and this develops dispassion for the objects of clinging, and liberates a being from dukkha and saṃsāra.[379]</ref>[380][381]
Prajñā is important in all Buddhist traditions. It is variously described as wisdom regarding the impermanent and not-self nature of dharmas (phenomena), the functioning of karma and rebirth, and knowledge of dependent origination.[382] Likewise, vipaśyanā is described in a similar way, such as in the Paṭisambhidāmagga, where it is said to be the contemplation of things as impermanent, unsatisfactory and not-self.[383]
Some scholars such as Bronkhorst and Vetter have argued that the idea that insight leads to liberation was a later development in Buddhism and that there are inconsistencies with the early Buddhist presentation of samadhi and insight.[384][385][note 28] However, others such as Collett Cox and Damien Keown have argued that insight is a key aspect of the early Buddhist process of liberation, which cooperates with samadhi to remove the obstacles to enlightenment (i.e., the āsavas).[387][388]
In Theravāda Buddhism, the focus of vipassanā meditation is to continuously and thoroughly know how phenomena (dhammas) are impermanent (annica), not-Self (anatta) and dukkha.[389][390] The most widely used method in modern Theravāda for the practice of vipassanā is that found in the Satipatthana Sutta.[391] There is some disagreement in contemporary Theravāda regarding samatha and vipassanā. Some in the Vipassana Movement strongly emphasize the practice of insight over samatha, and other Theravadins disagree with this.[391]
In Mahāyāna Buddhism, the development of insight (vipaśyanā) and tranquility (śamatha) are also taught and practiced. The many different schools of Mahāyāna Buddhism have a large repertoire of meditation techniques to cultivate these qualities. These include visualization of various Buddhas, recitation of a Buddha's name, the use of tantric Buddhist mantras and dharanis.[392][393] Insight in Mahāyāna Buddhism also includes gaining a direct understanding of certain Mahāyāna philosophical views, such as the emptiness view and the consciousness-only view. This can be seen in meditation texts such as Kamalaśīla's Bhāvanākrama ( ""Stages of Meditation"", 9th century), which teaches insight (vipaśyanā) from the Yogācāra-Madhyamaka perspective.[394]
According to Harvey, most forms of Buddhism ""consider saddhā (Skt śraddhā), ‘trustful confidence’ or ‘faith’, as a quality which must be balanced by wisdom, and as a preparation for, or accompaniment of, meditation.""[395] Because of this devotion (Skt. bhakti; Pali: bhatti) is an important part of the practice of most Buddhists.[396] Devotional practices include ritual prayer, prostration, offerings, pilgrimage, and chanting.[397] Buddhist devotion is usually focused on some object, image or location that is seen as holy or spiritually influential. Examples of objects of devotion include paintings or statues of Buddhas and bodhisattvas, stupas, and bodhi trees.[398] Public group chanting for devotional and ceremonial is common to all Buddhist traditions and goes back to ancient India where chanting aided in the memorization of the orally transmitted teachings.[399] Rosaries called malas are used in all Buddhist traditions to count repeated chanting of common formulas or mantras. Chanting is thus a type of devotional group meditation which leads to tranquility and communicates the Buddhist teachings.[400]
In East Asian Pure Land Buddhism, devotion to the Buddha Amitabha is the main practice. In Nichiren Buddhism, devotion to the Lotus Sutra is the main practice. Devotional practices such as pujas have been a common practice in Theravada Buddhism, where offerings and group prayers are made to deities and particularly images of Buddha.[401] According to Karel Werner and other scholars, devotional worship has been a significant practice in Theravada Buddhism, and deep devotion is part of Buddhist traditions starting from the earliest days.[402][403]
Guru devotion is a central practice of Indo-Tibetan Buddhism.[404][405] The guru is considered essential and to the Buddhist devotee, the guru is the ""enlightened teacher and ritual master"" in Vajrayana spiritual pursuits.[404][406] For someone seeking Buddhahood, the guru is the Buddha, the Dharma and the Sangha, wrote the 12th-century Buddhist scholar Sadhanamala.[406]
The veneration of and obedience to teachers is also important in Theravada and Zen Buddhism.[407]
Based on the Indian principle of ahimsa (non-harming), the Buddha's ethics strongly condemn the harming of all sentient beings, including all animals.  He thus condemned the animal sacrifice of the brahmins as well hunting, and killing animals for food.[408] This led to various policies by Buddhist kings such as Asoka meant to protect animals, such as the establishing of 'no slaughter days' and the banning of hunting on certain circumstances.[409]
However, early Buddhist texts depict the Buddha as allowing monastics to eat meat. This seems to be because monastics begged for their food and thus were supposed to accept whatever food was offered to them.[410] This was tempered by the rule that meat had to be ""three times clean"" which meant that ""they had not seen, had not heard, and had no reason to suspect that the animal had been killed so that the meat could be given to them"".[411] Also, while the Buddha did not explicitly promote vegetarianism in his discourses, he did state that gaining one's livelihood from the meat trade was unethical.[412] However, this rule was not a promotion of a specific diet, but a rule against the actual killing of animals for food.[413] There was also a famed schism which occurred in the Buddhist community when Devadatta attempted to make vegetarianism compulsory and the Buddha disagreed.[411]
In contrast to this, various Mahayana sutras and texts like the Mahaparinirvana sutra, Surangama sutra and the Lankavatara sutra state that the Buddha promoted vegetarianism out of compassion.[414] Indian Mahayana thinkers like Shantideva promoted the avoidance of meat.[415] Throughout history, the issue of whether Buddhists should be vegetarian has remained a much debated topic and there is a variety of opinions on this issue among modern Buddhists.
In the East Asian Buddhism, most monastics are expected to be vegetarian, and the practice is seen as very virtuous and it is taken up by some devout laypersons. Most Theravadins in Sri Lanka and Southeast Asia do not practice vegetarianism and eat whatever is offered by the lay community, who are mostly also not vegetarians. But there are exceptions, some monks choose to be vegetarian and some abbots like Ajahn Sumedho have encouraged the lay community to donate vegetarian food to the monks.[416] Mahasi Sayadaw meanwhile, has recommended vegetarianism as the best way to make sure one's meal is pure in three ways.[417] Also, the new religious movement Santi Asoke, promotes vegetarianism. According to Peter Harvey, in the Theravada world, vegetarianism is ""universally admired, but little practiced.""[417] Because of the rule against killing, in many Buddhist countries, most butchers and others who work in the meat trade are non-Buddhists.[418]
Likewise, most Tibetan Buddhists have historically tended not to be vegetarian, however, there have been some strong debates and pro-vegetarian arguments by some pro-vegetarian Tibetans.[419] Some influential figures have spoken and written in favor of vegetarianism throughout history, including well known figures like Shabkar and the 17th Karmapa Ogyen Trinley Dorje, who has mandated vegetarianism in all his monasteries.[420]
Buddhism, like all Indian religions, was initially an oral tradition in ancient times.[421] The Buddha's words, the early doctrines, concepts, and their traditional interpretations were orally transmitted from one generation to the next. The earliest oral texts were transmitted in Middle Indo-Aryan languages called Prakrits, such as Pali, through the use of communal recitation and other mnemonic techniques.[422]
The first Buddhist canonical texts were likely written down in Sri Lanka, about 400 years after the Buddha died.[421] The texts were part of the Tripitakas, and many versions appeared thereafter claiming to be the words of the Buddha. Scholarly Buddhist commentary texts, with named authors, appeared in India, around the 2nd century CE.[421] These texts were written in Pali or Sanskrit, sometimes regional languages, as palm-leaf manuscripts, birch bark, painted scrolls, carved into temple walls, and later on paper.[421]
Unlike what the Bible is to Christianity and the Quran is to Islam, but like all major ancient Indian religions, there is no consensus among the different Buddhist traditions as to what constitutes the scriptures or a common canon in Buddhism.[421] The general belief among Buddhists is that the canonical corpus is vast.[423][424][425] This corpus includes the ancient Sutras organised into Nikayas or Agamas, itself the part of three basket of texts called the Tripitakas.[426] Each Buddhist tradition has its own collection of texts, much of which is translation of ancient Pali and Sanskrit Buddhist texts of India. The Chinese Buddhist canon, for example, includes 2184 texts in 55 volumes, while the Tibetan canon comprises 1108 texts – all claimed to have been spoken by the Buddha – and another 3461 texts composed by Indian scholars revered in the Tibetan tradition.[427] The Buddhist textual history is vast; over 40,000 manuscripts – mostly Buddhist, some non-Buddhist – were discovered in 1900 in the Dunhuang Chinese cave alone.[427]
The Early Buddhist Texts refers to the literature which is considered by modern scholars to be the earliest Buddhist material. The first four Pali Nikayas, and the corresponding Chinese Āgamas are generally considered to be among the earliest material.[428][429][430] Apart from these, there are also fragmentary collections of EBT materials in other languages such as Sanskrit, Khotanese, Tibetan and Gāndhārī. The modern study of early Buddhism often relies on comparative scholarship using these various early Buddhist sources to identify parallel texts and common doctrinal content.[431] One feature of these early texts are literary structures which reflect oral transmission, such as widespread repetition.[432]After the development of the different early Buddhist schools, these schools began to develop their own textual collections, which were termed Tripiṭakas (Triple Baskets).[433]
Many early Tripiṭakas, like the Pāli Tipitaka, were divided into three sections: Vinaya Pitaka (focuses on monastic rule), Sutta Pitaka (Buddhist discourses) and Abhidhamma Pitaka, which contain expositions and commentaries on the doctrine.
The Pāli Tipitaka (also known as the Pali Canon) of the Theravada School constitutes the only complete collection of Buddhist texts in an Indic language which has survived until today.[434] However, many Sutras, Vinayas and Abhidharma works from other schools survive in Chinese translation, as part of the Chinese Buddhist Canon. According to some sources, some early schools of Buddhism had five or seven pitakas.[435]
Much of the material in the Pali Canon is not specifically ""Theravadin"", but is instead the collection of teachings that this school preserved from the early, non-sectarian body of teachings. According to Peter Harvey, it contains material at odds with later Theravadin orthodoxy. He states: ""The Theravadins, then, may have added texts to the Canon for some time, but they do not appear to have tampered with what they already had from an earlier period.""[436]
A distinctive feature of many Tripitaka collections is the inclusion of a genre called Abhidharma, which dates from the 3rd century BCE and later. According to Collett Cox, the genre began as explanations and elaborations of the teachings in the suttas but over time evolved into an independent system of doctrinal exposition.[437]
Over time, the various Abhidharma traditions developed various disagreements which each other on points of doctrine, which were discussed in the different Abhidharma texts of these schools.[45] The major Abhidharma collections which modern scholars have the most information about are those of the Theravāda and Sarvāstivāda schools.[438]
In Sri Lanka and South India, the Theravāda Abhidhamma system was the most influential. In addition to the Abhidharma project, some of the schools also began accumulating a literary tradition of scriptural commentary on their respective Tripitakas. These commentaries were particularly important in the Theravāda school, and the Pali commentaries (Aṭṭhakathā) remain influential today. Both Abhidhamma and the Pali commentaries influenced the Visuddhimagga, an important 5th-century text by the Theravada scholar Buddhaghosa, who also translated and compiled many of the Aṭṭhakathās from older Sinhalese sources.[439][440]
The Sarvāstivāda school was one of the most influential Abhidharma traditions in North India.[441] The magnum opus of this tradition was the massive Abhidharma commentary called the Mahāvibhaṣa ('Great Commentary'), compiled at a great synod in Kashmir during the reign of Kanishka II (c. 158–176).[442] The Abhidharmakosha of Vasubandhu is another very influential Abhidharma work from the northern tradition, which continues to be studied in East Asian Buddhism and in Indo-Tibetan Buddhism.[443]
The Mahāyāna sūtras are a very broad genre of Buddhist scriptures that the Mahāyāna Buddhist tradition holds are original teachings of the Buddha. Modern historians generally hold that the first of these texts were composed probably around the 1st century BCE or 1st century CE.[444][445][446]
In Mahāyāna, these texts are generally given greater authority than the early Āgamas and Abhidharma literature, which are called ""Śrāvakayāna"" or ""Hinayana"" to distinguish them from Mahāyāna sūtras.[447] Mahāyāna traditions mainly see these different classes of texts as being designed for different types of persons, with different levels of spiritual understanding. The Mahāyāna sūtras are mainly seen as being for those of ""greater"" capacity.[448][better source needed]
The Mahāyāna sūtras often claim to articulate the Buddha's deeper, more advanced doctrines, reserved for those who follow the bodhisattva path. That path is explained as being built upon the motivation to liberate all living beings from unhappiness. Hence the name Mahāyāna (lit., the Great Vehicle). Besides the teaching of the bodhisattva, Mahāyāna texts also contain expanded cosmologies and mythologies, with many more Buddhas and powerful bodhisattvas, as well as new spiritual practices and ideas.[449]
The modern Theravada school does not treat the Mahāyāna sūtras as authoritative or authentic teachings of the Buddha.[450] Likewise, these texts were not recognized as authoritative by many early Buddhist schools and in some cases, communities such as the Mahāsāṃghika school split up due to this disagreement.[451]
Recent scholarship has discovered many early Mahāyāna texts which shed light into the development of Mahāyāna. Among these is the Śālistamba Sutra which survives in Tibetan and Chinese translation. This text contains numerous sections which are remarkably similar to Pali suttas.[452][453] The Śālistamba Sutra was cited by Mahāyāna scholars such as the 8th-century Yasomitra to be authoritative.[454] This suggests that Buddhist literature of different traditions shared a common core of Buddhist texts in the early centuries of its history, until Mahāyāna literature diverged about and after the 1st century CE.[452]
Mahāyāna also has a very large literature of philosophical and exegetical texts. These are often called śāstra (treatises) or vrittis (commentaries). Some of this literature was also written in verse form (karikās), the most famous of which is the Mūlamadhyamika-karikā (Root Verses on the Middle Way) by Nagarjuna, the foundational text of the Madhyamika school.
During the Gupta Empire, a new class of Buddhist sacred literature began to develop, which are called the Tantras.[455] By the 8th century, the tantric tradition was very influential in India and beyond. Besides drawing on a Mahāyāna Buddhist framework, these texts also borrowed deities and material from other Indian religious traditions, such as the Śaiva and Pancharatra traditions, local god/goddess cults, and local spirit worship (such as yaksha or nāga spirits).[456][457]
Some features of these texts include the widespread use of mantras, meditation on the subtle body, worship of fierce deities, and antinomian and transgressive practices such as ingesting alcohol and performing sexual rituals.[458][459][460]
Historically, the roots of Buddhism lie in the religious thought of Iron Age India around the middle of the first millennium BCE.[461] This was a period of great intellectual ferment and socio-cultural change known as the ""Second urbanisation"", marked by the growth of towns and trade, the composition of the Upanishads and the historical emergence of the Śramaṇa traditions.[462][463][note 29]
New ideas developed both in the Vedic tradition in the form of the Upanishads, and outside of the Vedic tradition through the Śramaṇa movements.[466][467][468] The term Śramaṇa refers to several Indian religious movements parallel to but separate from the historical Vedic religion, including Buddhism, Jainism and others such as Ājīvika.[469]
Several Śramaṇa movements are known to have existed in India before the 6th century BCE (pre-Buddha, pre-Mahavira), and these influenced both the āstika and nāstika traditions of Indian philosophy.[470] According to Martin Wilshire, the Śramaṇa tradition evolved in India over two phases, namely Paccekabuddha and Savaka phases, the former being the tradition of individual ascetic and the latter of disciples, and that Buddhism and Jainism ultimately emerged from these.[471] Brahmanical and non-Brahmanical ascetic groups shared and used several similar ideas,[472] but the Śramaṇa traditions also drew upon already established Brahmanical concepts and philosophical roots, states Wiltshire, to formulate their own doctrines.[470][473] Brahmanical motifs can be found in the oldest Buddhist texts, using them to introduce and explain Buddhist ideas.[474] For example, prior to Buddhist developments, the Brahmanical tradition internalised and variously reinterpreted the three Vedic sacrificial fires as concepts such as Truth, Rite, Tranquility or Restraint.[475] Buddhist texts also refer to the three Vedic sacrificial fires, reinterpreting and explaining them as ethical conduct.[476]
The Śramaṇa religions challenged and broke with the Brahmanic tradition on core assumptions such as Atman (soul, self), Brahman, the nature of afterlife, and they rejected the authority of the Vedas and Upanishads.[477][478][479] Buddhism was one among several Indian religions that did so.[479]
The history of Indian Buddhism may be divided into five periods:[480] Early Buddhism (occasionally called pre-sectarian Buddhism), Nikaya Buddhism or Sectarian Buddhism: The period of the early Buddhist schools, Early Mahayana Buddhism, Late Mahayana, and the era of Vajrayana or the ""Tantric Age"".
According to Lambert Schmithausen Pre-sectarian Buddhism is ""the canonical period prior to the development of different schools with their different positions.""[481]
The early Buddhist Texts include the four principal Pali Nikāyas [note 30] (and their parallel Agamas found in the Chinese canon) together with the main body of monastic rules, which survive in the various versions of the patimokkha.[482][483][484] However, these texts were revised over time, and it is unclear what constitutes the earliest layer of Buddhist teachings. One method to obtain information on the oldest core of Buddhism is to compare the oldest extant versions of the Theravadin Pāli Canon and other texts.[note 31] The reliability of the early sources, and the possibility to draw out a core of oldest teachings, is a matter of dispute.[487] According to Vetter, inconsistencies remain, and other methods must be applied to resolve those inconsistencies.[485][note 32]
According to Schmithausen, three positions held by scholars of Buddhism can be distinguished:[492]
According to Mitchell, certain basic teachings appear in many places throughout the early texts, which has led most scholars to conclude that Gautama Buddha must have taught something similar to the Four Noble Truths, the Noble Eightfold Path, Nirvana, the three marks of existence, the five aggregates, dependent origination, karma and rebirth.[499]
According to N. Ross Reat, all of these doctrines are shared by the Theravada Pali texts and the Mahasamghika school's Śālistamba Sūtra.[500] A recent study by Bhikkhu Analayo concludes that the Theravada Majjhima Nikaya and Sarvastivada Madhyama Agama contain mostly the same major doctrines.[501] Richard Salomon, in his study of the Gandharan texts (which are the earliest manuscripts containing early discourses), has confirmed that their teachings are ""consistent with non-Mahayana Buddhism, which survives today in the Theravada school of Sri Lanka and Southeast Asia, but which in ancient times was represented by eighteen separate schools.""[502]
However, some scholars argue that critical analysis reveals discrepancies among the various doctrines found in these early texts, which point to alternative possibilities for early Buddhism.[503][504][505] The authenticity of certain teachings and doctrines have been questioned. For example, some scholars think that karma was not central to the teaching of the historical Buddha, while other disagree with this position.[506][507] Likewise, there is scholarly disagreement on whether insight was seen as liberating in early Buddhism or whether it was a later addition to the practice of the four jhānas.[488][508][509] Scholars such as Bronkhorst also think that the four noble truths may not have been formulated in earliest Buddhism, and did not serve in earliest Buddhism as a description of ""liberating insight"".[510] According to Vetter, the description of the Buddhist path may initially have been as simple as the term ""the middle way"".[141] In time, this short description was elaborated, resulting in the description of the eightfold path.[141]
According to numerous Buddhist scriptures, soon after the parinirvāṇa (from Sanskrit: ""highest extinguishment"") of Gautama Buddha, the first Buddhist council was held to collectively recite the teachings to ensure that no errors occurred in oral transmission. Many modern scholars question the historicity of this event.[511] However, Richard Gombrich states that the monastic assembly recitations of the Buddha's teaching likely began during Buddha's lifetime, and they served a similar role of codifying the teachings.[512]
The so called Second Buddhist council resulted in the first schism in the Sangha. Modern scholars believe that this was probably caused when a group of reformists called Sthaviras (""elders"") sought to modify the Vinaya (monastic rule), and this caused a split with the conservatives who rejected this change, they were called Mahāsāṃghikas.[513][514] While most scholars accept that this happened at some point, there is no agreement on the dating, especially if it dates to before or after the reign of Ashoka.[515]
Buddhism may have spread only slowly throughout India until the time of the Mauryan emperor Ashoka (304–232 BCE), who was a public supporter of the religion. The support of Aśoka and his descendants led to the construction of more stūpas (such as at Sanchi and Bharhut), temples (such as the Mahabodhi Temple) and to its spread throughout the Maurya Empire and into neighbouring lands such as Central Asia and to the island of Sri Lanka.
During and after the Mauryan period (322–180 BCE), the Sthavira community gave rise to several schools, one of which was the Theravada school which tended to congregate in the south and another which was the Sarvāstivāda school, which was mainly in north India. Likewise, the Mahāsāṃghika groups also eventually split into different Sanghas. Originally, these schisms were caused by disputes over monastic disciplinary codes of various fraternities, but eventually, by about 100 CE if not earlier, schisms were being caused by doctrinal disagreements too.[516]
Following (or leading up to) the schisms, each Saṅgha started to accumulate their own version of Tripiṭaka (triple basket of texts).[47][517] In their Tripiṭaka, each school included the Suttas of the Buddha, a Vinaya basket (disciplinary code) and some schools also added an Abhidharma basket which were texts on detailed scholastic classification, summary and interpretation of the Suttas.[47][518] The doctrine details in the Abhidharmas of various Buddhist schools differ significantly, and these were composed starting about the third century BCE and through the 1st millennium CE.[519][520][521]
According to the edicts of Aśoka, the Mauryan emperor sent emissaries to various countries west of India to spread ""Dharma"", particularly in eastern provinces of the neighbouring Seleucid Empire, and even farther to Hellenistic kingdoms of the Mediterranean. It is a matter of disagreement among scholars whether or not these emissaries were accompanied by Buddhist missionaries.[522]
In central and west Asia, Buddhist influence grew, through Greek-speaking Buddhist monarchs and ancient Asian trade routes, a phenomenon known as Greco-Buddhism. An example of this is evidenced in Chinese and Pali Buddhist records, such as Milindapanha and the Greco-Buddhist art of Gandhāra. The Milindapanha describes a conversation between a Buddhist monk and the 2nd-century BCE Greek king Menander, after which Menander abdicates and himself goes into monastic life in the pursuit of nirvana.[523][524] Some scholars have questioned the Milindapanha version, expressing doubts whether Menander was Buddhist or just favourably disposed to Buddhist monks.[525]
The Kushan empire (30–375 CE) came to control the Silk Road trade through Central and South Asia, which brought them to interact with Gandharan Buddhism and the Buddhist institutions of these regions. The Kushans patronised Buddhism throughout their lands, and many Buddhist centers were built or renovated (the Sarvastivada school was particularly favored), especially by Emperor Kanishka (128–151 CE).[526][527] Kushan support helped Buddhism to expand into a world religion through their trade routes.[528] Buddhism spread to Khotan, the Tarim Basin, and China, eventually to other parts of the far east.[527] Some of the earliest written documents of the Buddhist faith are the Gandharan Buddhist texts, dating from about the 1st century CE, and connected to the Dharmaguptaka school.[529][530][531]
The Islamic conquest of the Iranian Plateau in the 7th-century, followed by the Muslim conquests of Afghanistan and the later establishment of the Ghaznavid kingdom with Islam as the state religion in Central Asia between the 10th- and 12th-century led to the decline and disappearance of Buddhism from most of these regions.[532]
The origins of Mahāyāna (""Great Vehicle"") Buddhism are not well understood and there are various competing theories about how and where this movement arose. Theories include the idea that it began as various groups venerating certain texts or that it arose as a strict forest ascetic movement.[533]
The first Mahāyāna works were written sometime between the 1st century BCE and the 2nd century CE.[445][533] Much of the early extant evidence for the origins of Mahāyāna comes from early Chinese translations of Mahāyāna texts, mainly those of Lokakṣema. (2nd century CE).[note 36] Some scholars have traditionally considered the earliest Mahāyāna sūtras to include the first versions of the Prajnaparamita series, along with texts concerning Akṣobhya, which were probably composed in the 1st century BCE in the south of India.[535][note 37]
There is no evidence that Mahāyāna ever referred to a separate formal school or sect of Buddhism, with a separate monastic code (Vinaya), but rather that it existed as a certain set of ideals, and later doctrines, for bodhisattvas.[537][538] Records written by Chinese monks visiting India indicate that both Mahāyāna and non-Mahāyāna monks could be found in the same monasteries, with the difference that Mahāyāna monks worshipped figures of Bodhisattvas, while non-Mahayana monks did not.[539]
Mahāyāna initially seems to have remained a small minority movement that was in tension with other Buddhist groups, struggling for wider acceptance.[540] However, during the fifth and sixth centuries CE, there seems to have been a rapid growth of Mahāyāna Buddhism, which is shown by a large increase in epigraphic and manuscript evidence in this period. However, it still remained a minority in comparison to other Buddhist schools.[541]
Mahāyāna Buddhist institutions continued to grow in influence during the following centuries, with large monastic university complexes such as Nalanda (established by the 5th-century CE Gupta emperor, Kumaragupta I) and Vikramashila (established under Dharmapala c. 783 to 820) becoming quite powerful and influential. During this period of Late Mahāyāna, four major types of thought developed: Mādhyamaka, Yogācāra, Buddha-nature (Tathāgatagarbha), and the epistemological tradition of Dignaga and Dharmakirti.[542] According to Dan Lusthaus, Mādhyamaka and Yogācāra have a great deal in common, and the commonality stems from early Buddhism.[543]
During the Gupta period (4th–6th centuries) and the empire of Harṣavardana (c. 590–647 CE), Buddhism continued to be influential in India, and large Buddhist learning institutions such as Nalanda and Valabahi Universities were at their peak.[544] Buddhism also flourished under the support of the Pāla Empire (8th–12th centuries). Under the Guptas and Palas, Tantric Buddhism or Vajrayana developed and rose to prominence. It promoted new practices such as the use of mantras, dharanis, mudras, mandalas and the visualization of deities and Buddhas and developed a new class of literature, the Buddhist Tantras. This new esoteric form of Buddhism can be traced back to groups of wandering yogi magicians called mahasiddhas.[545][546]
The question of the origins of early Vajrayana has been taken up by various scholars. David Seyfort Ruegg has suggested that Buddhist tantra employed various elements of a ""pan-Indian religious substrate"" which is not specifically Buddhist, Shaiva or Vaishnava.[547]
According to Indologist Alexis Sanderson, various classes of Vajrayana literature developed as a result of royal courts sponsoring both Buddhism and Saivism. Sanderson has argued that Buddhist tantras can be shown to have borrowed practices, terms, rituals and more form Shaiva tantras. He argues that Buddhist texts even directly copied various Shaiva tantras, especially the Bhairava Vidyapitha tantras.[548][549] Ronald M. Davidson meanwhile, argues that Sanderson's claims for direct influence from Shaiva Vidyapitha texts are problematic because ""the chronology of the Vidyapitha tantras is by no means so well established""[550] and that the Shaiva tradition also appropriated non-Hindu deities, texts and traditions. Thus while ""there can be no question that the Buddhist tantras were heavily influenced by Kapalika and other Saiva movements"" argues Davidson, ""the influence was apparently mutual.""[551]
Already during this later era, Buddhism was losing state support in other regions of India, including the lands of the Karkotas, the Pratiharas, the Rashtrakutas, the Pandyas and the Pallavas. This loss of support in favor of Hindu faiths like Vaishnavism and Shaivism, is the beginning of the long and complex period of the Decline of Buddhism in the Indian subcontinent.[552] The Islamic invasions and conquest of India (10th to 12th century), further damaged and destroyed many Buddhist institutions, leading to its eventual near disappearance from India by the 1200s.[553]
The Silk Road transmission of Buddhism to China is most commonly thought to have started in the late 2nd or the 1st century CE, though the literary sources are all open to question.[554][note 38] The first documented translation efforts by foreign Buddhist monks in China were in the 2nd century CE, probably as a consequence of the expansion of the Kushan Empire into the Chinese territory of the Tarim Basin.[556]
The first documented Buddhist texts translated into Chinese are those of the Parthian An Shigao (148–180 CE).[557] The first known Mahāyāna scriptural texts are translations into Chinese by the Kushan monk Lokakṣema in Luoyang, between 178 and 189 CE.[558] From China, Buddhism was introduced into its neighbours Korea (4th century), Japan (6th–7th centuries), and Vietnam (c. 1st–2nd centuries).[559][560]
During the Chinese Tang dynasty (618–907), Chinese Esoteric Buddhism was introduced from India and Chan Buddhism (Zen) became a major religion.[561][562] Chan continued to grow in the Song dynasty (960–1279) and it was during this era that it strongly influenced Korean Buddhism and Japanese Buddhism.[563] Pure Land Buddhism also became popular during this period and was often practised together with Chan.[564] It was also during the Song that the entire Chinese canon was printed using over 130,000 wooden printing blocks.[565]
During the Indian period of Esoteric Buddhism (from the 8th century onwards), Buddhism spread from India to Tibet and Mongolia. Johannes Bronkhorst states that the esoteric form was attractive because it allowed both a secluded monastic community as well as the social rites and rituals important to laypersons and to kings for the maintenance of a political state during succession and wars to resist invasion.[566] During the Middle Ages, Buddhism slowly declined in India,[567] while it vanished from Persia and Central Asia as Islam became the state religion.[568][569]
The Theravada school arrived in Sri Lanka sometime in the 3rd century BCE. Sri Lanka became a base for its later spread to southeast Asia after the 5th century CE (Myanmar, Malaysia, Indonesia, Thailand, Cambodia and coastal Vietnam).[570][571] Theravada Buddhism was the dominant religion in Burma during the Mon Hanthawaddy Kingdom (1287–1552).[572] It also became dominant in the Khmer Empire during the 13th and 14th centuries and in the Thai Sukhothai Kingdom during the reign of Ram Khamhaeng (1237/1247–1298).[573][574]
Buddhists generally classify themselves as either Theravāda or Mahāyāna.[575] This classification is also used by some scholars[576] and is the one ordinarily used in the English language.[web 8] An alternative scheme used by some scholars divides Buddhism into the following three traditions or geographical or cultural areas: Theravāda (or ""Southern Buddhism"", ""South Asian Buddhism""), East Asian Buddhism (or just ""Eastern Buddhism"") and Indo-Tibetan Buddhism (or ""Northern Buddhism"").[note 39]
Some scholars[note 40] use other schemes. Buddhists themselves have a variety of other schemes. Hinayana (literally ""lesser or inferior vehicle"") is sometimes used by Mahāyāna followers to name the family of early philosophical schools and traditions from which contemporary Theravāda emerged, but as the Hinayana term is considered derogatory, a variety of other terms are used instead, including: Śrāvakayāna, Nikaya Buddhism, early Buddhist schools, sectarian Buddhism and conservative Buddhism.[577][578]
Not all traditions of Buddhism share the same philosophical outlook, or treat the same concepts as central. Each tradition, however, does have its own core concepts, and some comparisons can be drawn between them:[579][580]
The Theravāda tradition bases itself on the Pāli Canon, considers itself to be the more orthodox form of Buddhism and tends to be more conservative in doctrine and monastic discipline.[585][586][587] The Pāli Canon is the only complete Buddhist canon surviving in an ancient Indian language. This language, Pāli, serves as the school's sacred language and lingua franca.[588] Besides the Pāli Canon, Theravāda scholastics also often rely on a post-canonical Pāli literature which comments on and interprets the Pāli Canon. These later works such as the Visuddhimagga, a doctrinal summa written in the fifth century by the exegete Buddhaghosa also remain influential today.[589]
Theravāda derives from the Mahāvihāra (Tāmraparṇīya) sect, a Sri Lankan branch of the Vibhajyavāda Sthaviras, which began to establish itself on the island from the 3rd century BCE onwards.
Theravāda flourished in south India and Sri Lanka in ancient times; from there it spread for the first time into mainland southeast Asia about the 11th century into its elite urban centres.[590] By the 13th century, Theravāda had spread widely into the rural areas of mainland southeast Asia,[590] displacing Mahayana Buddhism and some traditions of Hinduism.[591][592][593]
In the modern era, Buddhist figures such as Anagarika Dhammapala and King Mongkut sought to re-focus the tradition on the Pāli Canon, as well as emphasize the rational and ""scientific"" nature of Theravāda while also opposing ""superstition"".[594] This movement, often termed Buddhist modernism, has influenced most forms of modern Theravāda. Another influential modern turn in Theravāda is the Vipassana Movement, which led to the widespread adoption of meditation by laypersons.
Theravāda is primarily practised today in Sri Lanka, Burma, Laos, Thailand, Cambodia as well as small portions of China, Vietnam, Malaysia and Bangladesh. It has a growing presence in the west, especially as part of the Vipassana Movement.
Mahāyāna (""Great Vehicle"") refers to all forms of Buddhism which consider the Mahāyāna Sutras as authoritative scriptures and accurate rendering of Buddha's words.[452] These traditions have been the more liberal form of Buddhism allowing different and new interpretations that emerged over time.[595] The focus of Mahāyāna is the path of the bodhisattva (bodhisattvayāna), though what this path means is interpreted in many different ways.
The first Mahāyāna texts date to sometime between the 1st century BCE and the 2st century CE. It remained a minority movement until the time of the Guptas and Palas, when great Mahāyāna monastic centres of learning such as Nālandā University were established as evidenced by records left by three Chinese visitors to India.[596][597] These universities supported Buddhist scholarship, as well as studies into non-Buddhist traditions and secular subjects such as medicine. They hosted visiting students who then spread Buddhism to East and Central Asia.[596][598]
Native Mahāyāna Buddhism is practised today in China, Japan, Korea, Singapore, parts of Russia and most of Vietnam (also commonly referred to as ""Eastern Buddhism""). The Buddhism practised in Tibet, the Himalayan regions, and Mongolia is also a form of Mahāyāna, but is also different in many ways due to its adoption of tantric practices and is discussed below under the heading of ""Vajrayāna"" (also commonly referred to as ""Northern Buddhism""). 
There are a variety of strands in Eastern Buddhism, of which ""the Pure Land school of Mahāyāna is the most widely practised today.""[599] In most of China, these different strands and traditions are generally fused together. Vietnamese Mahāyāna is similarly very eclectic. In Japan in particular, they form separate denominations with the five major ones being: Nichiren, peculiar to Japan; Pure Land; Shingon, a form of Vajrayana; Tendai, and Zen. In Korea, nearly all Buddhists belong to the Chogye school, which is officially Son (Zen), but with substantial elements from other traditions.[600]
The goal and philosophy of the Vajrayāna remains Mahāyānist, but its methods are seen by its followers as far more powerful, so as to lead to Buddhahood in just one lifetime.[601] The practice of using mantras was adopted from Hinduism, where they were first used in the Vedas.[602]
Tibetan Buddhism preserves the Vajrayana teachings of eighth-century India.[13] Tantric Buddhism is largely concerned with ritual and meditative practices.[603] A central feature of Buddhist Tantra is deity yoga which includes visualisation and identification with an enlightened yidam or meditation deity and its associated mandala. Another element of Tantra is the need for ritual initiation or empowerment (abhiṣeka) by a Guru or Lama.[604] Some Tantras like the Guhyasamāja Tantra features new forms of antinomian ritual practice such as the use taboo substances like alcohol, sexual yoga, and charnel ground practices which evoke wrathful deities.[605][606]
Buddhist institutions are often housed and centered around monasteries (Sanskrit:viharas) and temples. Buddhist monastics originally followed a life of wandering, never staying in one place for long. During the three month rainy season (vassa) they would gather together in one place for a period of intense practice and then depart again.[607][608] Some of the earliest Buddhist monasteries were at groves (vanas) or woods (araññas), such as Jetavana and Sarnath's Deer Park. There originally seems to have been two main types of monasteries, monastic settlements (sangharamas) were built and supported by donors, and woodland camps  (avasas) were set up by monks. Whatever structures were built in these locales were made out of wood and were sometimes temporary structures built for the rainy season.[609][610]
Over time, the wandering community slowly adopted more settled cenobitic forms of monasticism.[611] Also, these monasteries slowly evolved from the simpler collections of rustic dwellings of early Buddhism into larger more permanent structures meant to house the entire community, who now lived in a more collective fashion.[612] During the Gupta era, even larger monastic university complexes (like Nalanda) arose, with larger and more artistically ornate structures, as well as large land grants and accumulated wealth.[613]
There are many different forms of Buddhist structures. Classic Indian Buddhist institutions mainly made use of the following structures: monasteries, rock-hewn cave complexes (such as the Ajanta Caves), stupas (funerary mounds which contained relics), and temples such as the Mahabodhi Temple.[614]
In Southeast Asia, the most widespread institutions are centered on wats, which refers to an establishment with various buildings such as an ordination hall, a library, monks' quarters and stupas. East Asian Buddhist institutions also use various structures including monastic halls, temples, lecture halls, bell towers and pagodas. In Japanese Buddhist temples, these different structures are usually grouped together in an area termed the garan. In Indo-Tibetan Buddhism, Buddhist institutions are generally housed in gompas. They include monastic quarters, stupas and prayer halls with Buddha images.
The complexity of Buddhist institutions varies, ranging from minimalist and rustic forest monasteries to large monastic centers like Tawang Monastery. The core of traditional Buddhist institutions is the monastic community (Sangha) who manage and lead religious services. They are supported by the lay community who visit temples and monasteries for religious services and holidays.
In the modern era, the Buddhist ""meditation centre"", which is mostly used by laypersons and often also staffed by them, has also become widespread.[615]
Buddhism has faced various challenges and changes during the colonisation of Buddhist states by Christian countries and its persecution under modern states. Like other religions, the findings of modern science has challenged its basic premises. One response to some of these challenges has come to be called Buddhist modernism. Early Buddhist modernist figures such as the American convert Henry Olcott (1832–1907) and Anagarika Dharmapala  (1864–1933) reinterpreted and promoted Buddhism as a scientific and rational religion which they saw as compatible with modern science.[616]
East Asian Buddhism meanwhile suffered under various wars which ravaged China during the modern era, such as the Taiping rebellion and World War II (which also affected Korean Buddhism). During the Republican period (1912–49), a new movement called Humanistic Buddhism was developed by figures such as Taixu (1899–1947), and though Buddhist institutions were destroyed during the Cultural Revolution (1966–76), there has been a revival of the religion in China after 1977.[617] Japanese Buddhism also went through a period of modernisation during the Meiji period.[618] In Central Asia meanwhile, the arrival of Communist repression to Tibet (1966–1980) and Mongolia  (between 1924–1990) had a strong negative impact on Buddhist institutions, though the situation has improved somewhat since the 80s and 90s.[619]
While there were some encounters of Western travellers or missionaries such as St. Francis Xavier and Ippolito Desideri with Buddhist cultures, it was not until the 19th century that Buddhism began to be studied by Western scholars. It was the work of pioneering scholars such as Eugène Burnouf, Max Müller, Hermann Oldenberg and Thomas William Rhys Davids that paved the way for modern Buddhist studies in the West. The English words such as Buddhism, ""Boudhist"", ""Bauddhist"" and Buddhist were coined in the early 19th-century in the West,[620] while in 1881, Rhys Davids founded the Pali Text Society – an influential Western resource of Buddhist literature in the Pali language and one of the earliest publisher of a journal on Buddhist studies.[621] It was also during the 19th century that Asian Buddhist immigrants (mainly from China and Japan) began to arrive in Western countries such as the United States and Canada, bringing with them their Buddhist religion. This period also saw the first Westerners to formally convert to Buddhism, such as Helena Blavatsky and Henry Steel Olcott.[622] An important event in the introduction of Buddhism to the West was the 1893 World Parliament of Religions, which for the first time saw well-publicized speeches by major Buddhist leaders alongside other religious leaders.
The 20th century saw a prolific growth of new Buddhist institutions in Western countries, including the Buddhist Society, London (1924), Das Buddhistische Haus (1924) and Datsan Gunzechoinei in St Petersburg. The publication and translations of Buddhist literature in Western languages thereafter accelerated. After the second world war, further immigration from Asia, globalisation, the secularisation on Western culture as well a renewed interest in Buddhism among the 60s counterculture led to further growth in Buddhist institutions.[623] Influential figures on post-war Western Buddhism include Shunryu Suzuki, Jack Kerouac, Alan Watts, Thích Nhất Hạnh, and the 14th Dalai Lama. While Buddhist institutions have grown, some of the central premises of Buddhism such as the cycles of rebirth and Four Noble Truths have been problematic in the West.[624][625][626] In contrast, states Christopher Gowans, for ""most ordinary [Asian] Buddhists, today as well as in the past, their basic moral orientation is governed by belief in karma and rebirth"".[627] Most Asian Buddhist laypersons, states Kevin Trainor, have historically pursued Buddhist rituals and practices seeking better rebirth,[628] not nirvana or freedom from rebirth.[629]
Buddhism has spread across the world,[631][632] and Buddhist texts are increasingly translated into local languages. While Buddhism in the West is often seen as exotic and progressive, in the East it is regarded as familiar and traditional. In countries such as Cambodia and Bhutan, it is recognised as the state religion and receives government support.
In certain regions such as Afghanistan and Pakistan, militants have targeted violence and destruction of historic Buddhist monuments.[633][634]
A number of modern movements in Buddhism emerged during the second half of the 20th century.[635][636] These new forms of Buddhism are diverse and significantly depart from traditional beliefs and practices.[637]
In India, B.R. Ambedkar launched the Navayana tradition – literally, ""new vehicle"". Ambedkar's Buddhism rejects the foundational doctrines and historic practices of traditional Theravada and Mahayana traditions, such as monk lifestyle after renunciation, karma, rebirth, samsara, meditation, nirvana, Four Noble Truths and others.[638][639][640] Ambedkar's Navayana Buddhism considers these as superstitions and re-interprets the original Buddha as someone who taught about class struggle and social equality.[641][642] Ambedkar urged low caste Indian Dalits to convert to his Marxism-inspired[640] reinterpretation called the Navayana Buddhism, also known as Bhimayana Buddhism. Ambedkar's effort led to the expansion of Navayana Buddhism in India.[643][641]
The Thai King Mongkut (r. 1851–68), and his son King Chulalongkorn (r. 1868–1910), were responsible for modern reforms of Thai Buddhism.[644] Modern Buddhist movements include Secular Buddhism in many countries, Won Buddhism in Korea, the Dhammakaya movement in Thailand and several Japanese organisations, such as Shinnyo-en, Risshō Kōsei Kai or Soka Gakkai.
Some of these movements have brought internal disputes and strife within regional Buddhist communities. For example, the Dhammakaya movement in Thailand teaches a ""true self"" doctrine, which traditional Theravada monks consider as heretically denying the fundamental anatta (not-self) doctrine of Buddhism.[645][646][647]
Buddhism has not been immune from sexual abuse and misconduct scandals, with victims coming forward in various buddhist schools such as Zen and Tibetan.[648][649][650][651] “There are huge cover ups in the Catholic church, but what has happened within Tibetan Buddhism is totally along the same lines,” says Mary Finnigan, an author and journalist who has been chronicling such alleged abuses since the mid-80s. [652] One notably covered case in media of various Western country was that of Sogyal Rinpoche which began in 1994,[653] and end up by his retirement from his position as Rigpa's spiritual director in 2017.[654]
Buddhism has had a profound influence on various cultures, especially in Asia. Buddhist philosophy, Buddhist art, Buddhist architecture, Buddhist cuisine and Buddhist festivals continue to be influential elements of the modern Culture of Asia, especially in East Asia and the Sinosphere as well as in Southeast Asia and the Indosphere. According to Litian Fang, Buddhism has ""permeated a wide range of fields, such as politics, ethics, philosophy, literature, art and customs,"" in these Asian regions.[655]
Buddhist teachings influenced the development of modern Hinduism as well as other Asian religions like Taoism and Confucianism. For example, various scholars have argued that key Hindu thinkers such as Adi Shankara and Patanjali, author of the Yoga sutras, were influenced by Buddhist ideas.[656][657] Likewise, Buddhist practices were influential in the early development of Indian Yoga.[658]
Buddhist philosophers like Dignaga were very influential in the development of Indian logic and epistemology.[659] Buddhist educational institutions like Nalanda and Vikramashila preserved various disciplines of classical Indian knowledge such as Grammar and Medicine and taught foreign students from China.[660]
In an effort to preserve their sacred scriptures, Buddhist institutions such as temples and monasteries housed schools which educated the populace and promoted writing and literacy. This led to high levels of literacy among some traditional Buddhist societies such as Burma. According to David Steinberg, ""Early British observers claimed that Burma was the most literate state between Suez and Japan, and one British traveler in the early nineteenth century believed that Burmese women had a higher percentage of literacy than British women.""[661]
Buddhist institutions were also at the forefront of the adoption of Chinese technologies related to bookmaking, including paper, and block printing which Buddhists sometimes deployed on a large scale. The first surviving example of a printed text is a Buddhist charm, the first full printed book is the Buddhist Diamond Sutra (c. 868) and the first hand colored print is an illustration of Guanyin dated to 947.[662]
Buddhists were also influential in the study and practice of traditional forms of Indian medicine. Buddhists spread these traditional approaches to health, sometimes called ""Buddhist medicine"", throughout East and Southeast Asia, where they remain influential today in regions like Sri Lanka, Burma, Tibet and Thailand.[663]
In the Western world, Buddhism has had a strong influence on modern New Age spirituality and other alternative spiritualities. This began with its influence on 20th century Theosophists such as Helena Blavatsky, which were some of the first Westerners to take Buddhism seriously as a spiritual tradition.[664]
More recently, Buddhist meditation practices have influenced the development of modern psychology, particularly the practice of Mindfulness-based stress reduction (MBSR) and other similar mindfulness based modalities.[665][666] The influence of Buddhism on psychology can also be seen in certain forms of modern psychoanalysis.[667][668]
Buddhism also influenced the modern avant-garde movements during the 1950s and 60s through people like D. T. Suzuki and his influence on figures like Jack Kerouac and Allen Ginsberg.[669]
Shamanism is a widespread practice in Buddhist societies. Buddhist monasteries have long existed alongside local shamanic traditions. Lacking an institutional orthodoxy, Buddhists adapted to the local cultures, blending their own traditions with pre-existing shamanic culture. There was very little conflict between the sects, mostly limited to the shamanic practice of animal sacrifice, which Buddhists see as equivalent to killing one's parents. However, Buddhism requires acceptance of Buddha as the greatest being in the cosmos, and local shamanic traditions were bestowed an inferior status.[670]
Research into Himalayan religion has shown that Buddhist and shamanic traditions overlap in many respects: the worship of localized deities, healing rituals and exorcisms. The shamanic Gurung people have adopted some of the Buddhist beliefs such and rebirth but maintain the shamanic rites of ""guiding the soul"" after death. Geoffrey Samuel describes Shamanic Buddhism: ""Vajrayana Buddhism as practiced in Tibet may be described as shamanic, in that it is centered around communication with an alternative mode of reality via the alternative states of consciousness of Tantric Yoga"".[670]
Buddhism is practised by an estimated 488 million,[5] 495 million,[671] or 535 million[672] people as of the 2010s, representing 7% to 8% of the world's total population.
China is the country with the largest population of Buddhists, approximately 244 million or 18% of its total population.[5][note 41] They are mostly followers of Chinese schools of Mahayana, making this the largest body of Buddhist traditions. Mahayana, also practised in broader East Asia, is followed by over half of world Buddhists.[5]
According to a demographic analysis reported by Peter Harvey:[672] Mahayana has 360 million adherents; Theravada has 150 million adherents; and Vajrayana has 18 million adherents.
According to Johnson & Grim (2013), Buddhism has grown from a total of 138 million adherents in 1910, of which 137 million were in Asia, to 495 million in 2010, of which 487 million are in Asia.[671] Over 98% of all Buddhists live in the Asia-Pacific and South Asia region.[674] North America had about 3.9 million Buddhists, Europe 1.3 million, while South America, Africa and the Middle East had an estimated combined total of about 1 million Buddhists in 2010.[674]
Buddhism is the dominant religion in Bhutan,[675] Myanmar,[675] Cambodia,[675] Tibet,[675] Laos,[675] Mongolia,[675] Sri Lanka[675] and Thailand.[5][675] Large Buddhist populations live in China (18%),[675] Japan (36%),[675] Taiwan (35%),[675] Macau (17%),[675] North Korea (14%),[675] Nepal (11%),[675] Vietnam (10%),[675] Singapore (33%),[675] Hong Kong (15%)[675] and South Korea (23%).[675]
In Russia, Buddhists form majority in Tuva (52%) and Kalmykia (53%). Buryatia (20%) and Zabaykalsky Krai (15%) also have significant Buddhist populations.[676]
Buddhism is also growing by conversion. In United States, only about a third (32%) of Buddhists in the United States are Asian; a majority (53%) are white. Buddhism in the America is primarily made up of native-born adherents, whites and converts.[677] In New Zealand, about 25–35% of the total Buddhists are converts to Buddhism.[678][679]
The 10 countries with the largest Buddhist population densities are:[674]
Subnotes
"
Taliban - Wikipedia," 
Non-state allies
The Taliban (Pashto: طالبان‎, romanized: ṭālibān, lit. 'students') or Taleban, who refer to themselves as the Islamic Emirate of Afghanistan (IEA),[44] are a Sunni Islamic fundamentalist political movement and military organisation in Afghanistan currently waging war (an insurgency, or jihad) within that country.[45][46] Since 2016, the Taliban's leader is Mawlawi Hibatullah Akhundzada.[47][48][49]
From 1996 to 2001, the Taliban held power over roughly three quarters of Afghanistan, and enforced a strict interpretation of Sharia, or Islamic law.[50] The Taliban emerged in 1994 as one of the prominent factions in the Afghan Civil War[51] and largely consisted of students (talib) from the Pashtun areas of eastern and southern Afghanistan who had been educated in traditional Islamic schools, and fought during the Soviet–Afghan War.[52][53][54][55] Under the leadership of Mohammed Omar, the movement spread throughout most of Afghanistan, sequestering power from the Mujahideen warlords. The totalitarian[56][57][58] Islamic Emirate of Afghanistan was established in 1996 and the Afghan capital was transferred to Kandahar. It held control of most of the country until being overthrown after the American-led invasion of Afghanistan in December 2001 following the September 11 attacks. At its peak, formal diplomatic recognition of the Taliban's government was acknowledged by only three nations: Pakistan, Saudi Arabia, and the United Arab Emirates. The group later regrouped as an insurgency movement to fight the American-backed Karzai administration and the NATO-led International Security Assistance Force (ISAF) in the War in Afghanistan.
The Taliban have been condemned internationally for the harsh enforcement of their interpretation of Islamic Sharia law, which has resulted in the brutal treatment of many Afghans, especially women.[59][60]  During their rule from 1996 to 2001, the Taliban and their allies committed massacres against Afghan civilians, denied UN food supplies to 160,000 starving civilians and conducted a policy of scorched earth, burning vast areas of fertile land and destroying tens of thousands of homes.[61][62][63][64][65][66] During their rule they banned hobbies and activities such as kite flying and keeping birds as pets,[67] and discriminatorily targeted many ethnic minorities, including Shiite Muslims,[68] while their enforcement of identifiable badges on Hindus was likened to Nazi Germany's treatment of Jews.[69] According to the United Nations, the Taliban and their allies were responsible for 76% of Afghan civilian casualties in 2010, 80% in 2011, and 80% in 2012.[70][71][72][73][74][75] The Taliban has also engaged in cultural genocide, destroying numerous monuments including the famous 1500-year old Buddhas of Bamiyan.[76][77][78][79]
The Taliban's ideology has been described as combining an ""innovative"" form of sharia Islamic law based on Deobandi fundamentalism[80] and the militant Islamism and Salafi jihadism of Osama bin Laden[80] with Pashtun social and cultural norms known as Pashtunwali,[1][2][81][page needed][82] as most Taliban are Pashtun tribesmen.
The Pakistani Inter-Services Intelligence and military are widely alleged by the international community and the Afghan government to have provided support to the Taliban during their founding and time in power, and of continuing to support the Taliban during the insurgency. Pakistan states that it dropped all support for the group after the 11 September attacks.[83][84][85][86][87][88] In 2001, reportedly 2,500 Arabs under command of Al-Qaeda leader Osama bin Laden fought for the Taliban.[89]
The word Taliban is Pashto, طالبان ṭālibān, meaning ""students"", the plural of ṭālib. This is a loanword from Arabic طالب ṭālib,  using the Persian plural ending -ān ان. In Arabic طالبان ṭālibān means not ""students"" but ""two students"", as it is a dual form, the Arabic plural being طلاب ṭullāb—occasionally causing some confusion to Arabic speakers. Since becoming a loanword in English, Taliban, besides a plural noun referring to the group, has also been used as a singular noun referring to an individual. For example, John Walker Lindh has been referred to as ""an American Taliban"", rather than ""an American Talib"". 
The spelling Taliban has come to be predominant over Taleban in English.[90][91] In American English, a ""The"" prefix is used thereby referring to the group ""The Taliban"" rather than just ""Taliban"". Meanwhile in English language media in Pakistan, there is often no prefix used.[92] Both Pakistani and Indian English-language media also tend to name the group ""Afghan Taliban"".[93][94] Additionally in Pakistan the word Talibans is often used when referring to more than one Taliban member.
Within Afghanistan, the Taliban is often referred to in Dari as گروه طالبان Goroh-e Taleban, meaning ""Taliban group"".[95] As per Dari/Persian grammar, there is no ""The"" prefix. Meanwhile in Pashto, there is normally a prefix when referring to the group as per Pashto grammar: د طالبان Da Taliban or د طالبانو Da Talibano.
After the Soviet Union intervened and occupied Afghanistan in 1979, Islamic mujahideen fighters engaged in war with those Soviet forces.
Pakistan's President Muhammad Zia-ul-Haq feared that the Soviets were planning to invade also Balochistan, Pakistan, so he sent Akhtar Abdur Rahman to Saudi Arabia to garner support for the Afghan resistance against Soviet occupation forces. A while later, the US CIA and Saudi Arabian General Intelligence Directorate (GID) funnelled funding and equipment through the Pakistani Inter-Service Intelligence Agency (ISI) to the Afghan mujahideen.[96]
About 90,000 Afghans, including Mohammed Omar, were trained by Pakistan's ISI during the 1980s.[96] British professor Carole Hillenbrand concluded that the Taliban have arisen from those US-Saudi-Pakistan-supported mujahideen: ""The West helped the Taliban to fight the Soviet takeover of Afghanistan"".[97]
After the fall of the Soviet-backed regime of Mohammad Najibullah in 1992, many Afghan political parties, but not Gulbuddin Hekmatyar's Hezb-e Islami Gulbuddin, Hizb-e Wahdat, and Ittihad-i Islami, in April agreed on a peace and power-sharing agreement, the Peshawar Accord, which created the Islamic State of Afghanistan and appointed an interim government for a transitional period; but that Islamic State and its government were paralysed right from the start, due to rivalling groups contending for total power over Kabul and Afghanistan.[98]
Hekmatyar's Hezb-e Islami Gulbuddin party refused to recognise the interim government, and in April infiltrated Kabul to take power for itself, thus starting this civil war. In May, Hekmatyar started attacks against government forces and Kabul.[99] Hekmatyar received operational, financial and military support from Pakistan's ISI.[100] With that help,  Hekmatyar's forces were able to destroy half of Kabul.[101] Iran assisted the Hizb-e Wahdat forces of Abdul Ali Mazari. Saudi Arabia supported the Ittihad-i Islami faction.[99][101][102]  The conflict between these militias also escalated into war.
Due to this sudden initiation of civil war, working government departments, police units or a system of justice and accountability for the newly created Islamic State of Afghanistan did not have time to form.[citation needed] Atrocities were committed by individuals inside different factions.[citation needed] Ceasefires, negotiated by representatives of the Islamic State's newly appointed Defense Minister Ahmad Shah Massoud, President Sibghatullah Mojaddedi and later President Burhanuddin Rabbani (the interim government), or officials from the International Committee of the Red Cross (ICRC), commonly collapsed within days.[99] The countryside in northern Afghanistan, parts of which were under the control of Defense Minister Massoud, remained calm and some reconstruction took place. The city of Herat under the rule of Islamic State ally Ismail Khan also witnessed relative calm.[citation needed]
Meanwhile, southern Afghanistan was neither under the control of foreign-backed militias nor the government in Kabul, but was ruled by local leaders such as Gul Agha Sherzai and their militias. The Taliban only first emerged on the scene in August 1994, announcing to liberate Afghanistan from its present corrupt leadership of warlords, and establish a pure Islamic society.[citation needed]
The Taliban are a movement of religious students (talib) from the Pashtun areas of eastern and southern Afghanistan who were educated in traditional Islamic schools in Pakistan.[52] There were also Tajik and Uzbek students, demarking them from the more ethnic-centric mujahideen groups ""which played a key role in the Taliban’s rapid growth and success.""[103]
Mullah Mohammad Omar in September 1994 in his hometown of Kandahar with 50 students founded the group.[104][105][106]
Omar had since 1992 been studying in the Sang-i-Hisar madrassa in Maiwand (northern Kandahar Province). He was unhappy that Islamic law had not been installed in Afghanistan after the ousting of communist rule, and now with his group pledged to rid Afghanistan of warlords and criminals.[104]
Within months, 15,000 students, often Afghan refugees, from religious schools or madrasas – one source calls them Jamiat Ulema-e-Islam-run madrasas[105] – in Pakistan joined the group.
The US government covertly provided violent schoolbooks filled with militant Islamic teachings and jihad and images of weapons and soldiers in an effort to inculcate in children anti-Soviet insurgency and hate for foreigners. The Taliban used the American textbooks but scratched out human faces in keeping with strict fundamentalist interpretation. The United States Agency for International Development gave millions of dollars to the University of Nebraska at Omaha in the 1980s to develop and publish the textbooks in local languages.[107]
Those early Taliban were motivated by the suffering among the Afghan people, which they believed resulted from power struggles between Afghan groups not adhering to the moral code of Islam; in their religious schools they had been taught a belief in strict Islamic law.[104][53][54]
Sources state that Pakistan was heavily involved, already in October 1994, in the ""creating"" of the Taliban.[108][109] Pakistan's Inter-Services Intelligence agency (ISI), strongly supporting the Taliban in 1994, hoped for a new ruling power in Afghanistan favourable to Pakistan.[104] Even if the Taliban received financial support from Pakistan in 1995 and 1996, and even if ""Pakistani support was forthcoming from an early stage of the Taliban movement’s existence, the connection was fragile and statements from both the Pakistani ISI as well as the Taliban early on demonstrated the uneasy nature of the relationship. The ISI and Pakistan aimed to exert control, while the Taliban leadership manoeuvred between keeping its independence and sustaining support."" The main supporters in Pakistan were General Naseerullah Babar, who mainly thought in terms of geopolitics (opening trade routes to Central Asia), and Maulana Fazl-ur-Rehman of the Jamiat Ulema-e-Islam (F), as ""the group represented Deobandism and aimed to counter the influence of the Jama’at-e Islami and growing Wahhabism.""[110]
On 3 November 1994, the Taliban in a surprise attack conquered Kandahar City.[104] Before 4 January 1995, they controlled 12 Afghan provinces.[104]
Militias controlling the different areas often surrendered without a fight. Omar's commanders were a mixture of former small-unit military commanders and madrassa teachers.[111][112][113][114][115]
At these stages, the Taliban were popular, because they stamped out corruption, curbed lawlessness, and made the roads and area safe.[104]
In a bid to establish their rule over all Afghanistan, the Taliban expanded from their Kandahar base sweeping large territories. In early 1995 the movement moved towards Kabul, but they suffered a devastating defeat against government forces of the Islamic State of Afghanistan under the command of Ahmad Shah Massoud. While retreating from Kabul, Taliban fighters started shelling the city,[116] killing many civilians. The media reported in March 1995 that, following the Taliban's shelling, they lost much respect from Afghans and were seen as just another ""power-hungry"" militia.[117]
After a series of setbacks, the Taliban managed to take control of the western city of Herat on 5 September 1995. Following allegations by the recognised government that Pakistan was aiding the Taliban, a large mob of people attacked the Pakistani embassy in Kabul the day after.[118]
On 26 September 1996, as the Taliban prepared for another major offensive, Massoud ordered a full retreat from Kabul to continue anti-Taliban resistance in the northeastern Hindu Kush mountains instead of engaging in street battles in Kabul. The Taliban entered Kabul on 27 September 1996 and established the Islamic Emirate of Afghanistan. Analysts described the Taliban then as developing into a proxy force for Pakistan's regional interests.[101][113][116][119][120][121]
The military goal of the Taliban during the period 1995 to 2001 was to return the order of Abdur Rahman (the Iron Emir) by the re-establishment of a state with Pashtun dominance within the northern areas.[122] The Taliban sought to establish an Islamic government through law and order alongside a strict interpretation of Sharia law, in accordance with the Hanafi school of Islamic jurisprudence and the religious edicts of Mullah Omar, upon the entire land of Afghanistan.[50] By 1998, the Taliban's Emirate controlled 90% of Afghanistan.[104]
In December 2000, the UNSC in Resolution 1333, recognising humanitarian needs of the Afghan people, condemning the use of Taliban territory for training of ""terrorists"" and Taliban providing safe haven to Osama bin Laden, issued severe sanctions against Afghanistan under Taliban control.[123]
In October 2001, the United States, with allies including the Afghan Northern Alliance, invaded Afghanistan and routed the Taliban regime. The Taliban leadership fled into Pakistan.[104]
When the Taliban took power in 1996, twenty years of continuous warfare had devastated Afghanistan's infrastructure and economy. There was no running water, little electricity, few telephones, functioning roads or regular energy supplies. Basic necessities like water, food, housing and others were in desperately short supply. In addition, the clan and family structure that provided Afghans with a social/economic safety net was also badly damaged.  Afghanistan's infant mortality was the highest in the world. A full quarter of all children died before they reached their fifth birthday, a rate several times higher than most other developing countries.[124][125][126]
International charitable and/or development organisations (non-governmental organisations or NGOs) were extremely important to the supply of food, employment, reconstruction, and other services, but the Taliban proved highly suspicious towards the 'help' those organisations offered (see § United Nations and NGOs). With one million plus deaths during the years of war, the number of families headed by widows had reached 98,000 by 1998. In Kabul, where vast portions of the city had been devastated by rocket attacks, more than half of its 1.2 million people benefited in some way from NGO activities, even for drinking water.  The civil war and its never-ending refugee stream continued throughout the Taliban's reign. The Mazar, Herat, and Shomali valley offensives displaced more than three-quarters of a million civilians, using ""scorched earth"" tactics to prevent them from supplying the enemy with aid.[127][128][129]
Taliban decision-makers, particularly Mullah Omar, seldom if ever talked directly to non-Muslim foreigners, so aid providers had to deal with intermediaries whose approvals and agreements were often reversed.[130] Around September 1997 the heads of three UN agencies in Kandahar were expelled from the country after protesting when a female attorney for the UN High Commissioner for Refugees was forced to talk from behind a curtain so her face would not be visible.[131]
When the UN increased the number of Muslim women staff to satisfy Taliban demands, the Taliban then required all female Muslim UN staff travelling to Afghanistan to be chaperoned by a mahram or a blood relative.[132] In July 1998, the Taliban closed ""all NGO offices"" in Kabul by force after those organisations refused to move to a bombed-out former Polytechnic College as ordered.[133] One month later the UN offices were also shut down.[134] As food prices rose and conditions deteriorated, Planning Minister Qari Din Mohammed explained the Taliban's indifference to the loss of humanitarian aid:
We Muslims believe God the Almighty will feed everybody one way or another. If the foreign NGOs leave then it is their decision. We have not expelled them.[135]The few organisations active in Kandahar were not subjected to the same demands and continued their operations. 
The Taliban were largely founded by Pakistan's Inter-Services Intelligence beginning in 1994; the I.S.I. used the Taliban to establish a regime in Afghanistan which would be favourable to Pakistan, as they were trying to gain strategic depth. Since the creation of the Taliban, the ISI and the Pakistani military have given financial, logistical and military support.[83][136][137][138][139][140][141][142][143][144][145][146][147][148][149][150]
According to Pakistani Afghanistan expert Ahmed Rashid, ""between 1994 and 1999, an estimated 80,000 to 100,000 Pakistanis trained and fought in Afghanistan"" on the side of the Taliban. Peter Tomsen stated that up until 9/11 Pakistani military and ISI officers along with thousands of regular Pakistani armed forces personnel had been involved in the fighting in Afghanistan.[151][152]
During 2001, according to several international sources, 28,000–30,000 Pakistani nationals, 14,000–15,000 Afghan Taliban and 2,000–3,000 Al-Qaeda militants were fighting against anti-Taliban forces in Afghanistan as a roughly 45,000 strong military force.  Pakistani President Pervez Musharraf – then as Chief of Army Staff – was responsible for sending thousands of Pakistanis to fight alongside the Taliban and Bin Laden against the forces of Ahmad Shah Massoud. Of the estimated 28,000 Pakistani nationals fighting in Afghanistan, 8,000 were militants recruited in madrassas filling regular Taliban ranks. The document further states that the parents of those Pakistani nationals ""know nothing regarding their child's military involvement with the Taliban until their bodies are brought back to Pakistan"". A 1998 document by the US State Department confirms that ""20–40 percent of [regular] Taliban soldiers are Pakistani.""  According to the State Department report and reports by Human Rights Watch, the other Pakistani nationals fighting in Afghanistan were regular Pakistani soldiers, especially from the Frontier Corps but also from the army providing direct combat support.[89][153][119][154][155][156][157]
Human Rights Watch wrote in 2000:
Of all the foreign powers involved in efforts to sustain and manipulate the ongoing fighting [in Afghanistan], Pakistan is distinguished both by the sweep of its objectives and the scale of its efforts, which include soliciting funding for the Taliban, bankrolling Taliban operations, providing diplomatic support as the Taliban's virtual emissaries abroad, arranging training for Taliban fighters, recruiting skilled and unskilled manpower to serve in Taliban armies, planning and directing offensives, providing and facilitating shipments of ammunition and fuel, and ... directly providing combat support.[84]On 1 August 1997, the Taliban launched an attack on Sheberghan, the main military base of Abdul Rashid Dostum. Dostum has said the reason the attack was successful was due to 1500 Pakistani commandos taking part and that the Pakistani air force also gave support.[158]
In 1998, Iran accused Pakistan of sending its air force to bomb Mazar-i-Sharif in support of Taliban forces and directly accused Pakistani troops for ""war crimes at Bamiyan"". The same year, Russia said Pakistan was responsible for the ""military expansion"" of the Taliban in northern Afghanistan by sending large numbers of Pakistani troops, some of whom had subsequently been taken as prisoners by the anti-Taliban United Front.[159][160]
During 2000, the UN Security Council imposed an arms embargo against military support to the Taliban, with UN officials explicitly singling out Pakistan. The UN secretary-general implicitly criticised Pakistan for its military support and the Security Council stated it was ""deeply distress[ed] over reports of involvement in the fighting, on the Taliban side, of thousands of non-Afghan nationals"". In July 2001, several countries, including the United States, accused Pakistan of being ""in violation of U.N. sanctions because of its military aid to the Taliban"". The Taliban also obtained financial resources from Pakistan. In 1997 alone, after the capture of Kabul by the Taliban, Pakistan gave $30 million in aid and a further $10 million for government wages.[161][162][163]
During 2000, MI6 reported that the ISI was taking an active role in several Al-Qaeda training camps. The ISI helped with the construction of training camps for both the Taliban and Al-Qaeda. From 1996 to 2001 the Al-Qaeda of Osama bin Laden and Ayman al-Zawahiri became a state within the Taliban state. Bin Laden sent Arab and Central Asian Al-Qaeda militants to join the fight against the United Front, among them his Brigade 055.[164][165][166][167][168]
The role of the Pakistani military has been described by international observers as well as by the anti-Taliban leader Ahmad Shah Massoud as a ""creeping invasion"".[151]
In late 1996 Ahmad Shah Massoud and Abdul Rashid Dostum, former enemies, created the United Front (Northern Alliance) against the Taliban that were preparing offensives against the remaining areas under the control of Massoud and those under the control of Dostum. The United Front included beside the dominantly Tajik forces of Massoud and the Uzbek forces of Dostum, Hazara troops led by Haji Mohammad Mohaqiq and Pashtun forces under the leadership of commanders such as Abdul Haq and Haji Abdul Qadir. Notable politicians and diplomats of the United Front included Abdul Rahim Ghafoorzai, Abdullah Abdullah and Massoud Khalili. From the Taliban conquest of Kabul in September 1996 until November 2001 the United Front controlled roughly 30% of Afghanistan's population in provinces such as Badakhshan, Kapisa, Takhar and parts of Parwan, Kunar, Nuristan, Laghman, Samangan, Kunduz, Ghōr and Bamyan.
After longstanding battles, especially for the northern city of Mazar-i-Sharif, Abdul Rashid Dostum and his Junbish forces were defeated by the Taliban and their allies in 1998. Dostum subsequently went into exile. Ahmad Shah Massoud remained the only major anti-Taliban leader inside Afghanistan who was able to defend vast parts of his territory against the Taliban.
In the areas under his control Massoud set up democratic institutions and signed the Women's Rights Declaration.  In the area of Massoud, women and girls did not have to wear the Afghan burqa. They were allowed to work and to go to school. In at least two known instances, Massoud personally intervened against cases of forced marriage.
It is our conviction and we believe that both men and women are created by the Almighty. Both have equal rights. Women can pursue an education, women can pursue a career, and women can play a role in society – just like men.[153][169]Massoud is adamant that in Afghanistan women have suffered oppression for generations. He says that ""the cultural environment of the country suffocates women. But the Taliban exacerbate this with oppression."" His most ambitious project is to shatter this cultural prejudice and so give more space, freedom and equality to women – they would have the same rights as men.[153]Afghan traditions would need a generation or more to overcome and could only be challenged by education, he said. Humayun Tandar, who took part as an Afghan diplomat in the 2001 International Conference on Afghanistan in Bonn, said that ""strictures of language, ethnicity, region were [also] stifling for Massoud. That is why ... he wanted to create a unity which could surpass the situation in which we found ourselves and still find ourselves to this day."" This applied also to strictures of religion. Jean-José Puig describes how Massoud often led prayers before a meal or at times asked his fellow Muslims to lead the prayer but also did not hesitate to ask a Christian friend Jean-José Puig or the Jewish Princeton University Professor Michael Barry: ""Jean-José, we believe in the same God. Please, tell us the prayer before lunch or dinner in your own language.""[153]
Human Rights Watch cites no human rights crimes for the forces under direct control of Massoud for the period from October 1996 until the assassination of Massoud in September 2001. 400,000 to one million Afghans fled from the Taliban to the area of Massoud.[157][170][171] National Geographic concluded in its documentary Inside the Taliban: ""The only thing standing in the way of future Taliban massacres is Ahmad Shah Massoud.""[157]
The Taliban repeatedly offered Massoud a position of power to make him stop his resistance. Massoud declined. He explained in one interview:
The Taliban say: ""Come and accept the post of prime minister and be with us"", and they would keep the highest office in the country, the presidentship. But at what cost?! The difference between us concerns mainly our way of thinking about the very principles of the society and the state. We can not accept their conditions of compromise, or else we would have to give up the principles of modern democracy. We are fundamentally against the system called ""the Emirate of Afghanistan"".[172]The United Front in its Proposals for Peace demanded the Taliban to join a political process leading towards nationwide democratic elections. In early 2001, Massoud employed a new strategy of local military pressure and global political appeals. Resentment was increasingly gathering against Taliban rule from the bottom of Afghan society, including the Pashtun areas. Massoud publicised their cause of ""popular consensus, general elections and democracy"" worldwide. At the same time he was very wary not to revive the failed Kabul government of the early 1990s. Already in 1999, he started the training of police forces which he trained specifically in order to keep order and protect the civilian population in case the United Front would be successful.[153][172][173] Massoud stated:
The Taliban are not a force to be considered invincible. They are distanced from the people now. They are weaker than in the past. There is only the assistance given by Pakistan, Osama bin Laden and other extremist groups that keep the Taliban on their feet. With a halt to that assistance, it is extremely difficult to survive.[174]From 1999 onwards, a renewed process was set into motion by the Tajik Ahmad Shah Massoud and the Pashtun Abdul Haq to unite all the ethnicities of Afghanistan. While Massoud united the Tajiks, Hazara and Uzbeks as well as some Pashtun commanders under his United Front command, the famed Pashtun commander Abdul Haq received increasing numbers of defecting Pashtun Taliban as ""Taliban popularity trended downward"". Both agreed to work together with the exiled Afghan king Zahir Shah. International officials who met with representatives of the new alliance, which Pulitzer Prize winner Steve Coll referred to as the ""grand Pashtun-Tajik alliance"", said, ""It's crazy that you have this today ... Pashtuns, Tajiks, Uzbeks, Hazara ... They were all ready to buy in to the process ... to work under the king's banner for an ethnically balanced Afghanistan."" Senior diplomat and Afghanistan expert Peter Tomsen wrote: ""The 'Lion of Kabul' [Abdul Haq] and the 'Lion of Panjshir' [Ahmad Shah Massoud] ... Haq, Massoud, and Karzai, Afghanistan's three leading moderates, could transcend the Pashtun–non-Pashtun, north–south divide."" The most senior Hazara and Uzbek leader were also part of the process. In late 2000, Massoud officially brought together this new alliance in a meeting in Northern Afghanistan to discuss, among other things, ""a Loya Jirga, or a traditional council of elders, to settle political turmoil in Afghanistan"".  That part of the Pashtun–Tajik–Hazara–Uzbek peace plan did eventually materialise. An account of the meeting by author and journalist Sebastian Junger says: ""In 2000, when I was there ... I happened to be there in a very interesting time. ... Massoud brought together Afghan leaders from all ethnic groups. They flew from London, Paris, the USA, all parts of Afghanistan, Pakistan, India. He brought them all into the northern area where he was. He held a council of ... prominent Afghans from all over the world, brought there to discuss the Afghan government after the Taliban. ... we met all these men and interviewed them briefly. One was Hamid Karzai; I did not have any idea who he would end up being"".[173][175][176][177][178]
In early 2001, Ahmad Shah Massoud with ethnic leaders from all of Afghanistan addressed the European Parliament in Brussels asking the international community to provide humanitarian help to the people of Afghanistan. He stated that the Taliban and Al-Qaeda had introduced ""a very wrong perception of Islam"" and that without the support of Pakistan and Bin Laden the Taliban would not be able to sustain their military campaign for up to a year. On this visit to Europe he also warned that his intelligence had gathered information about a large-scale attack on US soil being imminent. The president of the European Parliament, Nicole Fontaine, called him the ""pole of liberty in Afghanistan"".[179][180][181][182]
On 9 September 2001, Massoud, then aged 48, was the target of a suicide attack by two Arabs posing as journalists at Khwaja Bahauddin, in the Takhar Province of Afghanistan. Massoud, who had survived countless assassination attempts over a period of 26 years, died in a helicopter taking him to a hospital. The first attempt on Massoud's life had been carried out by Hekmatyar and two Pakistani ISI agents in 1975, when Massoud was only 22 years old. In early 2001, Al-Qaeda would-be assassins were captured by Massoud's forces while trying to enter his territory.[102][173][183][184] The funeral, though in a rather rural area, was attended by hundreds of thousands of mourning people.
The assassination of Massoud is believed to have a connection to the September 11 attacks on US soil, which killed nearly 3000 people, and which appeared to be the terrorist attack that Massoud had warned against in his speech to the European Parliament several months earlier. John P. O'Neill was a counter-terrorism expert and the Assistant Director of the FBI until late 2001. He retired from the FBI and was offered the position of director of security at the World Trade Center (WTC). He took the job at the WTC two weeks before 9/11. On 10 September 2001, O'Neill told two of his friends, ""We're due. And we're due for something big. ... Some things have happened in Afghanistan. [referring to the assassination of Massoud] I don't like the way things are lining up in Afghanistan. ... I sense a shift, and I think things are going to happen ... soon."" O'Neill died on 11 September 2001, when the South Tower collapsed.[185][186]
After 9/11, Massoud's United Front troops and United Front troops of Abdul Rashid Dostum (who returned from exile) ousted the Taliban from power in Kabul with American air support in Operation Enduring Freedom. From October to December 2001, the United Front gained control of much of the country and played a crucial role in establishing the post-Taliban interim government under Hamid Karzai.
On 20 September 2001, US president George W. Bush, speaking to a joint session of Congress, tentatively blamed Al-Qaeda for the 11 September attacks, stating that the ""leadership of Al Qaeda ha[d] great influence in Afghanistan and support[ed] the Taliban regime in controlling most of that country"". Bush said, ""We condemn the Taliban regime"", and went on to state, ""Tonight the United States of America makes the following demands on the Taliban"", which he said were ""not open to negotiation or discussion"":[187][188]
The US petitioned the international community to back a military campaign to overthrow the Taliban. The UN issued two resolutions on terrorism after the 11 September attacks. The resolutions called on all states to ""[increase] cooperation and full implementation of the relevant international conventions relating to terrorism"" and specified consensus recommendations for all countries.[189][190] According to a research briefing by the House of Commons Library, although the United Nations Security Council (UNSC) did not authorise the U.S.-led military campaign, it was ""widely (although not universally) perceived to be a legitimate form of self-defense under the UN Charter"", and the council ""moved quickly to authorize a military operation to stabilize the country"" in the wake of the invasion.[191] Moreover, on 12 September 2001, NATO approved a campaign against Afghanistan as self-defense against armed attack.[192]
The Taliban ambassador to Pakistan, Abdul Salem Zaeef, responded to the ultimatum by demanding ""convincing evidence"" that Bin Laden was involved in the attacks, stating ""our position is that if America has evidence and proof, they should produce it"".  Additionally, the Taliban insisted that any trial of Bin Laden be held in an Afghan court. Zaeef also claimed that ""4,000 Jews working in the Trade Center had prior knowledge of the suicide missions, and 'were absent on that day'.""  This response was generally dismissed as a delaying tactic, rather than a sincere attempt to cooperate with the ultimatum.[193][194][195][196][197][198]
[check quotation syntax]
On 22 September, the United Arab Emirates, and later Saudi Arabia, withdrew recognition of the Taliban as Afghanistan's legal government, leaving neighbouring Pakistan as the only remaining country with diplomatic ties. On 4 October, the Taliban agreed to turn bin Laden over to Pakistan for trial in an international tribunal  that operated according to Islamic Sharia law, but Pakistan blocked the offer as it was not possible to guarantee his safety.  On 7 October, the Taliban ambassador to Pakistan offered to detain bin Laden and try him under Islamic law if the US made a formal request and presented the Taliban with evidence. A Bush administration official, speaking on condition of anonymity, rejected the Taliban offer, and stated that the US would not negotiate their demands.[199][200][201]
On 7 October 2001, less than one month after the 11 September attacks, the US, aided by the United Kingdom, Canada, and other countries including several from the NATO alliance, initiated military action, bombing Taliban and Al-Qaeda-related camps.[202][203] The stated intent of military operations was to remove the Taliban from power, and prevent the use of Afghanistan as a terrorist base of operations.[204]
The CIA's elite Special Activities Division (SAD) units were the first US forces to enter Afghanistan (many different countries' intelligence agencies were on the ground or operating within theatre before SAD, and SAD are not technically military forces, but civilian paramilitaries). They joined with the Afghan United Front (Northern Alliance) to prepare for the subsequent arrival of US Special Operations forces. The United Front (Northern Alliance) and SAD and Special Forces combined to overthrow the Taliban with minimal coalition casualties, and without the use of international conventional ground forces. The Washington Post stated in an editorial by John Lehman in 2006:
What made the Afghan campaign a landmark in the US Military's history is that it was prosecuted by Special Operations forces from all the services, along with Navy and Air Force tactical power, operations by the Afghan Northern Alliance and the CIA were equally important and fully integrated. No large Army or Marine force was employed.[205]On 14 October, the Taliban offered to discuss handing over Osama bin Laden to a neutral country in return for a bombing halt, but only if the Taliban were given evidence of bin Laden's involvement.[206] The US rejected this offer, and continued military operations. Mazar-i-Sharif fell to United Front troops of Ustad Atta Mohammad Noor and Abdul Rashid Dostum on 9 November, triggering a cascade of provinces falling with minimal resistance.
In November 2001, before the capture of Kunduz by United Front troops under the command of Mohammad Daud Daud, thousands of top commanders and regular fighters of the Taliban and Al-Qaeda, Pakistani Inter-Services Intelligence agents and military personnel, and other volunteers and sympathizers in the Kunduz airlift, dubbed the Airlift of Evil by US military forces around Kunduz and subsequently used as a term in media reports, were evacuated and airlifted out of Kunduz by Pakistan Army cargo aircraft to Pakistan Air Force air bases in Chitral and Gilgit in Pakistan's Northern Areas.[207][208][209][210][211][212]
On the night of 12 November, the Taliban retreated south from Kabul. On 15 November, they released eight Western aid workers after three months in captivity. By 13 November, the Taliban had withdrawn from both Kabul and Jalalabad. Finally, in early December, the Taliban gave up Kandahar, their last stronghold, dispersing without surrendering.
The United States has conducted targeted killings against Taliban leaders, mainly using Special Forces, and sometimes unmanned aerial vehicles. British forces also used similar tactics, mostly in Helmand Province, Afghanistan. During Operation Herrick, British special forces carried out targeted killings against at least fifty high and local Taliban commanders in Helmand Province.[citation needed]
The Taliban have also used targeted killings. In 2011 alone, they killed notable anti-Taliban leaders, such as former Afghan President Burhanuddin Rabbani, the police chief in northern Afghanistan, the commander of the elite anti-Taliban 303 Pamir Corps, Mohammad Daud Daud, and the police chief of Kunduz, Abdul Rahman Saidkhaili. All of them belonged to the Massoud faction of the United Front. According to Guantanamo Bay charge sheets, the United States Department of Defense believes the Taliban may maintain a 40-man undercover unit called ""Jihad Kandahar"", which is used for undercover operations, including targeted killings.[213]
After the attacks of 11 September 2001 on the United States, Pakistan has been accused of continuing to support the Taliban, an allegation Pakistan denies.[86][214]
With the fall of Kabul to anti-Taliban forces in November 2001, ISI forces worked with and helped Taliban militias who were in full retreat.  In November 2001, Taliban, Al-Qaeda combatants and ISI operatives were safely evacuated from Kunduz on Pakistan Army cargo aircraft to Pakistan Air Force bases in Chitral and Gilgit in Pakistan's Northern Areas (see Kunduz airlift). Former Pakistani president Pervez Musharraf wrote in his memoirs that Richard Armitage, the former US deputy secretary of state, said Pakistan would be ""bombed back to the stone-age"" if it continued to support the Taliban, although Armitage has since denied using the ""stone age"" phrase.[215][216][217][207][218][219][220][221][222]
In May and June 2003, high Taliban officials proclaimed the Taliban regrouped and ready for guerrilla war to expel US forces from Afghanistan.[223][224] In late 2004, the then hidden Taliban leader Mohammed Omar announced an insurgency against ""America and its puppets"" (i.e. transitional Afghan government forces) to ""regain the sovereignty of our country"".[225]
On 29 May 2006, while according to American website The Spokesman-Review Afghanistan faced ""a mounting threat from armed Taliban fighters in the countryside"", a US military truck of a convoy in Kabul lost control and plowed into twelve civilian vehicles, killing one and injuring six people. The surrounding crowd got angry and a riot arose, lasting all day ending with 20 dead and 160 injured. When stone-throwing and gunfire had come from a crowd of some 400 men, the US troops had used their weapons ""to defend themselves"" while leaving the scene, a US military spokesman said. A correspondent for the Financial Times in Kabul suggested that this was the outbreak of ""a ground swell of resentment"" and ""growing hostility to foreigners"" that had been growing and building since 2004, and may also have been triggered by a US air strike a week earlier in southern Afghanistan killing 30 civilians, where she assumed that ""the Taliban had been sheltering in civilian houses"".[226][227]
The continued support from tribal and other groups in Pakistan, the drug trade, and the small number of NATO forces, combined with the long history of resistance and isolation, indicated that Taliban forces and leaders were surviving. Suicide attacks and other terrorist methods not used in 2001 became more common. Observers suggested that poppy eradication, which hurts the livelihoods of those Afghans who have resorted to their production, and civilian deaths caused by airstrikes abetted the resurgence. These observers maintained that policy should focus on ""hearts and minds"" and on economic reconstruction, which could profit from switching from interdicting to diverting poppy production—to make medicine.[228][229]
Other commentators viewed Islamabad's shift from war to diplomacy as an effort to appease growing discontent.[230] Because of the Taliban's leadership structure, Mullah Dadullah's assassination in May 2007 did not have a significant effect, other than to damage incipient relations with Pakistan.[231]
On 8 February 2009, US commander of operations in Afghanistan General Stanley McChrystal and other officials said that the Taliban leadership was in Quetta, Pakistan.[232]
By 2009, a strong insurgency had coalesced, known as Operation Al Faath, the Arabic word for ""victory"" taken from the Koran,[233][234][235] in the form of a guerrilla war. The Pashtun tribal group, with over 40 million members (including Afghans and Pakistanis) had a long history of resistance to occupation forces, so the Taliban may have comprised only a part of the insurgency. Most post-invasion Taliban fighters were new recruits, mostly drawn from local madrasas.
In December 2009, Asia Times Online reported that the Taliban had offered to give the US ""legal guarantees"" that it would not allow Afghanistan to be used for attacks on other countries, and that the US had given no response.[236]
As of July 2016, the US Time magazine estimated 20% of Afghanistan to be under Taliban control with southernmost Helmand Province as their stronghold,[237] while US and international Resolute Support coalition commanding General Nicholson in December 2016 likewise stated that 10% was in Taliban hands while another 26% of Afghanistan was contested between the Afghan government and various insurgency groups.[238]
In August 2017, reacting to a hostile speech by US President Trump, a Taliban spokesman retorted that the Taliban would keep fighting to free Afghanistan of ""American invaders"".[239]
On 29 May 2020, it was reported that Mullah Omar's son Mullah Mohammad Yaqoob was now acting as leader of the Taliban after numerous Quetta Shura members were infected with COVID-19.[240] It was previously confirmed on 7 May 2020 that Yaqoob had become head of the Taliban military commission, making him the insurgents' military chief.[241] Among those infected in the Quetta Shura, which continued to hold in-person meetings, were Hibatullah and Sirajuddin Haqqani.[240]
According to a 55-page report by the United Nations, the Taliban, while trying to consolidate control over northern and western Afghanistan, committed systematic massacres against civilians.  UN officials stated that there had been ""15 massacres"" between 1996 and 2001. They also said, that ""[t]hese have been highly systematic and they all lead back to the [Taliban] Ministry of Defense or to Mullah Omar himself.""  ""These are the same type of war crimes as were committed in Bosnia and should be prosecuted in international courts"", one UN official was quoted as saying. The documents also reveal the role of Arab and Pakistani support troops in these killings. Bin Laden's so-called 055 Brigade was responsible for mass-killings of Afghan civilians.  The report by the United Nations quotes ""eyewitnesses in many villages describing Arab fighters carrying long knives used for slitting throats and skinning people"". The Taliban's former ambassador to Pakistan, Mullah Abdul Salam Zaeef, in late 2011 stated that cruel behaviour under and by the Taliban had been ""necessary"".[62][63][89][242]
In 1998, the United Nations accused the Taliban of denying emergency food by the UN's World Food Programme to 160,000 hungry and starving people ""for political and military reasons"".[243] The UN said the Taliban were starving people for their military agenda and using humanitarian assistance as a weapon of war.
On 8 August 1998 the Taliban launched an attack on Mazar-i Sharif. Of 1500 defenders only 100 survived the engagement. Once in control the Taliban began to kill people indiscriminately. At first shooting people in the street, they soon began to target Hazaras. Women were raped, and thousands of people were locked in containers and left to suffocate. This ethnic cleansing left an estimated 5,000 to 6,000 dead. At this time ten Iranian diplomats and a journalist were killed. Iran assumed the Taliban had murdered them, and mobilised its army, deploying men along the border with Afghanistan. By the middle of September there were 250,000 Iranian personnel stationed on the border. Pakistan mediated and the bodies were returned to Tehran towards the end of the month. The killings of the diplomats had been carried out by Sipah-e-Sahaba, a Pakistani Sunni group with close ties to the ISI.  They burned orchards, crops and destroyed irrigation systems, and forced more than 100,000 people from their homes with hundreds of men, women and children still unaccounted for.[244][245][246][247][248]
In a major effort to retake the Shomali Plains to the north of Kabul from the United Front, the Taliban indiscriminately killed civilians, while uprooting and expelling the population. Among others, Kamal Hossein, a special reporter for the UN, reported on these and other war crimes. In Istalif, a town famous for handmade potteries and which was home to more than 45,000 people, the Taliban gave 24 hours' notice to the population to leave, then completely razed the town leaving the people destitute.[66][249]
In 1999 the town of Bamian was taken, hundreds of men, women and children were executed. Houses were razed and some were used for forced labour. There was a further massacre at the town of Yakaolang in January 2001. An estimated 300 people were murdered, along with two delegations of Hazara elders who had tried to intercede.[250][251]
By 1999, the Taliban had forced hundreds of thousands of people from the Shomali Plains and other regions conducting a policy of scorched earth burning homes, farm land and gardens.[66]
Several Taliban and al-Qaeda commanders ran a network of human trafficking, abducting women and selling them into sex slavery in Afghanistan and Pakistan.[252] Time magazine writes: ""The Taliban often argued that the restrictions they placed on women were actually a way of revering and protecting the opposite sex. The behavior of the Taliban during the six years they expanded their rule in Afghanistan made a mockery of that claim.""[252]
The targets for human trafficking were especially women from the Tajik, Uzbek, Hazara and other ethnic groups in Afghanistan. Some women preferred to commit suicide over slavery, killing themselves. During one Taliban and al-Qaeda offensive in 1999 in the Shomali Plains alone, more than 600 women were kidnapped.[252] Arab and Pakistani al-Qaeda militants, with local Taliban forces, forced them into trucks and buses.[252] Time magazine writes: ""The trail of the missing Shomali women leads to Jalalabad, not far from the Pakistan border. There, according to eyewitnesses, the women were penned up inside Sar Shahi camp in the desert. The more desirable among them were selected and taken away. Some were trucked to Peshawar with the apparent complicity of Pakistani border guards. Others were taken to Khost, where bin Laden had several training camps."" Officials from relief agencies say, the trail of many of the vanished women leads to Pakistan where they were sold to brothels or into private households to be kept as slaves.[252]
Not all Taliban commanders engaged in human trafficking. Many Taliban were opposed to the human trafficking operations conducted by al-Qaeda and other Taliban commanders. Nuruludah, a Taliban commander, is quoted as saying that in the Shomali Plains, he and 10 of his men freed some women who were being abducted by Pakistani members of al-Qaeda. In Jalalabad, local Taliban commanders freed women that were being held by Arab members of al-Qaeda in a camp.[252]
To PHR's knowledge, no other regime in the world has methodically and violently forced half of its population into virtual house arrest, prohibiting them on pain of physical punishment.[254]Brutal repression of women was widespread under the Taliban and faced significant international condemnation.[137][255][256][257][258][259][260][261][262] Abuses were myriad and violently enforced by the religious police.[263] For example, the Taliban issued edicts forbidding women from being educated, forcing girls to leave schools and colleges.[264][265] Women leaving their houses were required to be accompanied by a male relative and were obligated to wear the burqa, a traditional dress covering the entire body except for a small slit to see out of.[264][265] Those accused of disobeying were publicly beaten. In one instance, a young woman named Sohaila was charged with adultery after walking with a man who was not a relative; she was publicly flogged in Ghazi Stadium, receiving 100 lashes.[266] Female employment was restricted to the medical sector, where male medical personnel were prohibited from treating women and girls.[264] This extensive ban on the employment of women further resulted in the widespread closure of primary schools, as almost all teachers prior to the Taliban's rise had been women, further restricting access to education not only to girls but also to boys. Restrictions became especially severe after the Taliban took control of the capital. In February 1998, for instance, religious police forced all women off the streets of Kabul and issued new regulations ordering people to blacken their windows so that women would not be visible from outside.[267]
According to the United Nations, the Taliban and its allies were responsible for 76% of civilian casualties in Afghanistan in 2009, 75% in 2010 and 80% in 2011.[73][268]
According to Human Rights Watch, the Taliban's bombings and other attacks which have led to civilian casualties ""sharply escalated in 2006"" when ""at least 669 Afghan civilians were killed in at least 350 armed attacks, most of which appear to have been intentionally launched at non-combatants.""[269][270]
The United Nations reported that the number of civilians killed by both the Taliban and pro-government forces in the war rose nearly 50% between 2007 and 2009.  The high number of civilians killed by the Taliban is blamed in part on their increasing use of improvised explosive devices (IEDs), ""for instance, 16 IEDs have been planted in girls' schools"" by the Taliban.[271]
In 2009, Colonel Richard Kemp, formerly Commander of British forces in Afghanistan and the intelligence coordinator for the British government, drew parallels between the tactics and strategy of Hamas in Gaza to those of the Taliban. Kemp wrote:
Like Hamas in Gaza, the Taliban in southern Afghanistan are masters at shielding themselves behind the civilian population and then melting in among them for protection. Women and children are trained and equipped to fight, collect intelligence, and ferry arms and ammunition between battles. Female suicide bombers are increasingly common. The use of women to shield gunmen as they engage NATO forces is now so normal it is deemed barely worthy of comment. Schools and houses are routinely booby-trapped. Snipers shelter in houses deliberately filled with women and children.[272][273]Taliban between 2008 and 2012 several times claimed to have assassinated Western and Afghani medical or aid workers in Afghanistan, either for fear of the vaccination of children against polio, or for suspicion that the 'medical workers' were in truth spies, or for suspecting them to be proselytising Christianity.
In August 2008, three Western women (British, Canadian, US) working for aid group 'International Rescue Committee' were murdered in Kabul. Taliban claimed to have killed them because they were foreign spies.[274] In October 2008, the British woman Gayle Williams working for Christian UK charity 'Serve Afghanistan' – focusing on training and education for disabled persons – was murdered near Kabul. Taliban claimed they killed her because her organisation ""was preaching Christianity in Afghanistan"".[274]  In all 2008 until October, 29 aid workers, 5 of whom non-Afghanis, were killed in Afghanistan.[274]
In August 2010, the Taliban claimed to have murdered 10 medical aid workers passing through Badakhshan Province on the way from Kabul to Nuristan Province — but also Afghan Islamic party/militia Hezb-e Islami Gulbuddin has claimed those killings. The victims were six Americans, one Briton, one German and two Afghanis, working for self-proclaimed ""non-profit, Christian organization"" called 'International Assistance Mission'. Taliban said they murdered them because of proselytising Christianity, having Bibles translated in Dari language in their possession when they were encountered. IAM contended afterwards that they ""were not missionaries"".[275]
In December 2012, unidentified gunmen killed four female UN polio-workers in Karachi in Pakistan; Western news media suggested a connection with the outspoken Taliban objections against and suspicions about such 'polio vaccinations'.[276] Eventually in 2012, a Pakistani Taliban commander in North Waziristan in Pakistan banned polio vaccinations,[277]  and in March 2013, the Afghan government was forced to suspend vaccination efforts from the Nuristan Province because of a large Taliban influence in the province.[278] However, in May 2013, Taliban leaders changed their stance on polio vaccination, saying the vaccine is the only way to prevent polio and that they would work with immunisation volunteers so long as polio workers are ""unbiased"" and ""harmonised with the regional conditions, Islamic values and local cultural traditions.""[279][280]
Before the Taliban came to power, education was highly regarded in Afghanistan and Kabul University attracted students from across Asia and the Middle East. However, the Taliban imposed restrictions on modern education, banned female education and encouraged only Islamic religious schools and the teaching of the Quran. Around half of the schools in Afghanistan were destroyed.[281] The Taliban have carried out brutal attacks on teachers and students and issued threats to parents and teachers.[282]
As per a 1998 UNICEF report, 9 out of 10 girls and 2 out of 3 boys did not enroll in schools. By 2000, fewer than 4-5% of Afghan children were being educated at the primary school level and even fewer at higher secondary and university levels.[281]
Attacks on educational institutions, students and teachers and the forced enforcement of Islamic teachings have continued even after the Taliban were deposed from power. In December 2017, United Nations Office for the Coordination of Humanitarian Affairs (OCHA) reported that over 1,000 schools had been destroyed, damaged or occupied and 100s of teachers and students been killed by the Taliban.[283]
The Taliban have inflicted Cultural genocide on the Afghan people by destroying their historical and cultural texts, artifacts and sculptures.
In 1992, it attacked and looted the National Museum of Afghanistan resulting in loss of 70% of the 100,000 artifacts of Afghan culture and history.[284]
On 11 August 1998, it destroyed the Puli Khumri Public Library. The library contained over 55,000 books and old manuscripts and was considered by Afghans as one of the most valuable and beautiful collections of their nation and their culture.[285][286]
On 2 March 2002, the Buddhas of Bamiyan were destroyed with dynamite, on orders from its leader Mullah Omar.[287]
In October of the same year, it destroyed at least 2,750 ancient works of art at the National Museum of Afghanistan.[288]
Afghanistan has had a rich musical culture, where music plays an important part in social functions like births and marriages and has also played a big role in uniting an ethnically diverse country.[289] However, since coming to power and even after being deposed, the Taliban has banned all music including cultural folk music and has attacked and killed a number of musicians.[289][290][291][292]
During the Taliban rule of 1996–2001, they banned many recreational activities and games, such as football, kite flying, and chess. General entertainment such as televisions, cinemas, music, VCRs and satellite dishes were also banned.[293] It has been reported that when children were caught kiting, a highly popular activity among Afghan children, they would have gotten beaten.[67]
Also included in the list of banned items were ""musical instruments and accessories"" and all visual representation of living creatures.[289]
Political
Militant
 Islam portal
The Taliban's ideology has been described as an ""innovative form of sharia combining Pashtun tribal codes,""[294] or Pashtunwali, with radical Deobandi interpretations of Islam favoured by JUI and its splinter groups. Also contributing to the mix was the militant Islamism and extremist jihadism of Osama bin Laden.[295] Their ideology was a departure from the Islamism of the anti-Soviet mujahideen rulers[clarification needed] they replaced who tended to be mystical Sufis, traditionalists,[clarification needed] or radical Islamists[clarification needed] inspired by the Muslim Brotherhood (Ikhwan).[296]
According to journalist Ahmed Rashid, at least in the first years of their rule, the Taliban adopted Deobandi and Islamist anti-nationalist beliefs, and opposed ""tribal and feudal structures,"" eliminating traditional tribal or feudal leaders from leadership roles.[297]
The Taliban strictly enforced their ideology in major cities like Herat, Kabul, and Kandahar. But in rural areas the Taliban had little direct control, and promoted village jirgas, so it did not enforce its ideology as stringently in rural areas.[298]
The Taliban regime interpreted the Sharia law in accordance with the Hanafi school of Islamic jurisprudence and the religious edicts of Mullah Omar.[50] The Taliban forbade pork and alcohol, many types of consumer technology such as music,[299] television,[299] filming,[299] and the Internet, as well as most forms of art such as paintings or photography,[299] male and female participation in sport,[300] including football and chess;[300] recreational activities such as kite-flying and keeping pigeons or other pets were also forbidden, and the birds were killed according to the Taliban's ruling.[300] Movie theatres were closed and repurposed as mosques.[300] Celebration of the Western and Iranian New Year was forbidden.[301] Taking photographs and displaying pictures or portraits was forbidden, as it was considered by the Taliban as a form of idolatry.[300] Women were banned from working,[302] girls were forbidden to attend schools or universities,[302] were requested to observe purdah and to be accompanied outside their households by male relatives; those who violated these restrictions were punished.[302] Men were forbidden to shave their beards and required to let them grow and keep them long according to the Taliban's liking, and to wear turbans outside their households.[303][304] Prayer was made compulsory and those who didn't respect the religious obligation after the azaan were arrested.[303] Gambling was banned,[301] and thieves were punished by amputating their hands or feet.[300] In 2000, the Taliban leader Mullah Omar officially banned opium cultivation and drug trafficking in Afghanistan;[305][306][307] the Talibans succeeded in nearly eradicating the majority of the opium production (99%) by 2001.[306][307][308] Under the Taliban governance of Afghanistan, both drug users and dealers were severely prosecuted.[305]
The Taliban emphasised dreams as a means of revelation.[309] Like Wahhabis and other Deobandis, the Taliban do not consider Shiites to be Muslims. The Shia in Afghanistan consist mostly of the Hazara ethnic group, which totalled almost 10% of Afghanistan's population and were persecuted during Taliban rule.[310] However, a few Shiite Islamists did support Taliban rule, such as Ustad Muhammad Akbari.[311] In recent years, the Taliban have attempted to court Shiites, appointing a Shiite cleric as a regional governor and recruiting Hazaras to fight against ISIL-KP, in order to distance themselves from their past sectarian reputation and improve relations with the Shiite government of Iran.[312]
Along with Shiite Muslims, the small Christian community was also persecuted by the Taliban.[313] The Taliban announced in May 2001 that it would enforce badges on Afghanistan's Hindu population, which has been compared to the treatment of Jews in Nazi Germany.[69] The Sikhs of Afghanistan were generally more tolerated by the Taliban compared to Shiites, Hindus and Christians.[314] The last remaining Jews of Afghanistan during their rule, Zablon Simintov and Isaac Levy, both spent time in prison for continuous ""arguing"" but were later released from prison when Taliban officials became annoyed with their arguing.[315]
The Taliban were averse to debating doctrine with other Muslims. ""The Taliban did not allow even Muslim reporters to question [their] edicts or to discuss interpretations of the Qur'an.""[124]
The Taliban frequently used the pre-Islamic Pashtun tribal code, Pashtunwali, in deciding certain social matters. Such is the case with the Pashtun practice of dividing inheritances equally among sons, even though the Qur'an clearly states that women are to receive one-half a man's share.[316][317]
According to Ali A. Jalali and Lester Grau, the Taliban ""received extensive support from Pashtuns across the country who thought that the movement might restore their national dominance. Even Pashtun intellectuals in the West, who differed with the Taliban on many issues, expressed support for the movement on purely ethnic grounds.""[318]
In 1999, Mullah Omar issued a decree protecting the Buddha statues at Bamyan, two 6th-century monumental statues of standing buddhas carved into the side of a cliff in the Bamyan valley in the Hazarajat region of central Afghanistan. But in March 2001, the statues were destroyed by the Taliban of Mullah Omar, following a decree stating: ""all the statues around Afghanistan must be destroyed.""[319]
Yahya Massoud, brother of the anti-Taliban and resistance leader Ahmad Shah Massoud, recalls the following incident after the destruction of the Buddha statues at Bamyan:
It was the spring of 2001. I was in Afghanistan's Panjshir Valley, together with my brother Ahmad Shah Massoud, the leader of the Afghan resistance against the Taliban, and Bismillah Khan, who currently serves as Afghanistan's interior minister. One of our commanders, Commandant Momin, wanted us to see 30 Taliban fighters who had been taken hostage after a gun battle. My brother agreed to meet them.
I remember that his first question concerned the centuries-old Buddha statues that were dynamited by the Taliban in March of that year, shortly before our encounter. Two Taliban combatants from Kandahar confidently responded that worshiping anything outside of Islam was unacceptable and that therefore these statues had to be destroyed. My brother looked at them and said, this time in Pashto, 'There are still many sun- worshippers in this country. Will you also try to get rid of the sun and drop darkness over the Earth?'[320]The Taliban ideology was not static. Before its capture of Kabul, members of the Taliban talked about stepping aside once a government of ""good Muslims"" took power and law and order were restored. The decision making process of the Taliban in Kandahar was modelled on the Pashtun tribal council (jirga), together with what was believed to be the early Islamic model. Discussion was followed by a building of a consensus by the believers.[321]
As the Taliban's power grew, decisions were made by Mullah Omar without consulting the jirga and without Omar's visits to other parts of the country. He visited the capital, Kabul, only twice while in power.  Taliban spokesman Mullah Wakil explained:
Decisions are based on the advice of the Amir-ul Momineen. For us consultation is not necessary. We believe that this is in line with the Sharia. We abide by the Amir's view even if he alone takes this view. There will not be a head of state. Instead there will be an Amir al-Mu'minin. Mullah Omar will be the highest authority and the government will not be able to implement any decision to which he does not agree. General elections are incompatible with Sharia and therefore we reject them.[322]Another evolution of Taliban ideology was Mullah Omar 1999 decree calling for the protection of the Buddha statues at Bamyan and the March 2001 destruction of them.[323]
The author Ahmed Rashid suggests that the devastation and hardship of the Soviet invasion and the following period influenced Taliban ideology.[324] It is said that the Taliban did not include scholars learned in Islamic law and history. The refugee students, brought up in a totally male society, not only had no education in mathematics, science, history or geography, but also had no traditional skills of farming, herding, or handicraft-making, nor even knowledge of their tribal and clan lineages.[324] In such an environment, war meant employment, peace meant unemployment. Dominating women simply affirmed manhood. For their leadership, rigid fundamentalism was a matter not only of principle, but also of political survival. Taliban leaders ""repeatedly told"" Rashid that ""if they gave women greater freedom or a chance to go to school, they would lose the support of their rank and file.""[325]
The Taliban have been criticised for their strictness toward those who disobeyed their imposed rules, and Mullah Omar's taking of the title of Amir al-Mu'minin.
Mullah Omar was criticised for calling himself Amir al-Mu'minin on the grounds that he lacked scholarly learning, tribal pedigree, or connections to the Prophet's family. Sanction for the title traditionally required the support of all of the country's ulema, whereas only some 1,200 Pashtun Taliban-supporting Mullahs had declared Omar the Amir. According to Ahmed Rashid, ""no Afghan had adopted the title since 1834, when King Dost Mohammed Khan assumed the title before he declared jihad against the Sikh kingdom in Peshawar. But Dost Mohammed was fighting foreigners, while Omar had declared jihad against other Afghans.""[329]
Another criticism was that the Taliban called their 20% tax on truckloads of opium ""zakat"", which is traditionally limited to 2.5% of the zakat-payers' disposable income (or wealth).[329]
Taliban have been compared to the 7th-century Kharijites for developing extreme doctrines that set them apart from both mainstream Sunni and Shiʿa Muslims. The Kharijites were particularly noted for adopting a radical approach to takfir, whereby they declared other Muslims to be unbelievers and therefore deemed them worthy of death.[330][331][332]
In particular the Taliban have been accused of takfir towards Shia. After the August 1998 slaughter of 8000 mostly Shia Hazaras non-combatants at Mazar-i-Sharif,  Mullah Niazi, the Taliban commander of the attack and the new governor of Mazar, declared from Mazar's central mosque:
Last year you rebelled against us and killed us. From all your homes you shot at us. Now we are here to deal with you. The Hazaras are not Muslims and now have to kill Hazaras. You either accept to be Muslims or leave Afghanistan. Wherever you go we will catch you. If you go up we will pull you down by your feet; if you hide below, we will pull you up by your hair.[333]Until his death in 2013, Mullah Mohammed Omar was the supreme commander of the Taliban. Mullah Akhtar Mansour was elected as his replacement in 2015,[334] and following Mansour's killing in a May 2016 US drone strike, Mawlawi Hibatullah Akhundzada became the group's leader.[47]
The Taliban initially enjoyed goodwill from Afghans weary of the warlords' corruption, brutality, and incessant fighting.[335]
This popularity was not universal, particularly among non-Pashtuns.
In 2001, the Taliban, de jure, controlled 85% of Afghanistan. De facto the areas under its direct control were mainly Afghanistan's major cities and highways. Tribal khans and warlords had de facto direct control over various small towns, villages, and rural areas.[336]
Rashid described the Taliban government as ""a secret society run by Kandaharis ... mysterious, secretive, and dictatorial.""[337] They did not hold elections, as their spokesman explained:
The Sharia does not allow politics or political parties. That is why we give no salaries to officials or soldiers, just food, clothes, shoes, and weapons. We want to live a life like the Prophet lived 1400 years ago, and jihad is our right. We want to recreate the time of the Prophet, and we are only carrying out what the Afghan people have wanted for the past 14 years.[338]They modelled their decision-making process on the Pashtun tribal council (jirga), together with what they believed to be the early Islamic model. Discussion was followed by a building of a consensus by the ""believers"".[321] Before capturing Kabul, there was talk of stepping aside once a government of ""good Muslims"" took power, and law and order were restored.
As the Taliban's power grew, decisions were made by Mullah Omar without consulting the jirga and without consulting other parts of the country. He visited the capital, Kabul, only twice while in power. Instead of an election, their leader's legitimacy came from an oath of allegiance (""Bay'ah""), in imitation of the Prophet and the first four Caliphs. On 4 April 1996, Mullah Omar had ""the Cloak of the Prophet Mohammed"" taken from its shrine for the first time in 60 years. Wrapping himself in the relic, he appeared on the roof of a building in the center of Kandahar while hundreds of Pashtun mullahs below shouted ""Amir al-Mu'minin!"" (Commander of the Faithful), in a pledge of support. Taliban spokesman Mullah Wakil explained:
Decisions are based on the advice of the Amir-ul Momineen. For us consultation is not necessary. We believe that this is in line with the Sharia. We abide by the Amir's view even if he alone takes this view. There will not be a head of state. Instead there will be an Amir al-Mu'minin. Mullah Omar will be the highest authority, and the government will not be able to implement any decision to which he does not agree. General elections are incompatible with Sharia and therefore we reject them.[322]The Taliban were very reluctant to share power, and since their ranks were overwhelmingly Pashtun they ruled as overlords over the 60% of Afghans from other ethnic groups. In local government, such as Kabul city council[337] or Herat,[339] Taliban loyalists, not locals, dominated, even when the Pashto-speaking Taliban could not communicate with the roughly half of the population who spoke Dari or other non-Pashtun tongues.[339] Critics complained that this ""lack of local representation in urban administration made the Taliban appear as an occupying force.""[130]
Consistent with the governance of early Muslims was the absence of state institutions or ""a methodology for command and control"" that is standard today even among non-Westernized states. The Taliban did not issue press releases, policy statements, or hold regular press conferences. The outside world and most Afghans did not even know what their leaders looked like, since photography was banned.[340] The ""regular army"" resembled a lashkar or traditional tribal militia force with only 25,000 men (of whom 11,000 were non-Afghans).
Cabinet ministers and deputies were mullahs with a ""madrasah education."" Several of them, such as the Minister of Health and Governor of the State bank, were primarily military commanders who left their administrative posts to fight when needed. Military reverses that trapped them behind lines or led to their deaths increased the chaos in the national administration.[341] At the national level, ""all senior Tajik, Uzbek and Hazara bureaucrats"" were replaced ""with Pashtuns, whether qualified or not."" Consequently, the ministries ""by and large ceased to function.""[130]
The Ministry of Finance had neither a budget nor ""qualified economist or banker."" Mullah Omar collected and dispersed cash without bookkeeping.
According to the testimony of Guantanamo captives before their Combatant Status Review Tribunals, the Taliban, in addition to conscripting men to serve as soldiers, also conscripted men to staff its civil service.[342]
The Kabul money markets responded positively during the first weeks of the Taliban occupation (1996). But the Afghani soon fell in value.  They imposed a 50% tax on any company operating in the country, and those who failed to pay were attacked.  They also imposed a 6% import tax on anything brought into the country,  and by 1998 had control of the major airports and border crossings which allowed them to establish a monopoly on all trade.  By 2001 the per capita income of the 25 million population was under $200,  and the country was close to total economic collapse.  As of 2007 the economy had begun to recover, with estimated foreign reserves of three billion dollars and a 13% increase in economic growth.[260][343][344][345][346][347]
Under the Transit treaty between Afghanistan and Pakistan a massive network for smuggling developed. It had an estimated turnover of 2.5 billion dollars with the Taliban receiving between $100 and $130 million per year. These operations along with the trade from the Golden Crescent financed the war in Afghanistan and also had the side effect of destroying start up industries in Pakistan. Ahmed Rashid also explained that the Afghan Transit Trade agreed on by Pakistan was ""the largest official source of revenue for the Taliban.""[348][349][350]
Between 1996 and 1999 Mullah Omar reversed his opinions on the drug trade, apparently as it only harmed kafirs. The Taliban controlled 96% of Afghanistan's poppy fields and made opium its largest source of taxation. Taxes on opium exports became one of the mainstays of Taliban income and their war economy. According to Rashid, ""drug money funded the weapons, ammunition and fuel for the war."" In The New York Times, the Finance Minister of the United Front, Wahidullah Sabawoon, declared the Taliban had no annual budget but that they ""appeared to spend US$300 million a year, nearly all of it on war."" He added that the Taliban had come to increasingly rely on three sources of money: ""poppy, the Pakistanis and bin Laden.""[350]
In an economic sense it seems he had little choice, as the war of attrition continued with the Northern Alliance the income from continued opium production was all that prevented the country from starvation. By 2000 Afghanistan accounted for an estimated 75% of the world's supply and in 2000 grew an estimated 3276 tonnes of opium from poppy cultivation on 82,171 hectares.  At this juncture Omar passed a decree banning the cultivation of opium, and production dropped to an estimated 74 metric tonnes from poppy cultivation on 1,685 hectares.  Many observers say the ban – which came in a bid for international recognition at the United Nations – was only issued in order to raise opium prices and increase profit from the sale of large existing stockpiles.  1999 had yielded a record crop and had been followed by a lower but still large 2000 harvest. The trafficking of accumulated stocks by the Taliban continued in 2000 and 2001. In 2002, the UN mentioned the ""existence of significant stocks of opiates accumulated during previous years of bumper harvests."" In September 2001 – before the 11 September attacks against the United States – the Taliban allegedly authorised Afghan peasants to sow opium again.[350][351][352][353]
There was also an environmental toll to the country, heavy deforestation from the illegal trade in timber with hundreds of acres of pine and cedar forests in Kunar Province and Paktya being cleared. Throughout the country millions of acres were denuded to supply timber to the Pakistani markets, with no attempt made at reforestation, which has led to significant environmental damage. By 2001, when the Afghan Interim Administration took power the country's infrastructure was in ruins, Telecommunications had failed, the road network was destroyed and Ministry of Finance buildings were in such a state of disrepair some were on the verge of collapse.  On 6 July 1999 then president Bill Clinton signed into effect executive order 13129. This order implemented a complete ban on any trade between America and the Taliban regime and on 10 August they froze £5000,000 in Ariana assets. On 19 December 2000 UN resolution 1333 was passed. It called for all assets to be frozen and for all states to close any offices belonging to the Taliban.  This included the offices of Ariana Afghan Airlines.  In 1999 the UN had passed resolution 1267 which had banned all international flights by Ariana apart from preapproved humanitarian missions.[354][355][356][357][358][359][360][361]
According to former US Secretary of State Hillary Clinton, ""Saudi Arabia remains a critical financial support base for al-Qaida, the Taliban, LeT and other terrorist groups... Donors in Saudi Arabia constitute the most significant source of funding to Sunni terrorist groups worldwide.""[362] Former CIA director James Woolsey described it as ""the soil in which Al-Qaeda and its sister terrorist organizations are flourishing.""[363]
According to the lawsuit, filed in December 2019 in the D.C. District Court on behalf of Gold Star families, some U.S. defense contractors involved in Afghanistan made illegal ""protection payments"" to the Taliban, funding a ""Taliban-led terrorist insurgency"" that killed or wounded thousands of Americans in Afghanistan.[364][365] In 2009, then-Secretary of State Hillary Clinton said that the ""protection money"" was ""one of the major sources of funding for the Taliban.""[366]
During its time in power (1996–2001), at its height ruling 90% of Afghanistan, the Taliban regime, or ""Islamic Emirate of Afghanistan"", gained diplomatic recognition from only three states: the United Arab Emirates, Pakistan, and Saudi Arabia, all of which provided substantial aid. The other nations, including the United Nations, recognised the government of the Islamic State of Afghanistan (1992–2002) (parts of whom were part of the United Front, also called Northern Alliance) as the legitimate government of Afghanistan. Regarding its relations with the rest of the world, the Taliban's Emirate of Afghanistan held a policy of isolationism: ""The Taliban believe in non-interference in the affairs of other countries and similarly desire no outside interference in their country's internal affairs"".[367]
Canada has designated the Taliban as a terrorist group.[368]
While China has been supporting the new government in Kabul both financially and politically, it is believed to have unofficial relations with the Taliban Government according to Malek Setiz, international relations adviser to the Foreign Ministry of Afghanistan.[369] Beijing's foreign ministry did not deny such interactions.[370]
India did not recognise the Taliban regime in Afghanistan and instead maintained close strategic and military ties with the Northern Alliance so as to contain the rise of Taliban during the 1990s. India was one of the closest allies of former Afghan president Mohammad Najibullah and strongly condemned his public execution by the Taliban. Pakistan and Kashmir-based militant groups thought to have ties with the Taliban have historically been involved in the Kashmir insurgency targeted against Indian security forces.[371][372][373][374]
In December 1999, Indian Airlines Flight 814 en route from Kathmandu to Delhi was hijacked and taken to Kandahar. The Taliban moved its militias near the hijacked aircraft, supposedly to prevent Indian special forces from storming the aircraft, and stalled the negotiations between India and the hijackers for days. The New York Times later reported that there were credible links between the hijackers and the Taliban. As a part of the deal to free the plane, India released three militants. The Taliban gave a safe passage to the hijackers and the released militants.[375][376]
Following the hijacking, India drastically increased its efforts to help Massoud, providing an arms depot in Dushanbe, Tajikistan.  India also provided a wide range of high-altitude warfare equipment, helicopter technicians, medical services, and tactical advice.  According to one report, Indian military support to anti-Taliban forces totalled US$70 million, including five Mil Mi-17 helicopters, and US$8 million worth of high-altitude equipment in 2001.  India extensively supported the new administration in Afghanistan,  leading several reconstruction projects and by 2001 had emerged as the country's largest regional donor.[377][378][379][380][381][382]
In the wake of terrorist attacks in India, there have been growing concerns about fundamentalist organisations such as the Taliban seeking to expand their activities into India. During the 2011 ICC Cricket World Cup which was co-hosted in India, Pakistani Interior Minister Rehman Malik and Interpol chief Ronald Noble revealed that a terrorist bid to disrupt the tournament had been foiled; following a conference with Noble, Malik said that the Taliban had begun to base their activities in India with reports from neighbouring countries exposing their activities in the country and a Sri Lankan terrorist planning to target cricketers was arrested in Colombo. In 2009, the Times of India called for India to reassess its Taliban threat.[383][384][385][386]
In 2012, Taliban said that they want to have cordial relations with India, and praised India for resisting the U.S. calls for more military involvement in Afghanistan.[387]
Iran has historically been an enemy of the Taliban. In early August 1998, after attacking the city of Mazar-i-Sharif, Taliban forces killed several thousand civilians[citation needed] and 11 Iranian diplomats and intelligence officers in the Iranian consulate. Alleged radio intercepts indicate Mullah Omar personally approved the killings.[388] In the following crisis between Iran and the Taliban, the Iranian government amassed up to 200,000 regular troops on the Afghan-Iranian border.[389] War was eventually averted.
Many US senior military officials such as Robert Gates,[390] Stanley McChrystal,[391] David Petraeus[392] and others believe that Iran's Islamic Revolutionary Guard Corps nowadays is involved in helping the Taliban to a certain extent. Reports in which NATO states accused Iran of supplying and training some Taliban insurgents started coming forward since 2004/2005.
We did interdict a shipment, without question the Revolutionary Guard's core Quds Force, through a known Taliban facilitator. Three of the individuals were killed... 48 122 millimetre rockets were intercepted with their various components... Iranians certainly view as making life more difficult for us if Afghanistan is unstable. We don't have that kind of relationship with the Iranians. That's why I am particularly troubled by the interception of weapons coming from Iran. But we know that it's more than weapons; it's money; it's also according to some reports, training at Iranian camps as well.[393]There are several sources as well stating the relationship between the Taliban and Iran in recent years. This said to occur from leadership change in the Taliban itself, with Akhtar Mansoor particularly seeking to improve ties with Iran.[22] Pro-Iran media outlets have also reported that the Taliban has included Shia Hazara fighters into its ranks.[394] The Taliban have also condemned ISIS linked attacks on the Hazara Shia minority.[395] In August 2019, The Washington Post reported that Iran's ""relationship with the Taliban now spans the economic, security and political realms and is likely to grow as the Taliban asserts itself again.""[396]
In August 2020, U.S. intelligence officials assessed that Iran had offered bounties to the Taliban-linked Haqqani network to kill foreign servicemembers, including Americans, in Afghanistan in 2019.[397] U.S. intelligence determined that Iran paid bounties to Taliban insurgents for the 2019 attack on Bagram airport.[398] According to CNN, Donald Trump's administration has ""never mentioned Iran's connection to the bombing, an omission current and former officials said was connected to the broader prioritization of the peace agreement and withdrawal from Afghanistan.""[397]
In January 2020, the Taliban condemned the U.S. killing of Iranian Quds Force commander Qasem Soleimani and hailed Soleimani as a ""great warrior"".[399]
Maulana Fazal-ur-Rehman, leader of the Pakistani Islamic (Deobandi) political party Jamiat Ulema-e Islam (F) (JUI), was an ally of Benazir Bhutto, Pakistani prime minister in 1993–1996, and then had access to the Pakistani government, army and the ISI, whom he influenced to help the Taliban.[400] The Pakistani Inter-Services Intelligence (ISI) has since 1994 heavily supported the Taliban, while the group conquered most of Afghanistan in 1994–98.[104][401][402][403]
Human Rights Watch writes, ""Pakistani aircraft assisted with troop rotations of Taliban forces during combat operations in late 2000 and ... senior members of Pakistan's intelligence agency and army were involved in planning military operations.""[404] Pakistan provided military equipment, recruiting assistance, training, and tactical advice.[405] Officially Pakistan denied supporting the Taliban militarily.
Author Ahmed Rashid claims that the Taliban had ""unprecedented access"" among Pakistan's lobbies and interest groups. He also writes that they at times were able to ""play off one lobby against another and extend their influence in Pakistan even further"".[406] By 1998–99, Taliban-style groups in Pakistan's Pashtun belt, and to an extent in Pakistan-administered Kashmir, ""were banning TV and videos ... and forcing people, particularly women, to adapt to the Taliban dress code and way of life.""[407]
After the attacks of 11 September 2001, and the US operation in Afghanistan the Afghan Taliban leadership is claimed to have fled to Pakistan where they regrouped and created several shuras to coordinate their insurgency in Afghanistan.[232]
Afghan officials implied the Pakistani ISI's involvement in a July 2008 Taliban attack on the Indian embassy. Numerous US officials have accused the ISI of supporting terrorist groups including the Afghan Taliban. US Defense Secretary Robert Gates and others suggest the ISI maintains links with groups like the Afghan Taliban as a ""strategic hedge"" to help Islamabad gain influence in Kabul once US troops exit the region.
US Chairman of the Joint Chiefs of Staff Admiral Mike Mullen in 2011 called the Haqqani network (the Afghan Taliban's most destructive element) a ""veritable arm of Pakistan's ISI"".[408][409]
From 2010, a report by a leading British institution also claimed that Pakistan's intelligence service still today has a strong link with the Taliban in Afghanistan. Published by the London School of Economics, the report said that Pakistan's Inter-Services Intelligence agency (ISI) has an ""official policy"" of support for the Taliban. It said the ISI provides funding and training for the Taliban, and that the agency has representatives on the so-called Quetta Shura, the Taliban's leadership council. It is alleged that the Quetta Shura is exiled in Quetta.  The report, based on interviews with Taliban commanders in Afghanistan, was written by Matt Waldman, a fellow at Harvard University.[232][410][411]
""Pakistan appears to be playing a double-game of astonishing magnitude,"" the report said. The report also linked high-level members of the Pakistani government with the Taliban. It said Asif Ali Zardari, the Pakistani president, met with senior Taliban prisoners in 2010 and promised to release them. Zardari reportedly told the detainees they were only arrested because of American pressure. ""The Pakistan government's apparent duplicity – and awareness of it among the American public and political establishment – could have enormous geopolitical implications,"" Waldman said. ""Without a change in Pakistani behaviour it will be difficult if not impossible for international forces and the Afghan government to make progress against the insurgency."" Afghan officials have long been suspicious of the ISI's role. Amrullah Saleh, the former director of Afghanistan's intelligence service, told Reuters that the ISI was ""part of a landscape of destruction in this country"".[412]
Pakistan, at least up to 2011, has always strongly denied all links with Taliban.[413][414][415][416][417][418]
On 15 June 2014 Pakistan army launches operation 'Zarb-e-Azb' in North Waziristan to remove and root-out Taliban from Pakistan. In this operation 327 hardcore terrorists had been killed while 45 hideouts and 2 bomb making factories of terrorists were destroyed in North Waziristan Agency as the operation continues.[419][420][421]
Saudi Arabia has been accused of supporting Taliban.[422] In a December 2009 diplomatic cable to U.S. State Department staff (made public in the diplomatic cable leaks the following year), U.S. Secretary of State Hillary Clinton urged U.S. diplomats to increase efforts to block money from Gulf Arab states from going to terrorists in Pakistan and Afghanistan, writing that ""Donors in Saudi Arabia constitute the most significant source of funding to Sunni terrorist groups worldwide"" and that ""More needs to be done since Saudi Arabia remains a critical financial support base for  al-Qaeda, the Taliban, LeT and other terrorist groups.""[423]
Qatar in 2013, with the approval of the US and the Afghan government, allowed the Afghan Taliban to set up a diplomatic and political office inside the country.[424][425] This was done in order to facilitate peace negotiations and with the support of other countries.[426][424]
Ahmed Rashid, writing in the Financial Times, stated that through the office Qatar has facilitated meetings between the Taliban and many countries and organisations, including the US state department, the UN, Japan, several European governments and non-governmental organisations, all of whom have been trying to push forward the idea of peace talks.[426]
In July 2017, Saudi Arabia, at the time in severe conflict with Qatar, without corroboration alleged Qatar to support terrorism including Taliban ""armed terrorists"".[424]
In September 2017, the presidents of both the United States and Afghanistan demanded Qatar to close down the office of the Taliban.[427] But in February 2020, Qatar facilitated a peace agreement between the United States and the Taliban. According to the agreement, the Taliban will cut all its connections with Al-Qaeda and begin peace negotiations with the Afghani Government. In return the United States will begin the withdrawal of its troops. They will have withdrawn all its troops in 14 months.[428]
Russia has been accused of arming the Taliban by multiple politicians including Rex Tillerson and the Afghan government.[429] There is no public evidence to substantiate such allegations, and several independent experts are sceptical that Russia materially supported the Taliban in any way.[430] According to the BBC, Russia ""is deeply concerned about the rise of Islamist fundamentalism in the region spreading in its direction. And it sees the Taliban as one potential bulwark against this.""[431]
In February and again in May 2019, a delegation of Taliban officials and senior Afghan politicians met in Moscow to hold a new round of Afghan peace talks.[432][433] Reuters reported that ""Russian officials as well as religious leaders and elders had asked for a ceasefire.""[434]
In June 2020, U.S. intelligence officials assessed with medium confidence that the Russian GRU military-intelligence agency had offered bounties to the Taliban militants to kill coalition forces in Afghanistan.[435][436] The Pentagon's top leaders said that Russian bounty program has not been corroborated.[437]
After the 9/11 attacks, the United Kingdom froze the Taliban's assets in the UK, nearly $200 million by early October 2001.  The UK also supported the US decision to remove the Taliban, both politically and militarily.[438][439]
The UN agreed that NATO would act on its behalf, focusing on counter-terrorist operations in Afghanistan after the Taliban had been ""defeated"". The United Kingdom took operational responsibility for Helmand Province, a major poppy-growing province in southern Afghanistan, deploying troops there in mid-2006, and encountered resistance by re-formed Taliban forces allegedly entering Afghanistan from Pakistan. The Taliban turned towards the use of improvised explosive devices.[440]
During 2008 the British government announced plans to pay Taliban fighters to switch sides or lay down their arms; the proceeding year they signalled their support of opening negotiations with the Taliban.[441][442]
The United States never recognised the Taliban government in Afghanistan. Ahmed Rashid states that the US indirectly supported the Taliban through its ally in Pakistan between 1994 and 1996 because Washington viewed the Taliban as anti-Iranian, anti-Shia and potentially pro-Western.  Washington furthermore hoped that the Taliban would support development planned by the US-based oil company Unocal. For example, it made no comment when the Taliban captured Herat in 1995, and expelled thousands of girls from schools. In late 1997, American Secretary of State Madeleine Albright began to distance the US from the Taliban, and the American-based oil company Unocal withdrew from negotiations on pipeline construction from Central Asia.[443][444][445][446]
One day before the August 1998 capture of Mazar, bin Laden affiliates bombed two US embassies in Africa, killing 224 and wounding 4,500, mostly Africans. The US responded by launching cruise missiles on suspected terrorist camps in Afghanistan, killing over 20 though failing to kill bin Laden or even many Al-Qaeda. Mullah Omar condemned the missile attack and American President Bill Clinton. Saudi Arabia expelled the Taliban envoy in protest over the refusal to turn over bin Laden, and after Mullah Omar allegedly insulted the Saudi royal family. In mid-October the UN Security Council voted unanimously to ban commercial aircraft flights to and from Afghanistan, and freeze its bank accounts worldwide.[447][448][449]
Adjusting its counterinsurgency strategy, in October 2009, the US announced plans to pay Taliban fighters to switch sides.[450]
On 26 November 2009, in an interview with CNN's Christiane Amanpour, President Hamid Karzai said there is an ""urgent need"" for negotiations with the Taliban, and made it clear that the Obama administration had opposed such talks. There was no formal American response.[451][452]
In December 2009, Asian Times Online reported that the Taliban had offered to give the US ""legal guarantees"" that they would not allow Afghanistan to be used for attacks on other countries, and that there had been no formal American response.[236]
On 6 December, US officials indicated that they have not ruled out talks with the Taliban. Several days later it was reported that Gates saw potential for reconciliation with the Taliban, but not with Al-Qaeda. Furthermore, he said that reconciliation would politically end the insurgency and the war. But he said reconciliation must be on the Afghan government's terms, and that the Taliban must be subject to the sovereignty of the government.[453][454]
In 2010, General McChrystal said his troop surge could lead to a negotiated peace with the Taliban.[455]
In an interview with Palgrave Macmillan about relations between the US and the Taliban, American academic Dr. Jonathan Cristol argued that Taliban leaders ""have been willing to negotiate, but from a position of relative strength and their goal is no longer a warm relationship with the US—that ship sailed long ago.""[456] In March 2020, the USA began a gradual withdrawal of its troops, to which they have agreed in a peace accord with the Taliban.[457]
On 29 February 2020, the Trump administration signed a conditional peace agreement with the Taliban,[458][459] which calls for the withdrawal of foreign troops in 14 months if the Taliban uphold the terms of the agreement.[460][461]
Despite the aid of United Nations (UN) and non-governmental organisations (NGOs) given (see § Afghanistan during Taliban rule), the Taliban's attitude in 1996–2001 toward the UN and NGOs was often one of suspicion. The UN did not recognise the Taliban as the legitimate government of Afghanistan, most foreign donors and aid workers were non-Muslims, and the Taliban vented fundamental objections to the sort of 'help' the UN offered. As the Taliban's Attorney General Maulvi Jalil-ullah Maulvizada put it in 1997:
Let us state what sort of education the UN wants. This is a big infidel policy which gives such obscene freedom to women which would lead to adultery and herald the destruction of Islam. In any Islamic country where adultery becomes common, that country is destroyed and enters the domination of the infidels because their men become like women and women cannot defend themselves. Anyone who talks to us should do so within Islam's framework. The Holy Koran cannot adjust itself to other people's requirements, people should adjust themselves to the requirements of the Holy Koran.[462]In July 1998, the Taliban closed ""all NGO offices"" by force after those organisations refused to move to a bombed-out former Polytechnic College as ordered.[133] One month later the UN offices were also shut down.[134]
Around 2000, the UN drew up sanctions against officials and leaders of Taliban, because of their harbouring Osama bin Laden. Several of the Taliban leaders have subsequently been killed.[463]
In 2009, British Foreign Secretary Ed Miliband and US Secretary Hillary Clinton had called for talks with 'regular Taliban fighters' while bypassing their top leaders who supposedly were 'committed to global jihad'. Kai Eide, the top UN official in Afghanistan, called for talks with Taliban at the highest level, suggesting Mullah Omar—even though Omar had recently dismissed such overtures as long as foreign troops were in Afghanistan.[464]
In 2010, the UN lifted sanctions on the Taliban, and requested that Taliban leaders and others be removed from terrorism watch lists. In 2010 the US and Europe announced support for President Karzai's latest attempt to negotiate peace with the Taliban.[463][465][466]
In 1996, bin Laden moved to Afghanistan from Sudan. He came without invitation, and sometimes irritated Mullah Omar with his declaration of war and fatwas against citizens of third-party countries,  but relations between the two groups improved over time, to the point that Mullah Omar rebuffed his group's patron Saudi Arabia, insulting Saudi minister Prince Turki while reneging on an earlier promise to turn bin Laden over to the Saudis.[467][468]
Bin Laden was able to forge an alliance between the Taliban and al-Qaeda. The al-Qaeda-trained 055 Brigade integrated with the Taliban army between 1997 and 2001. Several hundred Arab and Afghan fighters sent by bin Laden assisted the Taliban in the Mazar-e-Sharif slaughter in 1998.[469] From 1996 to 2001, the organisation of Osama Bin Laden and Ayman al-Zawahiri had become a virtual state within the Taliban state. The British newspaper The Telegraph stated in September 2001 that 2,500 Arabs under command of Bin Laden fought for the Taliban.[89]
Taliban-al-Qaeda connections were also strengthened by the reported marriage of one of bin Laden's sons to Omar's daughter. While in Afghanistan, bin Laden may have helped finance the Taliban.[470][471]
After the 1998 US embassy bombings in Africa, bin Laden and several al-Qaeda members were indicted in U.S. criminal court.  The Taliban rejected extradition requests by the US, variously claiming that bin Laden had ""gone missing"",  or that Washington ""cannot provide any evidence or any proof"" that bin Laden is involved in terrorist activities and that ""without any evidence, bin Laden is a man without sin... he is a free man.""[472][473][474][475]
Evidence against bin Laden included courtroom testimony and satellite phone records.  Bin Laden, in turn, praised the Taliban as the ""only Islamic government"" in existence, and lauded Mullah Omar for his destruction of idols such as the Buddhas of Bamyan.[476][477][478]
At the end of 2008, the Taliban was in talks to sever all ties with al-Qaeda.[479]
In 2011, Alex Strick van Linschoten and Felix Kuehn at New York University's Center on International Cooperation claimed that the two groups did not get along at times before the 11 September attacks, and they have continued to fight since on account of their differences.[480]
In July 2012, an anonymous senior-ranking Taliban commander stated that ""Our people consider al-Qaeda to be a plague that was sent down to us by the heavens. Some even concluded that al-Qaeda are actually the spies of America. Originally, the Taliban were naive and ignorant of politics and welcomed al-Qaeda into their homes. But al-Qaeda abused our hospitality."" He went on to further claim that about 70% of the Taliban are angry with al-Qaeda, revealing the icy relationship between the two groups.[481][482]
Malakand Taliban is a militant outfit led by Sufi Muhammad and his son in law Molvi Fazalullah.  Sufi Muhammad is in Pakistani government custody; Molvi Fazalullah is believed to be in Afghanistan.  In the last week of May 2011, eight security personnel and civilians fell victim to four hundred armed Taliban who attacked Shaltalo check post in Dir, a frontier District of Khyber Pakhtunkhwa, located a few kilometres away from the Afghan border. Although they have been linked with Waziristan-based Tehreek-e-Taliban Pakistan (TTP), the connection between these two groups was of a symbolic nature.[483]
Before the creation of the Tehrik-i-Taliban (Pakistan), some of their leaders and fighters were part of the 8,000 Pakistani militants fighting in the War in Afghanistan (1996–2001) and the War in Afghanistan (2001–present) against the United Islamic Front and NATO forces.[89] Most of them hail from the Pakistani side of the Af-Pak border regions. After the fall of the Afghan Taliban in late 2001 most Pakistani militants including members of today's TTP fled home to Pakistan.
After the creation of the Tehrik-i-Taliban Pakistan in 2007, headed by Baitullah Mehsud,  its members have officially defined goals to establish their rule over Pakistan's Federally Administered Tribal Areas. They engage the Pakistani army in heavy combat operations. Some intelligence analysts believe that the TTP's attacks on the Pakistani government, police and army strained the TTP's relations with the Afghan Taliban.[484][485][486]
The Afghan Taliban and the Tehrik-i-Taliban Pakistan differ greatly in their history, leadership and goals although they share a common interpretation of Islam and are both predominantly Pashtun.[485] The Afghan Taliban have no affiliation with the Tehrik-i-Taliban Pakistan and routinely deny any connection to the TTP. The New York Times quoted a spokesman for the Afghan Taliban stating that:
We don't like to be involved with them, as we have rejected all affiliation with Pakistani Taliban fighters ... We have sympathy for them as Muslims, but beside that, there is nothing else between us.[487]It is alleged that Afghan Taliban relied on support by the Pakistani army in the past and are still supported by them today in their campaign to control Afghanistan. Regular Pakistani army troops fought alongside the Afghan Taliban in the War in Afghanistan (1996–2001). Major leaders of the Afghan Taliban including Mullah Omar, Jalaluddin Haqqani and Siraj Haqqani are believed to enjoy or have enjoyed safe haven in Pakistan. In 2006 Jalaluddin Haqqani was allegedly called a 'Pakistani asset' by a senior official of Inter-Services Intelligence. Pakistan denies any links with Haqqani or other terrorist groups. Haqqani himself has denied any links with Pakistan as well.[153][87][119][488][489][490][491][492]
Afghan Taliban leader Mullah Omar asked the Tehrik-i-Taliban Pakistan in late 2008 and early 2009 to stop attacks inside Pakistan, to change their focus as an organisation and to fight the Afghan National Army and ISAF forces in Afghanistan instead.  In late December 2008 and early January 2009 he sent a delegation, led by former Guantanamo Bay detainee Mullah Abdullah Zakir, to persuade leading members of the TTP to put aside differences with Pakistan.[487]
Some regional experts state the common name ""Taliban"" may be more misleading than illuminating.[485]
Gilles Dorronsoro, a scholar of South Asia currently at the Carnegie Endowment for International Peace in Washington says:
The fact that they have the same name causes all kinds of confusion.[485]As the Pakistani Army began offensives against the Pakistani Taliban, many unfamiliar with the region thought incorrectly that the assault was against the Afghan Taliban of Mullah Omar which was not the case.[485]
The Pakistani Taliban were put under sanctions by UN Security Council for terrorist attacks in Pakistan and the 2010 Times Square car bombing attempt.[486]
"
Islamic republic - Wikipedia," 
Political
Militant
 Islam portal
An Islamic republic is a sovereign state that is officially ruled by Islamic laws and is contrasted to Islamic monarchy. As a name or title, four states are Islamic republics: Afghanistan, Iran, Mauritania and Pakistan. Pakistan first adopted the title under the constitution of 1956; Mauritania adopted it on 28 November 1958; Iran adopted it after the 1979 Iranian Revolution that overthrew the Pahlavi dynasty; and Afghanistan adopted it in 2004 after the fall of the Taliban government.
Despite sharing the ""Islamic republic"" name, the countries differ greatly in their governments and laws, and of the four only Iran is a religious theocratic state. As a term, it has come to mean several different things, some contradictory to others. To some Muslim religious leaders in the regions who advocate it, an Islamic republic is a state under a particular Islamic form of government. They see it as a compromise between a purely Islamic caliphate and a secular, nationalist republic. In their conception of the Islamic republic, the penal code of the state is required to be compatible with some or all laws of Sharia and the state may not be a monarchy as many Middle Eastern states are presently. Despite this, there are republics with Islam as a state religion and that are (at least partly) ruled by Islamic laws, but do not carry the ""Islamic republic"" name - examples include Iraq, Yemen, Sudan, Algeria, Maldives and Bangladesh.
Iran officially uses it as a title in all governance names referring to the country (e.g. the Islamic Republic of Iran Army or the Islamic Republic of Iran Broadcasting) as opposed to its equivalents in Afghanistan which are called the Afghan National Army and the Radio Television Afghanistan. Unlike the others, Iran also uses the IRI acronym of the Islamic Republic of Iran as part of official acronyms.
Afghanistan is an Islamic republic consisting of three branches, the executive, the legislative and the judicial. The nation is led by the president Ashraf Ghani, with Amrullah Saleh and Sarwar Danish as the vice presidents. The National Assembly is the legislature, a bicameral body having two chambers, the House of the People and the House of Elders. The Supreme Court is led by Chief Justice Said Yusuf Halem, the former Deputy Minister of Justice for Legal Affairs.[2][3]
Despite the Islamic name, the constitution formed in 2004 is very similar to the monarchy-era 1964 Constitution of Afghanistan.[4]
Two months after the Islamic Revolution in 1979, the new government held the Iranian Islamic Republic referendum on 10 and 11 Farvardin (30 and 31 March) to change the Pahlavi dynasty into an Islamic republic. On 12 Farvardin (1 April), it was announced that 98.2% of the Iranian voters wanted to establish the Islamic republic.[5][6]
Before the referendum, some political groups suggested various names for the ideology of the Iranian revolution such as the Republic (without Islam) or the Democratic Republic. Ruhollah Khomeini, the founder of the Islamic Republic of Iran, asked people to vote for the name Islamic Republic, not a word more and not a word less.[6][7]
According to the constitution, the Islamic Republic of Iran is a system based on the following beliefs:[8]
# the One God (as stated in the phrase ""There is no other god except God""), His exclusive sovereignty and right to legislate, and the necessity of submission to His commands;
According to a commentary on the constitution, just as the establishment of Islamic republic system is based on the beliefs of people, namely governing of right, justice and Quran. However, its continuation lasted with the same principles and there is an important role for the beliefs of Iranian people. Furthermore, those beliefs are of complete and determinate roles in all affairs. They are considered as guidelines for governors and statesmen. There is an important role for beliefs such as the principle of unity of God and believing in it.[9] In spite of that, there are other principles are to the submission in front of Allah and His order. Therefore, legislation is limited to Allah and laws so far as correspond to divine legislation are valid. Belief in divine revelation and prophecy are essential to Islamic worldview and there are two kinds of justice. The first kind is legislative (Tashri'i) and the other kind is creative (Takivini). Creative justice is based on justice and equality. Legislative justice is respected to making divine law in Islamic society. Besides, the basis of Shia school is in terms of imamate or leadership.
According to the principle of imamate in Shia, it is indispensable to obey of the prophet of Allah and of those possessed of authority. Shia clergy believes that the conception of the term ""those possessed of authority"" denoted on innocent Shia imams. When the Imam is absent, the valy faghih is in charge of leadership of society. In other words, religious leaders undertake the responsibility of the imamate. There is more emphasizing on the dignity (karamat) and the high value of humans which is along with freedom and responsibility. The principle of dignity is a necessary condition of the Islamic republic in terms of existence, but there are many meanings for the term dignity. Sometimes it refers to generosity, nobleness and honor, but Islam considers it two sorts of dignity for human beings, namely essential or innate dignity and acquired dignity. According to innate dignity, human being possessed of the right of living among other creatures. The principle is also mentioned by the Universal Declaration of Human Rights in 1948. On the basis of acquired dignity, the human is able to pass the degrees of perfection with the aid of actuality of his potentialities and talents.[10]
For the first time, Ruhollah Khomeini referred to the terms of Islamic republic for the Iranian people. He believes that the Iranian people want an Islamic state which is a republic. Responding to a journalist's question on the ambiguity of the term Islamic republic, Khomeini stated that the term republic has the same sense as other uses and Islamic republic has considered both Islamic ideology and the choice of people.[11]
The Islamic Republic of Mauritania is a country in the Maghreb region of western North Africa.[12][13][14]
Pakistan was the first country to adopt the adjective Islamic to modify its republican status under its otherwise secular constitution in 1956. Despite this definition, the country did not have a state religion until 1973, when a new constitution, more democratic and less secular, was adopted. Pakistan only uses the Islamic name on its passports, visas and coins. Although Islamic Republic is specifically mentioned in the constitution of 1973, all government documents are prepared under the name of the Government of Pakistan. The Constitution of Pakistan, Part IX, Article 227 states: ""All existing laws shall be brought in conformity with the Injunctions of Islam as laid down in the Quran and Sunnah,in this Part referred to as the Injunctions of Islam, and no law shall be enacted which is repugnant to such Injunctions"".
The Chechen Republic of Ichkeria used an Islamic republic government system from 1996 to 2000.[15]
Between 1978 and 2000, the Comoros was the Federal and Islamic Republic of the Comoros.
The Turkic Uyghur- and Kirghiz-controlled Turkish Islamic Republic of East Turkestan was declared in 1933 as an independent Islamic republic by Sabit Damulla Abdulbaki and Muhammad Amin Bughra. However, the Chinese Muslim 36th Division of the National Revolutionary Army defeated their armies and destroyed the republic during the Battles of Kashgar, Yangi Hissar and Yarkand.[16] The Chinese Muslim Generals Ma Fuyuan and Ma Zhancang declared the destruction of the rebel forces and the return of the area to the control of the Republic of China in 1934, followed by the executions of the Turkic Muslim Emirs Abdullah Bughra and Nur Ahmad Jan Bughra. The Chinese Muslim General Ma Zhongying then entered the Id Kah Mosque in Kashgar and lectured the Turkic Muslims on being loyal to the Nationalist Government.
In December 2015, the then-president Yahya Jammeh declared The Gambia to be an Islamic republic. Jammeh said that the move was designed to distance the West African state from its colonial past, that no dress code would be imposed and that citizens of other faiths would be allowed to practice freely.[17] However, he later ordered all female government employees to wear headscarves[18] before rescinding the decision shortly after. The announcement of an Islamic republic has been criticized as unconstitutional by at least one opposition group.[19] After the removal of Jammeh in 2017, his successor Adama Barrow said the Gambia would no longer be an Islamic republic.[20]
"
Engaged Spirituality - Wikipedia," Engaged spirituality refers to religious or spiritual people who actively engage in the world in order to transform it in positive ways while finding nurturance, inspiration and guidance in their spiritual beliefs and practices.[1] The term was inspired by engaged Buddhism, a concept and set of values developed by the Vietnamese Buddhist monk Thich Nhat Hanh. Engaged spirituality encompasses people committed to social change from all the major faith traditions as well as people who refer to themselves as ""spiritual but not religious"".[citation needed] It has numerous iterations in practice yet common themes unite the many forms it takes. For some in the Catholic tradition, liberation theology guides their form of engaged spirituality.
Individuals who practice this mode of spirituality tend to hold progressive values that supported by their spiritual practices galvanize their efforts for social change.[2] They see a deep connection between personal and social transformation such that they feel compelled to engage in organized causes or service activities.[3] Their activities are infused with their spiritual sensibilities regarding how matters of ultimate concern—the overarching context delineated by their faith tradition—are related to daily living, habits and practices. Examples of activities are peace activism, civil rights and human rights activism for minority groups, environmental activism and service on behalf of the poor and homeless.[4]
Unlike much of the pop spirituality that is promoted in countless books, audio programs and internet sites, engaged spirituality maintains a focus on societal transformation. Despite its politically liberal leanings, pop spirituality tends to concern itself primarily with personal psychological betterment that lacks a deep commitment to social change and activism.[5]
Engaged spirituality involves a synthesis of individual subjective experiences and outer collective activities. The individual and the collective mutually support, shape and transform each other. For example, prayer or meditation may serve as a way for an individual to gather strength and gain insight that will guide and enhance the efficacy of their social change efforts. Their experiences gathered in their outer activities which involve relating to and learning from others may influence the texture of their prayer or meditation experiences. Thus, there is a continual interwoven process of spiritual growth and reaffirmation to improving one’s local/global community.[6]
"
Anti-Masonry - Wikipedia," Anti-Masonry (alternatively called anti-Freemasonry) is ""avowed opposition to Freemasonry"",[1] which in some countries and religious groups has led to suppression. However, there is no homogeneous anti-Masonic movement. Anti-Masonry consists of radically differing criticisms from sometimes incompatible groups who are hostile to Freemasonry in some form.
The earliest[2] anti-Masonic document was a leaflet printed in 1698 by a Presbyterian minister named Winter. It reads:
TO ALL GODLY PEOPLE, In the Citie of London.
Having thought it needful to warn you of the Mischiefs and Evils practiced in the Sight of God by those called Freed Masons, I say take Care lest their Ceremonies and secret Swearings take hold of you; and be wary that none cause you to err from Godliness. For this devilish Sect of Men are Meeters in secret which swear against all without ther Following. They are the Anti Christ which was to come leading Men from Fear of God. For how should Men meet in secret Places and with secret Signs taking Care that none observed them to do the Work of GOD; are not these the Ways of Evil-doers?

Knowing how that God observeth privilly them that sit in Darkness they shall be smitten and the Secrets of their Hearts layed bare. Mingle not among this corrupt People lest you be found so at the World's Conflagration.[3]In 1826, William Morgan disappeared from the small town of Batavia, New York, and was presumed murdered after threatening to expose Freemasonry's ""secrets"" by publishing its rituals. His disappearance caused some Anti-masons to claim that he had been kidnapped and murdered by Masons. Morgan's disappearance sparked a series of protests against Freemasonry, which eventually spread to the political realm. Under the leadership of anti-Masonic Thurlow Weed, an Anti-Jacksonist movement became (since Jackson was a Mason) the Anti-Masonic Party. This political Party ran presidential candidates in 1828 and 1832, but by 1835 the party had disbanded everywhere except Pennsylvania.[citation needed]
In the United Kingdom, anti-Masonic sentiment grew following the publication of Martin Short's 1989 book, Inside the Brotherhood (Further Secrets of the Freemasons).[4] The allegations made by Short led several members of the British Government to propose laws requiring Freemasons who join the police or judiciary[5] to declare their membership publicly to the government amid accusations of Freemasons performing acts of mutual advancement and favour-swapping. This movement was initially led by Jack Straw, Home Secretary from 1997 until 2001.[5] In 1999, the Welsh Assembly became the only body in the United Kingdom to place a legal requirement on membership declaration for Freemasons.[6] Currently, existing members of the police and judiciary in England are asked to voluntarily admit to being Freemasons.[7] However, all first time successful judiciary candidates had to ""declare their freemasonry status"" before appointment until 2009, when – following a successful challenge in the European Court by Italian Freemasons – Jack Straw accepted that the policy was ""disproportionate"" and revoked it.[7] Conversely, new members of the police are not required to declare their status.[7]
In 2004, Rhodri Morgan, the First Minister of the Welsh Assembly, in Great Britain, said that he blocked Gerard Elias' appointment to counsel general because of links to hunting and freemasonry,[8] although it was claimed by non-Labour politicians that the real reason was in order to have a Labour supporter, Malcolm Bishop, in the role.[9]
Soviet Russia outlawed all secret societies, including Masonry, in 1922.[10] At one of the Second International meetings Grigory Zinoviev demanded to purge it of masons.[11] Freemasonry did not exist in the Soviet Union, China, or most other communist states. Postwar revivals of Freemasonry in Czechoslovakia and Hungary were suppressed in 1950.[12] However, Freemasonry in Cuba continued to exist following the Cuban Revolution, and according to Cuban folklore, Fidel Castro is said to have ""developed a soft spot for the Masons when they gave him refuge in a Masonic Lodge"" in the 1950s. However, when in power, Castro was also said to have ""kept them on a tight leash"" as they were considered a subversive element in Cuban society.[13]
Fascists treated Freemasonry as a potential source of opposition. Masonic writers state that the language used by the totalitarian regimes is similar to that used by other modern critics of Freemasonry.[14]
Consistently considered an ideological foe of Nazism in their world perception (Weltauffassung), Concentration Camp inmates who were Freemasons were graded as ""Political"" prisoners, and wore an inverted (point down) red triangle.[15]
In 1943, the Propaganda Abteilung, a delegation of Nazi Germany's propaganda ministry within occupied France, commissioned the propaganda film Forces occultes. The film virulently denounces Freemasonry, parliamentarianism and Jews as part of Vichy's drive against them and seeks to prove a Jewish-Masonic plot.
The number of Freemasons from Nazi occupied countries who were killed is not accurately known, but it is estimated that between 80,000 and 200,000 Freemasons perished under the Nazi regime.[16] The Government of the United Kingdom established Holocaust Memorial Day[17] to recognise all groups who were targets of the Nazi regime, and counter Holocaust denial. Freemasons are listed as being among those who were targeted.
In 1980, the Iraqi legal and penal code was changed by Saddam Hussein and the ruling Ba'ath Party, thereby making it a felony to ""promote or acclaim Zionist principles, including freemasonry, or who associate [themselves] with Zionist organizations.""[18]
Freemasonry has been alleged to hold back its members from fully committing to their nation.[19] Critics claim that compared to Operative Masonry's clear denunciations of treachery,[20] Speculative Masonry (Freemasonry after 1723) was far more ambiguous.[21] The old Catholic Encyclopedia alleges that Masonic disapproval of treachery is not on moral grounds but on the grounds of inconvenience to other Masons.[22] It also argues[23] that the adage ""Loyalty to freedom overrides all other considerations""[24] justifies treason, and quotes Albert Mackey, who said ""... if treason or rebellion were masonic crimes, almost every mason in the United Colonies (America), in 1776, would have been subject to expulsion and every Lodge to a forfeiture of its warrant by the Grand Lodges of England and Scotland, under whose jurisdiction they were at the time"".[19]
Freemasonry, however, charges its members that: ""In the state you are to be a quiet and peaceful subject, true to your government and just to your country; You are not to countenence disloyalty or rebellion, but patiently submit to legal authority and conform with cheerfulness to the government of the country in which you live.""[25]
With this charge in mind, American Freemasons are consistent advocates of the US Constitution, including the separation of church and state,[26] which was seen by the Roman Catholic Church as a veiled attack on the Church's place in public life.[27]
Freemasonry was persecuted in all the communist countries,[28][10] but the organization has survived in Cuba, allegedly providing safe haven for dissidents.[29]
After the 1826 disappearance of William Morgan, who was allegedly kidnapped by Freemasons[30] after publishing an exposé and then apparently killed,[31] the Morgan affair resulted in increased suspicion of Freemasonry and the formation of the Anti-Masonic Party. William A. Palmer of Vermont and Joseph Ritner of Pennsylvania were both elected governor of their respective states on anti-Masonic platforms.
John Quincy Adams, President of the United States during the Morgan affair, later declared, objecting to the oath of secrecy, in particular to keeping undefined secrets, and to the penalties for breaking the oath, ""Masonry ought forever to be abolished. It is wrong - essentially wrong - a seed of evil which can never produce any good.""[32]
Though few states passed laws directed at Freemasonry by name, laws regulating and restricting it were passed and many cases dealing with Freemasonry were seen in the courts.[33] Antimasonic legislation was passed in Vermont in 1833, including a provision by which the giving and willing taking of an unnecessary oath was made a crime. (Pub. Stat., sec. 5917),[34] and the state of New York enacted a Benevolent Orders Law to regulate such organizations.[33]
In 1938, a Japanese representative to the Welt-Dienst / World-Service congress hosted by Ulrich Fleischhauer stated, on behalf of Japan, that ""Judeo-Masonry is forcing the Chinese to turn China into a spearhead for an attack on Japan, and thereby forcing Japan to defend herself against this threat. Japan is at war not with China but with Freemasonry (Tiandihui), represented by General Chiang Kai-shek, the successor of his master, the Freemason Sun Yat-sen.""[28]
Freemasonry was outlawed in the Soviet Union during the Communist era and suppressed throughout Central Europe (Hungary and Czechoslovakia).[35]
Benito Mussolini decreed in 1924 that every member of his Fascist Party who was a Mason must abandon either one or the other organization, and in 1925, he dissolved Freemasonry in Italy, claiming that it was a political organization. One of the most prominent Fascists, General Capello, who had also been Deputy Grand Master of the Grande Oriente, Italy's leading Grand Lodge, gave up his membership in the Fascist Party rather than in Masonry. He was later arrested on false charges and sentenced to 30 years in jail.[36]
In 1919, Béla Kun[37] proclaimed the dictatorship of the proletariat in Hungary and Masonic properties were taken into public ownership. After the fall of the dictatorship of the proletariat the leaders of counter-revolution as Miklós Horthy blamed the Hungarian freemasons for their First World War defeat and for the revolution. Masonry was outlawed by a decree in 1920. This marked the start of raids by army officers on Masonic lodges[38] along with theft, and sometimes destruction, of Masonic libraries, records, archives, paraphernalia, and works of art. Several Masonic buildings were seized and used for anti-Masonic exhibitions. The masonic documents were archived, preserved and may still used for research.
In post war Hungary, lodges were re-established, but after five years[38] the government described them as ""meeting places of the enemies of the people's democratic republic, of capitalistic elements, and of the adherents of Western imperialism"". They were banned again in 1950.[28]
The Nazis claimed that high-degree Masons were willing members of the Jewish conspiracy and that Freemasonry was one of the causes of Germany's defeat in World War I.[39] In Mein Kampf, Adolf Hitler wrote that Freemasonry has succumbed to the Jews and has become an excellent instrument to fight for their aims and to use their strings to pull the upper strata of society into their designs. He continued, ""The general pacifistic paralysis of the national instinct of self-preservation begun by Freemasonry"" is then transmitted to the masses of society by the press.[40] In 1933 Hermann Göring, the Reichstag President and one of the key figures in the process of Gleichschaltung (""synchronization""), stated ""in National Socialist Germany, there is no place for Freemasonry"".[41]
The Enabling Act (Ermächtigungsgesetz in German) was passed by Germany's parliament (the Reichstag) on March 23, 1933. Using the Act, on January 8, 1934, the German Ministry of the Interior ordered the disbandment of Freemasonry, and confiscation of the property of all Lodges; stating that those who had been members of Lodges when Hitler came to power, in January 1933, were prohibited from holding office in the Nazi party or its paramilitary arms, and were ineligible for appointment in public service.[42] Consistently considered an ideological foe of Nazism in their world perception (Weltauffassung), special sections of the Security Service (SD) and later the Reich Security Main Office (RSHA) were established to deal with Freemasonry.[43] Masonic concentration camp inmates were graded as political prisoners, and wore an inverted (point down) red triangle.[44]
On August 8, 1935, as Führer and Chancellor, Adolf Hitler announced in the Nazi Party newspaper, Völkischer Beobachter, the final dissolution of all Masonic Lodges in Germany. The article accused a conspiracy of the Fraternity and World Jewry of seeking to create a World Republic.[45] In 1937 Joseph Goebbels inaugurated an ""Anti-Masonic Exposition"" to display objects seized by the state.[41] The Ministry of Defence forbade officers from becoming Freemasons, with officers who remained as Masons being sidelined.[28]
During the war, Freemasonry was banned by edict in all countries that were either allied with the Nazis or under Nazi control, including Norway and France. Anti-Masonic exhibitions were held in many occupied countries. Field-Marshal Friedrich Paulus was denounced as a ""High-grade Freemason"" when he surrendered to the Soviet Union in 1943.[46]
In 1943, the anti-Masonic propaganda film Forces occultes was produced in Nazi-occupied France, accusing the Freemasons of conspiring with Jews and Anglo-American nations to encourage France into a war with Germany.
The preserved records of the RSHA—i.e., Reichssicherheitshauptamt or the Office of the High Command of Security Service, which pursued the racial objectives of the SS through the Race and Resettlement Office—document the persecution of Freemasons.[43] The number of Freemasons from Nazi occupied countries who were killed is not accurately known, but it is estimated that between 80,000 and 200,000 Freemasons were murdered under the Nazi regime.[47]
In 1736 the Florentine Inquisition investigated a Masonic Lodge in Florence, Italy,[48] and the Lodge was condemned in June 1737 by the Chief Inquisitor in Rome. The lodge had originally been founded by English Masons, but accepted Italian members.
In 1738, Pope Clement XII issued In eminenti apostolatus, the first Papal prohibition on Freemasonry.
A more contemporary call for suppression is found in the encyclical Humanum genus of 1884, which calls Masonry a dangerous sect and demands that all bishops be vigilant on its abuses.
It is claimed that the dictator Miguel Primo de Rivera ordered the abolition of Freemasonry in Spain.[49] In September 1928, one of the two Grand Lodges in Spain was closed and approximately two-hundred (200) masons, most notably the Grand Master of the Grand Orient, were imprisoned for allegedly plotting against the government.[50]
Following the military coup of 1936, many Freemasons trapped in areas under Nationalist control were arrested and summarily killed in the White Terror (Spain), along with members of left wing parties and trade unionists. It was reported that Masons were tortured, garroted, shot, and murdered by organized death squads in every town in Spain. At this time one of the most rabid opponents of Freemasonry, Father Juan Tusquets Terrats, began to work for the Nationalists with the task of exposing masons. One of his close associates was Franco's personal chaplain, and over the next two years, these two men assembled a huge index of 80,000 suspected masons, even though there were little more than 5,000 masons in Spain. The results were horrific. Among other countless crimes, the lodge building in Cordoba was burnt, the Masonic Temple of Santa Cruz de Tenerife in the Canary Islands was confiscated and transformed into the headquarters of the Falange, and another was shelled by artillery. In Salamanca thirty (30) members of one lodge were shot, including a priest. Similar atrocities occurred across the country: fifteen (15) masons were shot in Logrono, seventeen (17) in Ceuta, thirty-three (33) in Algeciras, and thirty (30) in Valladolid, among them the Civil Governor. Few towns escaped the carnage as Freemasons in Lugo, Zamora, Cadiz and Granada were brutally rounded up and shot, and in Seville, the entire membership of several lodges were butchered. The slightest suspicion of being a mason was often enough to earn a place in a firing squad, and the blood-letting was so fierce that, reportedly, some masons were even hurled into working engines of steam trains. By 16 December 1937, according to the annual masonic assembly held in Madrid, all masons that had not escaped from the areas under nationalist control had been murdered.[50]
After the victory of dictator General Francisco Franco, Freemasonry was officially outlawed in Spain on 2 March 1940. Being a mason was automatically punishable by a minimum jail term of 12 years.[51] Masons of the 18º and above were deemed guilty of ‘Aggravated Circumstances’, and usually faced the death penalty.[52]
According to Francoists, the Republican Regime which Franco overthrew had a strong Masonic presence.[citation needed] In reality Spanish Masons were present in all sectors of politics and the armed forces.[53] At least four (4) of the Generals who supported Franco's rebellion were Masons, although many lodges contained fervent but generally conservative Republicans. Freemasonry was formally outlawed in the Law for the Repression of Freemasonry and Communism.[54] After Franco's decree outlawing masonry, Franco's supporters were given two months to resign from any lodge they might be a member. Many masons chose to go into exile instead, including prominent monarchists who had whole-heartedly supported the Nationalist rebellion in 1936. The common components in Spanish Masonry seems to have been upper or middle class conservative liberalism and strong anti-clericism.[55]
The Law for the Repression of Freemasonry and Communism was not abrogated until 1963.[54] References to a ""Judeo-Masonic plot"" are a standard component of Francoist speeches and propaganda and reveal the intense and paranoid obsession of the dictator with masonry. Franco produced at least 49 pseudonymous anti-masonic magazine articles and an anti-masonic book during his lifetime. According to Franco:
It was the Unlawful Societies Act of 1799 that saw the first statute ""for the more effectual suppression of societies established for seditious and treasonable purposes""; once enacted it affected all societies whose members were required to take an oath not authorised by law, shall be deemed ""unlawful combinations."" It was as a result of the intervention of the Grand Master of the Antients, The 4th Duke of Atholl, and the Acting Grand Master of the Moderns, the earl of Moira that a special exempting clause was inserted into this legislation in favour of societies ""held under the Denomination of Lodges of Freemasons"" provided that they had been ""usually held before the Act"" and their names, places and times of meeting and the names of the members were annually registered with the local Clerk to the Justices of the Peace. This continued on until 1967 when this Act was repealed by a section of the Criminal Justice Act which meant that the annual returns of all the Lodges to the authorities ceased.[56]
Since 1997, several members of the British Government have attempted to pass laws requiring Freemasons who join the police or judiciary[5] to declare their membership publicly to the government amid accusations of Freemasons performing acts of mutual advancement and favour-swapping. This movement was initially led by Jack Straw, Home Secretary from 1997 until 2001.[5] In 1999, the Welsh Assembly became the only body in the United Kingdom to place a legal requirement on membership declaration for Freemasons.[6] Currently, existing members of the police and judiciary in England are asked to voluntarily admit to being Freemasons.[7] However, all first time successful judiciary candidates ""must declare their freemasonry status"" before appointment.[7] Conversely, new members of the police are not required to declare their status.[7]
In 2004, Rhodri Morgan, the First Minister of the Welsh Assembly, said that he blocked Gerard Elias' appointment to counsel general because of links to hunting and Freemasonry,[8] although it was claimed by non-Labour politicians that the real reason was in order to have a Labour supporter, Malcolm Bishop, in the role.[9]
After the condemnation of Freemasonry by Clement XII in 1738, Sultan Mahmud I followed suit outlawing the organization and since that time Freemasonry was equated with atheism in the Ottoman Empire and the broader Islamic world.[57] The opposition in the Islamic world has been reinforced by the anticlerical and atheistic slant of the Grand Orient of France.[57]
On July 15, 1978, the Islamic Jurisdictional College—one of the most influential entities that interpret Sharia, or Islamic law—issued an opinion that deemed Freemasonry to be ""dangerous"" and ""clandestine"".[57]
After World War II, while under the British Mandate, Iraq used to have several lodges. This all changed with the 14 July Revolution in 1958, however, with the abolition of the Hashemite Monarchy and Iraq's declaration as a republic. The licences permitting lodges to meet were rescinded, and later, laws were introduced banning any further meetings. This position was later reinforced under Saddam Hussein the death penalty was ""prescribed"" for those who ""promote or acclaim Zionist principles, including Freemasonry, or who associate [themselves] with Zionist organizations"".[58]
Freemasonry is illegal in all Arab countries except Lebanon and Morocco.
Many Islamic anti-Masonic arguments are closely tied to both Anti-Semitism and Anti-Zionism, though other criticisms are made such as linking Freemasonry to Dajjal.[59] Some Muslim anti-Masons argue that Freemasonry promotes the interests of the Jews around the world and that one of its aims is to rebuild the Temple of Solomon in Jerusalem after destroying the Al-Aqsa Mosque.[60] In article 28 of its Covenant, Hamas states that Freemasonry, Rotary, and other similar groups ""work in the interest of Zionism and according to its instructions....""[61] Many countries with a significant Muslim population do not allow Masonic establishments within their jurisdictions. However, a few countries such as Turkey and Morocco have allowed establishment of Grand Lodges[62] while in countries such as Malaysia[63] and Lebanon,[64] there are District Grand Lodges operating under a warrant from an established Grand Lodge.
One of the first highly vocal Christian critics of freemasonry was Charles Finney. In his book The Character, Claims, and Practical Workings of Freemasonry, Finney not only ridicules the masons but also explains why he viewed leaving the association as an essential act three years after entering seminary.
A number of Protestant and Eastern Orthodox denominations discourage their congregants from joining Masonic lodges, although this differs in intensity according to the denomination. Some simply express mild concern as to whether Freemasonry is compatible with Christianity while, at the other extreme, some accuse the fraternity of outright devil worship, by quoting the writings of Leo Taxil and Abel Clarin de la Rive.[65]
The Roman Catholic Church has, since 1738, prohibited membership in Masonic organizations, citing both political and religious reasons. Until 1983 the penalty for Catholics who joined the fraternity was excommunication.[66] Since that time the punishment has been an interdict, barring the offender from Holy Communion. Although the canonical penalty changed in 1983, the prohibition on membership has not.[67]
There have long been conspiracy theories concerning Freemasonry in which the organization is either bent on world domination or already covertly in control of world politics.[68]
The covenant of the Palestinian Islamist movement Hamas claims that Freemasonry is a secret society founded as part of a Zionist plot to control the world.[69]
The earliest document accusing Freemasonry of being involved in a conspiracy was Enthüllungen des Systems der Weltbürger-Politik (“Disclosure of the System of Cosmopolitan Politics”), published in 1786.[70] The book claimed that there was a conspiracy of Freemasons, Illuminati and Jesuits who were plotting world revolution.[71] During the 19th Century, this theory was repeated by many Christian counter-revolutionaries,[72][73] who saw Freemasons as being behind every attack on the existing social system.[72][73]
Academic examinations of Anti-Masonry
"
Hermann Göring - Wikipedia," 
Hermann Wilhelm Göring (or Goering;[a] German: [ˈɡøːʁɪŋ] (listen); 12 January 1893 – 15 October 1946) was a German political and military leader and a convicted war criminal. Göring was one of the most powerful figures in the Nazi Party, which ruled Germany from 1933 to 1945. 
A veteran World War I fighter pilot ace, he was a recipient of the Pour le Mérite (""The Blue Max""). He was the last commander of Jagdgeschwader 1 (Jasta 1), the fighter wing once led by Manfred von Richthofen. An early member of the Nazi Party, Göring was among those wounded in Adolf Hitler's failed Beer Hall Putsch in 1923. While receiving treatment for his injuries, he developed an addiction to morphine which persisted until the last year of his life. After Hitler became Chancellor of Germany in 1933, Göring was named as minister without portfolio in the new government. One of his first acts as a cabinet minister was to oversee the creation of the Gestapo, which he ceded to Heinrich Himmler in 1934. 
Following the establishment of the Nazi state, Göring amassed power and political capital to become the second most powerful man in Germany. He was appointed commander-in-chief of the Luftwaffe (air force), a position he held until the final days of the regime. Upon being named Plenipotentiary of the Four Year Plan in 1936, Göring was entrusted with the task of mobilizing all sectors of the economy for war, an assignment which brought numerous government agencies under his control and helped him become one of the wealthiest men in the country. In September 1939 Hitler designated him as his successor and deputy in all his offices. After the Fall of France in 1940, he was bestowed the specially created rank of Reichsmarschall, which gave him seniority over all officers in Germany's armed forces.
By 1941, Göring was at the peak of his power and influence. As the Second World War progressed, Göring's standing with Hitler and with the German public declined after the Luftwaffe proved incapable of preventing the Allied bombing of Germany's cities and resupplying surrounded Axis forces in Stalingrad. Around that time, Göring increasingly withdrew from military and political affairs to devote his attention to collecting property and artwork, much of which was stolen from Jewish victims of the Holocaust. Informed on 22 April 1945 that Hitler intended to commit suicide, Göring sent a telegram to Hitler requesting his permission to assume leadership of the Reich. Considering his request an act of treason, Hitler removed Göring from all his positions, expelled him from the party, and ordered his arrest. After the war, Göring was convicted of conspiracy, crimes against peace, war crimes and crimes against humanity at the Nuremberg trials in 1946. He was sentenced to death by hanging, but committed suicide by ingesting cyanide hours before the sentence was to be carried out.
Göring was born on 12 January 1893[4] at the Marienbad Sanatorium in Rosenheim, Bavaria. His father, Heinrich Ernst Göring (31 October 1839 – 7 December 1913), a former cavalry officer, had been the first Governor-General of German South West Africa (modern-day Namibia).[5] Heinrich had three children from a previous marriage. Göring was the fourth of five children by Heinrich's second wife, Franziska Tiefenbrunn (1859–15 July 1943), a Bavarian peasant. Göring's elder siblings were Karl, Olga, and Paula; his younger brother was Albert. At the time that Göring was born, his father was serving as consul general in Haiti, and his mother had returned home briefly to give birth. She left the six-week-old baby with a friend in Bavaria and did not see the child again for three years, when she and Heinrich returned to Germany.[6]
Göring's godfather was Hermann Epenstein [de], a wealthy Jewish physician and businessman his father had met in Africa. Epenstein provided the Göring family, who were surviving on Heinrich's pension, first with a family home in Berlin-Friedenau,[7] then in a small castle called Veldenstein, near Nuremberg. Göring's mother became Epenstein's mistress around this time, and remained so for some fifteen years. Epenstein acquired the minor title of Ritter (knight) von Epenstein through service and donations to the Crown.[8]
Interested in a career as a soldier from a very early age, Göring enjoyed playing with toy soldiers and dressing up in a Boer uniform his father had given him. He was sent to boarding school at age eleven, where the food was poor and discipline was harsh. He sold a violin to pay for his train ticket home, and then took to his bed, feigning illness, until he was told he would not have to return.[9] He continued to enjoy war games, pretending to lay siege to the castle Veldenstein and studying Teutonic legends and sagas. He became a mountain climber, scaling peaks in Germany, at the Mont Blanc massif, and in the Austrian Alps. At sixteen he was sent to a military academy at Berlin Lichterfelde, from which he graduated with distinction.[10] (During the Nuremberg war-crimes trials in 1946, psychologist Gustave Gilbert measured him as having an intelligence quotient (IQ) of 138.)[11]
Göring joined the Prince Wilhelm Regiment (112th Infantry, Garrison: Mülhausen) of the Prussian Army in 1912. The next year his mother had a falling-out with Epenstein. The family was forced to leave Veldenstein and moved to Munich; Göring's father died shortly afterwards. When World War I began in August 1914, Göring was stationed at Mülhausen with his regiment.[10]
During the first year of World War I, Göring served with his infantry regiment in the area of Mülhausen, a garrison town less than 2 km from the French frontier. He was hospitalized with rheumatism, a result of the damp of trench warfare. While he was recovering, his friend Bruno Loerzer convinced him to transfer to what would become, by October 1916, the Luftstreitkräfte (""air combat forces"") of the German army, but his request was turned down. Later that year, Göring flew as Loerzer's observer in Feldflieger Abteilung 25 (FFA 25) – Göring had informally transferred himself. He was discovered and sentenced to three weeks' confinement to barracks, but the sentence was never carried out. By the time it was supposed to be imposed, Göring's association with Loerzer had been made official. They were assigned as a team to FFA 25 in the Crown Prince's Fifth Army. They flew reconnaissance and bombing missions, for which the Crown Prince invested both Göring and Loerzer with the Iron Cross, first class.[12]
After completing the pilot's training course, Göring was assigned to Jagdstaffel 5. Seriously wounded in the hip in aerial combat, he took nearly a year to recover. He then was transferred to Jagdstaffel 26, commanded by Loerzer, in February 1917. He steadily scored air victories until May, when he was assigned to command Jagdstaffel 27. Serving with Jastas 5, 26, and 27, he continued to win victories. In addition to his Iron Crosses (1st and 2nd Class), he received the Zähringer Lion with swords, the Friedrich Order, the House Order of Hohenzollern with swords third class, and finally, in May 1918, the coveted Pour le Mérite.[13] According to Hermann Dahlmann, who knew both men, Göring had Loerzer lobby for the award.[14] He finished the war with 22 victories.[15] A thorough post-war examination of Allied loss records showed that only two of his awarded victories were doubtful. Three were possible and 17 were certain, or highly likely.[16]
On 7 July 1918, following the death of Wilhelm Reinhard, successor to Manfred von Richthofen, Göring was made commander of the ""Flying Circus"", Jagdgeschwader 1.[17] His arrogance made him unpopular with the men of his squadron.[18]
In the last days of the war, Göring was repeatedly ordered to withdraw his squadron, first to Tellancourt airdrome, then to Darmstadt. At one point, he was ordered to surrender the aircraft to the Allies; he refused. Many of his pilots intentionally crash-landed their planes to keep them from falling into enemy hands.[19]
Like many other German veterans, Göring was a proponent of the Stab-in-the-back legend, the belief which held that the German Army had not really lost the war, but instead was betrayed by the civilian leadership: Marxists, Jews, and especially the Republicans, who had overthrown the German monarchy.[20]
Göring remained in aviation after the war. He tried barnstorming and briefly worked at Fokker. After spending most of 1919 living in Denmark, he moved to Sweden and joined Svensk Lufttrafik, a Swedish airline. Göring was often hired for private flights. During the winter of 1920–1921, he was hired by Count Eric von Rosen to fly him to his castle from Stockholm. Invited to spend the night, Göring may at this time have first seen the swastika emblem, which Rosen had set in the chimney piece as a family badge.[21][b]
This was also the first time that Göring saw his future wife; the count introduced his sister-in-law, Baroness Carin von Kantzow (née Freiin von Fock). Estranged from her husband of ten years, she had an eight-year-old son. Göring was immediately infatuated and asked her to meet him in Stockholm. They arranged a visit at the home of her parents and spent much time together through 1921, when Göring left for Munich to take political science at the university. Carin obtained a divorce, followed Göring to Munich, and married him on 3 February 1922.[22] Their first home together was a hunting lodge at Hochkreuth in the Bavarian Alps, near Bayrischzell, some 80 kilometres (50 mi) from Munich.[23] After Göring met Adolf Hitler and joined the Nazi Party in 1922, they moved to Obermenzing, a suburb of Munich.[24]
Göring joined the Nazi Party in 1922 after hearing a speech by Hitler.[24][25] He was given command of the Sturmabteilung (SA) as the Oberster SA-Führer in 1923.[26] He was later appointed an SA-Gruppenführer (Lieutenant General) and held this rank on the SA rolls until 1945. At this time, Carin—who liked Hitler—often played hostess to meetings of leading Nazis, including her husband, Hitler, Rudolf Hess, Alfred Rosenberg, and Ernst Röhm.[27] Hitler later recalled his early association with Göring:
I liked him. I made him the head of my SA. He is the only one of its heads that ran the SA properly. I gave him a dishevelled rabble. In a very short time he had organised a division of 11,000 men.[28]Hitler and the Nazi Party held mass meetings and rallies in Munich and elsewhere during the early 1920s, attempting to gain supporters in a bid for political power.[29] Inspired by Benito Mussolini's March on Rome, the Nazis attempted to seize power on 8–9 November 1923 in a failed coup known as the Beer Hall Putsch. Göring, who was with Hitler leading the march to the War Ministry, was shot in the groin.[30] Fourteen Nazis and four policemen were killed; many top Nazis, including Hitler, were arrested.[31] With Carin's help, Göring was smuggled to Innsbruck, where he received surgery and was given morphine for the pain. He remained in hospital until 24 December.[32] This was the beginning of his morphine addiction, which lasted until his imprisonment at Nuremberg.[33] Meanwhile, the authorities in Munich declared Göring a wanted man. The Görings—acutely short of funds and reliant on the good will of Nazi sympathizers abroad—moved from Austria to Venice. In May 1924 they visited Rome, via Florence and Siena. Göring met Mussolini, who expressed an interest in meeting Hitler, who was by then in prison.[34]
Personal problems continued to multiply. By 1925, Carin's mother was ill. The Görings—with difficulty—raised the money in the spring of 1925 for a journey to Sweden via Austria, Czechoslovakia, Poland, and Danzig (now Gdańsk). Göring had become a violent morphine addict; Carin's family were shocked by his deterioration. Carin, who was ill with epilepsy and a weak heart, had to allow the doctors to take charge of Göring; her son was taken by his father. Göring was certified a dangerous drug addict and was placed in Långbro asylum on 1 September 1925.[35] He was violent to the point where he had to be confined in a straitjacket, but his psychiatrist felt he was sane; the condition was caused solely by the morphine.[36] Weaned off the drug, he left the facility briefly, but had to return for further treatment. He returned to Germany when an amnesty was declared in 1927 and resumed working in the aircraft industry.[37] Hitler, who had written Mein Kampf while in prison, had been released in December 1924.[38] Carin Göring, ill with epilepsy and tuberculosis,[39] died of heart failure on 17 October 1931.
Meanwhile, the Nazi Party was in a period of rebuilding and waiting. The economy had recovered, which meant fewer opportunities for the Nazis to agitate. The SA was reorganised, but with Franz Pfeffer von Salomon as its head rather than Göring, and the Schutzstaffel (SS) was founded in 1925, initially as a bodyguard for Hitler. Membership in the party increased from 27,000 in 1925 to 108,000 in 1928 and 178,000 in 1929. In the May 1928 elections the Nazi Party only obtained 12 seats out of an available 491 in the Reichstag.[40] Göring was elected as a representative from Bavaria.[41]  He continued to be elected to the Reichstag in all subsequent elections during the Weimar and Nazi regimes.[42] The Great Depression led to a disastrous downturn in the German economy, and in the 1930 election, the Nazi Party won 6,409,600 votes and 107 seats.[43] In May 1931, Hitler sent Göring on a mission to the Vatican, where he met the future Pope Pius XII.[44]
In the July 1932 election, the Nazis won 230 seats to become far and away the largest party in the Reichstag. By longstanding tradition, the Nazis were thus entitled to select the President of the Reichstag, and elected Göring to the post.[45] He would retain this position until 23 April 1945.
The Reichstag fire occurred on the night of 27 February 1933. Göring was one of the first to arrive on the scene. Marinus van der Lubbe—a Communist radical—was arrested and claimed sole responsibility for the fire. Göring immediately called for a crackdown on Communists.[46]
The Nazis took advantage of the fire to advance their own political aims. The Reichstag Fire Decree, passed the next day on Hitler's urging, suspended basic rights and allowed detention without trial. Activities of the German Communist Party were suppressed, and some 4,000 Party members were arrested.[47] Göring demanded that the detainees should be shot, but Rudolf Diels, head of the Prussian political police, ignored the order.[48] Some researchers, including William L. Shirer and Alan Bullock, are of the opinion that the Nazi Party itself was responsible for starting the fire.[49][50]
At the Nuremberg trials, General Franz Halder testified that Göring admitted responsibility for starting the fire. He said that, at a luncheon held on Hitler's birthday in 1942, Göring said, ""The only one who really knows about the Reichstag is I, because I set it on fire!""[51] In his own Nuremberg testimony, Göring denied this story.[52]
During the early 1930s, Göring was often in the company of Emmy Sonnemann, an actress from Hamburg.[53] They were married on 10 April 1935 in Berlin; the wedding was celebrated on a huge scale. A large reception was held the night before at the Berlin Opera House. Fighter aircraft flew overhead on the night of the reception and the day of the ceremony,[54] at which Hitler was best man.[55] Göring's daughter, Edda, was born on 2 June 1938.[56]
When Hitler was named chancellor of Germany in January 1933, Göring was appointed as minister without portfolio, Minister of the Interior for Prussia, and Reich Commissioner of Aviation.[57] Wilhelm Frick was named Reich Interior Minister. Frick and head of the Schutzstaffel (SS) Heinrich Himmler hoped to create a unified police force for all of Germany, but Göring on 30 November 1933 established a Prussian police force, with Rudolf Diels at its head. The force was called the Geheime Staatspolizei (Secret State Police), or Gestapo. Göring, thinking that Diels was not ruthless enough to use the Gestapo effectively to counteract the power of the SA, handed over control of the Gestapo to Himmler on 20 April 1934.[58] By this time, the SA numbered over two million men.[59]
Hitler was deeply concerned that Ernst Röhm, the chief of the SA, was planning a coup. Himmler and Reinhard Heydrich plotted with Göring to use the Gestapo and SS to crush the SA.[60] Members of the SA got wind of the proposed action and thousands of them took to the streets in violent demonstrations on the night of 29 June 1934. Enraged, Hitler ordered the arrest of the SA leadership. Röhm was shot dead in his cell when he refused to commit suicide; Göring personally went over the lists of detainees—numbering in the thousands—and determined who else should be shot. At least 85 people were killed in the period of 30 June to 2 July, which is now known as the Night of the Long Knives.[61] Hitler admitted in the Reichstag on 13 July that the killings had been entirely illegal, but claimed a plot had been under way to overthrow the Reich. A retroactive law was passed making the action legal. Any criticism was met with arrests.[62]
One of the terms of the Treaty of Versailles, which had been in place since the end of World War I, stated that Germany was not allowed to maintain an air force. After the 1926 signing of the Kellogg–Briand Pact, police aircraft were permitted. Göring was appointed Air Traffic Minister in May 1933. Germany began to accumulate aircraft in violation of the Treaty, and in 1935 the existence of the Luftwaffe was formally acknowledged,[63] with Göring as Reich Aviation Minister.[64]
During a cabinet meeting in September 1936, Göring and Hitler announced that the German rearmament programme must be sped up. On 18 October, Hitler named Göring as Plenipotentiary of the Four Year Plan to undertake this task. Göring created a new organisation to administer the Plan and drew the ministries of labour and agriculture under its umbrella. He bypassed the economics ministry in his policy-making decisions, to the chagrin of Hjalmar Schacht, the minister in charge. Huge expenditures were made on rearmament, in spite of growing deficits.[65] Schacht resigned on 8 December 1937,[66] and Walther Funk took over the position, as well as control of the Reichsbank. In this way, both of these institutions were brought under Göring's control under the auspices of the Four Year Plan.[67] In July 1937, the Reichswerke Hermann Göring was established under state ownership – though led by Göring – with the aim of boosting steel production beyond the level which private enterprise could economically provide.[68]
In 1938, Göring was involved in the Blomberg–Fritsch Affair, which led to the resignations of the War Minister, Generalfeldmarschall Werner von Blomberg, and the army commander, General Werner von Fritsch. Göring had acted as witness at Blomberg's wedding to Margarethe Gruhn, a 26-year-old typist, on 12 January 1938. Information received from the police showed that the young bride was a prostitute.[69] Göring felt obligated to tell Hitler, but also saw this event as an opportunity to dispose of Blomberg. Blomberg was forced to resign. Göring did not want Fritsch to be appointed to that position and thus be his superior. Several days later, Heydrich revealed a file on Fritsch that contained allegations of homosexual activity and blackmail. The charges were later proven to be false, but Fritsch had lost Hitler's trust and was forced to resign.[70] Hitler used the dismissals as an opportunity to reshuffle the leadership of the military. Göring asked for the post of War Minister, but was turned down; he was appointed to the rank of Generalfeldmarschall. Hitler took over as supreme commander of the armed forces and created subordinate posts to head the three main branches of service.[71]
As minister in charge of the Four Year Plan, Göring became concerned with the lack of natural resources in Germany, and began pushing for Austria to be incorporated into the Reich. The province of Styria had rich iron ore deposits, and the country as a whole was home to many skilled labourers that would also be useful. Hitler had always been in favour of a takeover of Austria, his native country. He met the Austrian Chancellor Kurt Schuschnigg on 12 February 1938, threatening invasion if peaceful unification was not forthcoming.  The Nazi Party was made legal in Austria to gain a power base, and a referendum on reunification was scheduled for March. When Hitler did not approve of the wording of the plebiscite, Göring telephoned Schuschnigg and Austrian head of state Wilhelm Miklas to demand Schuschnigg's resignation, threatening invasion by German troops and civil unrest by the Austrian Nazi Party members. Schuschnigg resigned on 11 March and the plebiscite was cancelled. By 5:30 the next morning, German troops that had been massing on the border marched into Austria, meeting no resistance.[72]
Although Joachim von Ribbentrop had been named Foreign Minister in February 1938, Göring continued to involve himself in foreign affairs.[56] That July, he contacted the British government with the idea that he should make an official visit to discuss Germany's intentions for Czechoslovakia. Neville Chamberlain was in favour of a meeting, and there was talk of a pact being signed between Britain and Germany. In February 1938, Göring visited Warsaw to quell rumours about the upcoming invasion of Poland. He had conversations with the Hungarian government that summer as well, discussing their potential role in an invasion of Czechoslovakia. At the Nuremberg Rally that September, Göring and other speakers denounced the Czechs as an inferior race that must be conquered.[73] Chamberlain and Hitler had a series of meetings that led to the signing of the Munich Agreement (29 September 1938), which turned over control of the Sudetenland to Germany.[74] In March 1939, Göring threatened Czechoslovak president Emil Hácha with the bombing of Prague. Hácha then agreed to sign a communique accepting the German occupation of the remainder of Bohemia and Moravia.[75]
Although many in the party disliked him,[76] before the war Göring enjoyed widespread personal popularity among the German public because of his perceived sociability, colour and humour.[77][78] As the Nazi leader most responsible for economic matters, he presented himself as a champion of national interests over allegedly corrupt big business and the old German elite. The Nazi press was on Göring's side. Other leaders, such as Hess and Ribbentrop, were envious of his popularity.[77] In Britain and the United States, some viewed Göring as more acceptable than the other Nazis and as a possible mediator between the western democracies and Hitler.[78]
Göring and other senior officers were concerned that Germany was not yet ready for war, but Hitler insisted on pushing ahead as soon as possible.[79] On 30 August 1939, immediately prior to the outbreak of the Second World War, Hitler appointed Göring as the chairman of a new six-person Council of Ministers for Defense of the Reich which was set up to operate as a war cabinet.[80] The invasion of Poland, the opening action of World War II, began at dawn on 1 September 1939.[81] Later in the day, speaking to the Reichstag, Hitler designated Göring as his successor as Führer of all Germany, ""If anything should befall me"",[82] with Hess as the second alternate.[76] Big German victories followed one after the other in quick succession. With the help of the Luftwaffe, the Polish Air Force was defeated within a week.[83] The Fallschirmjäger seized vital airfields in Norway and captured Fort Eben-Emael in Belgium. Göring's Luftwaffe played critical roles in the Battles of the Netherlands, Belgium and France in May 1940.[84]
After the Fall of France, Hitler awarded Göring the Grand Cross of the Iron Cross for his successful leadership.[85] During the 1940 Field Marshal Ceremony, Hitler promoted Göring to the rank of Reichsmarschall des Grossdeutschen Reiches (Reich Marshal of the Greater German Reich), a specially-created rank which made him senior to all field marshals in the military, including the Luftwaffe. As a result of this promotion, he was the highest-ranking soldier in Germany until the end of the war. Göring had already received the Knight's Cross of the Iron Cross on 30 September 1939 as Commander in Chief of the Luftwaffe.[85]
The UK had declared war on Germany immediately after the invasion of Poland. In July 1940, Hitler began preparations for an invasion of Britain. As part of the plan, the Royal Air Force (RAF) had to be neutralized. Bombing raids commenced on British air installations and on cities and centres of industry.[86] Göring had by then already announced in a radio speech, ""If as much as a single enemy aircraft flies over German soil, my name is Meier!"",[87] something that would return to haunt him, when the RAF began bombing German cities on 11 May 1940.[88] Though he was confident the Luftwaffe could defeat the RAF within days, Göring, like Admiral Erich Raeder, commander-in-chief of the Kriegsmarine (navy),[89] was pessimistic about the chance of success of the planned invasion (codenamed Operation Sea Lion).[90] Göring hoped that a victory in the air would be enough to force peace without an invasion. The campaign failed, and Sea Lion was postponed indefinitely on 17 September 1940.[91] After their defeat in the Battle of Britain, the Luftwaffe attempted to defeat Britain via strategic bombing. On 12 October 1940 Hitler cancelled Sea Lion due to the onset of winter.[92] By the end of the year, it was clear that British morale was not being shaken by the Blitz, though the bombings continued through May 1941.[93]
In spite of the Molotov–Ribbentrop Pact, signed in 1939, Nazi Germany began Operation Barbarossa—the invasion of the Soviet Union—on 22 June 1941. Initially the Luftwaffe was at an advantage, destroying thousands of Soviet aircraft in the first month of fighting.[94] Hitler and his top staff were sure that the campaign would be over by Christmas, and no provisions were made for reserves of men or equipment.[95] But, by July, the Germans had only 1,000 planes remaining in operation, and their troop losses were over 213,000 men. The choice was made to concentrate the attack on only one part of the vast front; efforts would be directed at capturing Moscow.[96] After the long, but successful, Battle of Smolensk, Hitler ordered Army Group Centre to halt its advance to Moscow and temporarily diverted its Panzer groups north and south to aid in the encirclement of Leningrad and Kyiv.[97] The pause provided the Red Army with an opportunity to mobilize fresh reserves; historian Russel Stolfi considers it to be one of the major factors that caused the failure of the Moscow offensive, which was resumed in October 1941 with the Battle of Moscow.[97] Poor weather conditions, fuel shortages, a delay in building aircraft bases in Eastern Europe, and overstretched supply lines were also factors. Hitler did not give permission for even a partial retreat until mid-January 1942; by this time the losses were comparable to those of the French invasion of Russia in 1812.[98]
Hitler decided that the summer 1942 campaign would be concentrated in the south; efforts would be made to capture the oilfields in the Caucasus.[99] The Battle of Stalingrad, a major turning point of the war,[100] began on 23 August 1942 with a bombing campaign by the Luftwaffe.[101] The German Sixth Army entered the city, but because of its location on the front line, it was still possible for the Soviets to encircle and trap it there without reinforcements or supplies. When the Sixth Army was surrounded by the end of November in Operation Uranus, Göring promised that the Luftwaffe would be able to deliver a minimum of 300 tons of supplies to the trapped men every day. On the basis of these assurances, Hitler demanded that there be no retreat; they were to fight to the last man. Though some airlifts were able to get through, the amount of supplies delivered never exceeded 120 tons per day.[102][103] The remnants of the Sixth Army—some 91,000 men out of an army of 285,000—surrendered in early February 1943; only 5,000 of these captives survived the Russian prisoner of war camps to see Germany again.[104]
Meanwhile, the strength of the US and British bomber fleets had increased. Based in Britain, they began operations against German targets. The first thousand-bomber raid was staged on Cologne on 30 May 1942.[105] Air raids continued on targets further from England after auxiliary fuel tanks were installed on US fighter aircraft. Göring refused to believe reports that American fighters had been shot down as far east as Aachen in winter 1943. His reputation began to decline.[106]
The American P-51 Mustang, with a combat radius of over 1,800 miles (2,900 km) when using underwing drop tanks, began to escort the bombers in large formations to and from the target area in early 1944. From that point onwards, the Luftwaffe began to suffer casualties in aircrews it could not sufficiently replace. By targeting oil refineries and rail communications, Allied bombers crippled the German war effort by late 1944.[107] German civilians blamed Göring for his failure to protect the homeland.[108] Hitler began excluding him from conferences, but continued him in his positions at the head of the Luftwaffe and as plenipotentiary of the Four Year Plan.[109] As he lost Hitler's trust, Göring began to spend more time at his various residences.[110] On D-Day (6 June 1944), the Luftwaffe only had some 300 fighters and a small number of bombers in the area of the landings; the Allies had a total strength of 11,000 aircraft.[111]
As the Soviets approached Berlin, Hitler's efforts to organise the defence of the city became ever more meaningless and futile.[112] His last birthday, celebrated at the Führerbunker in Berlin on 20 April 1945, was the occasion for leave-taking for many top Nazis, Göring included. By this time, Göring's hunting lodge Carinhall had been evacuated, the building destroyed,[113] and its art treasures moved to Berchtesgaden and elsewhere.[114] Göring arrived at his estate at Obersalzberg on 22 April, the same day that Hitler, in a lengthy diatribe against his generals, first publicly admitted that the war was lost and that he intended to remain in Berlin to the end and then commit suicide.[115] He also stated that Göring was in a better position to negotiate a peace settlement.[116]
OKW operations chief Alfred Jodl was present for Hitler's rant, and notified Göring's chief of staff, Karl Koller, at a meeting a few hours later. Sensing its implications, Koller immediately flew to Berchtesgaden to notify Göring of this development. A week after the start of the Soviet invasion, Hitler had issued a decree naming Göring his successor in the event of his death, thus codifying the declaration he had made soon after the beginning of the war. The decree also gave Göring full authority to act as Hitler's deputy if Hitler ever lost his freedom of action.[116]
Göring feared being branded a traitor if he tried to take power, but also feared being accused of dereliction of duty if he did nothing. After some hesitation, Göring reviewed his copy of the 1941 decree naming him Hitler's successor. After conferring with Koller and Hans Lammers (the state secretary of the Reich Chancellery), Göring concluded that by remaining in Berlin to face certain death, Hitler had incapacitated himself from governing. All agreed that under the terms of the decree, it was incumbent upon Göring to take power in Hitler's stead.[117] He was also motivated by fears that his rival, Martin Bormann, would seize power upon Hitler's death and would have him killed as a traitor. With this in mind, Göring sent a carefully worded telegram asking Hitler for permission to take over as the leader of Germany, stressing that he would be acting as Hitler's deputy. He added that, if Hitler did not reply by 22:00 that night (23 April), he would assume that Hitler had indeed lost his freedom of action, and would assume leadership of the Reich.[118]
The telegram was intercepted by Bormann, who convinced Hitler that Göring was a traitor. Bormann argued that Göring's telegram was not a request for permission to act as Hitler's deputy, but a demand to resign or be overthrown.[119] Bormann also intercepted another telegram in which Göring directed Ribbentrop to report to him if there was no further communication from Hitler or Göring before midnight.[120] Hitler sent a reply to Göring—prepared with Bormann's help—rescinding the 1941 decree and threatening him with execution for high treason unless he immediately resigned from all of his offices. Göring duly resigned. Afterwards, Hitler (or Bormann, depending on the source) ordered the SS to place Göring, his staff, and Lammers under house arrest at Obersalzberg.[119][121] Bormann made an announcement over the radio that Göring had resigned for health reasons.[122]

By 26 April, the complex at Obersalzberg was under attack by the Allies, so Göring was moved to his castle at Mauterndorf. In his last will and testament, Hitler expelled Göring from the party, formally rescinded the decree making him his successor, and upbraided Göring for ""illegally attempting to seize control of the state.""[123] He then appointed Karl Dönitz, the Navy's commander-in-chief, as president of the Reich and commander-in-chief of the armed forces. Hitler and his wife, Eva Braun, committed suicide on 30 April 1945, a few hours after a hastily arranged wedding. Göring was freed on 5 May by a passing Luftwaffe unit, and he made his way to the US lines in hopes of surrendering to them rather than to the Soviets. He was taken into custody near Radstadt on 6 May by elements of the 36th Infantry Division of the US Army.[124] This move likely saved Göring's life; Bormann had ordered him executed if Berlin had fallen.[125]Göring was flown to Camp Ashcan, a temporary prisoner-of-war camp housed in the Palace Hotel at Mondorf-les-Bains, Luxembourg. Here he was weaned off dihydrocodeine (a mild morphine derivative)—he had been taking the equivalent of three or four grains (260 to 320 mg) of morphine a day—and was put on a strict diet; he lost 60 pounds (27 kg). His IQ was tested while in custody and found to be 138.[126] Top Nazi officials were transferred in September to Nuremberg, which was to be the location of a series of military tribunals beginning in November.[127]
Göring was the second-highest-ranking official tried at Nuremberg, behind Reich President (former Admiral) Karl Dönitz. The prosecution levelled an indictment of four charges, including a charge of conspiracy; waging a war of aggression; war crimes, including the plundering and removal to Germany of works of art and other property; and crimes against humanity, including the disappearance of political and other opponents under the Nacht und Nebel (Night and Fog) decree; the torture and ill-treatment of prisoners of war; and the murder and enslavement of civilians, including what was at the time estimated to be 5,700,000 Jews. Not permitted to present a lengthy statement, Göring declared himself to be ""in the sense of the indictment not guilty"".[128]
The trial lasted 218 days; the prosecution presented their case from November through March, and Göring's defence—the first to be presented—lasted from 8 to 22 March. The sentences were read out on 30 September 1946.[129] Göring, forced to remain silent while seated in the dock, communicated his opinions about the proceedings using gestures, shaking his head, or laughing. He constantly took notes and whispered with the other defendants, and tried to control the erratic behaviour of Hess, who was seated beside him.[130] During breaks in the proceedings, Göring tried to dominate the other defendants, and he was eventually placed in solitary confinement when he attempted to influence their testimony.[131] Göring told US psychiatrist Leon Goldensohn that the court was ""stupid"" to try ""little fellows"" like Funk and Kaltenbrunner instead of letting Göring take all the blame on himself.[132] He also claimed that he had never heard of most of the other defendants before the trial.[132]
On several occasions over the course of the trial, the prosecution showed films of the concentration camps and other atrocities. Everyone present, including Göring, found the contents of the films shocking; he said that the films must have been faked. Witnesses, including Paul Koerner and Erhard Milch, tried to portray Göring as a peaceful moderate. Milch stated it had been impossible to oppose Hitler or disobey his orders; to do so would likely have meant death for oneself and one's family.[133] When testifying on his own behalf, Göring emphasised his loyalty to Hitler, and claimed to know nothing about what had happened in the concentration camps, which were under Himmler's control. He gave evasive, convoluted answers to direct questions and had plausible excuses for all his actions during the war. He used the witness stand as a venue to expound at great length on his own role in the Reich, attempting to present himself as a peacemaker and diplomat before the outbreak of the war.[134] During cross-examination, chief prosecutor Robert H. Jackson read out the minutes of a meeting that had been held shortly after Kristallnacht, a major pogrom in November 1938. At the meeting, Göring had plotted to confiscate Jewish property in the wake of the pogrom.[135] Later, David Maxwell-Fyfe proved it was impossible for Göring not to have known about the Stalag Luft III murders—the shooting of fifty airmen who had been recaptured after escaping from Stalag Luft III—in time to have prevented the killings.[136] He also presented clear evidence that Göring knew about the extermination of the Hungarian Jews.[137]
Göring was found guilty on all four counts and was sentenced to death by hanging. The judgment stated:
There is nothing to be said in mitigation. For Göring was often, indeed almost always, the moving force, second only to his leader. He was the leading war aggressor, both as political and as military leader; he was the director of the slave labour programme and the creator of the oppressive programme against the Jews and other races, at home and abroad. All of these crimes he has frankly admitted. On some specific cases there may be conflict of testimony, but in terms of the broad outline, his own admissions are more than sufficiently wide to be conclusive of his guilt. His guilt is unique in its enormity. The record discloses no excuses for this man.[138]Göring made an appeal asking to be shot as a soldier instead of hanged as a common criminal, but the court refused.[139] He committed suicide with a potassium cyanide capsule the night before he was to be hanged.[140]
One theory as to how Göring obtained the poison holds that US Army Lieutenant Jack G. Wheelis, who was stationed at the Nuremberg Trials, retrieved the capsules from their hiding place among Göring's personal effects that had been confiscated by the Army and handed them over to the prisoner,[141] after being bribed by Göring, who gave him his gold watch, pen, and cigarette case.[142] In 2005, former US Army Private Herbert Lee Stivers, who served in the 1st Infantry Division's 26th Infantry Regiment—the honour guard for the Nuremberg Trials—claimed he gave Göring ""medicine"" hidden inside a fountain pen that a German woman had asked him to smuggle into the prison. Stivers later said that he did not know what was in the pill until after Göring's suicide.[143]
Göring's body, as with those of the men who were executed, was displayed at the execution ground for the witnesses of the executions. The bodies were cremated at Ostfriedhof, Munich, and the ashes were scattered in the Isar River.[144][145][146]
Göring's name is closely associated with the Nazi plunder of Jewish property. His name appears 135 times on the OSS Art Looting Investigation Unit (ALIU) Red Flag Names List[147] compiled by US Army intelligence in 1945-6 and declassified in 1997.[148]
The confiscation of Jewish property gave Göring the opportunity to amass a personal fortune. Some properties he seized himself or acquired for a nominal price. In other cases, he collected bribes for allowing others to steal Jewish property. He took kickbacks from industrialists for favourable decisions as Four Year Plan director, and money for supplying arms to the Spanish Republicans in the Spanish Civil War via Pyrkal in Greece (although Germany was supporting Franco and the Nationalists).[149]
Göring was appointed Reich Master of the Hunt in 1933 and Master of the German Forests in 1934. He instituted reforms to the forestry laws and acted to protect endangered species. Around this time he became interested in Schorfheide Forest, where he set aside 100,000 acres (400 km2) as a state park, which is still extant. There he built an elaborate hunting lodge, Carinhall, in memory of his first wife, Carin. By 1934, her body had been transported to the site and placed in a vault on the estate.[150] Through most of the 1930s, Göring kept pet lion cubs, borrowed from the Berlin Zoo, both at Carinhall and at his house at Obersalzberg.[151] The main lodge at Carinhall had a large art gallery where Göring displayed works that had been plundered from private collections and museums around Europe from 1939 onward.[152][153] Göring worked closely with the Einsatzstab Reichsleiter Rosenberg (Reichsleiter Rosenberg Taskforce), an organisation tasked with the looting of artwork and cultural material from Jewish collections, libraries, and museums throughout Europe.[154] Headed by Alfred Rosenberg, the task force set up a collection centre and headquarters in Paris. Some 26,000 railroad cars full of art treasures, furniture, and other looted items were sent to Germany from France alone. Göring repeatedly visited the Paris headquarters to review the incoming stolen goods and to select items to be sent on a special train to Carinhall and his other homes.[155] The estimated value of his collection, which numbered some 1,500 pieces, was $200 million.[156]
Göring was known for his extravagant tastes and garish clothing. He had various special uniforms made for the many posts he held;[157] his Reichsmarschall uniform included a jewel-encrusted baton. Hans-Ulrich Rudel, the top Stuka pilot of the war, recalled twice meeting Göring dressed in outlandish costumes: first, a medieval hunting costume, practicing archery with his doctor; and second, dressed in a red toga fastened with a golden clasp, smoking an unusually large pipe. Italian Foreign Minister Galeazzo Ciano once noted Göring wearing a fur coat that looked like what ""a high grade prostitute wears to the opera.""[158] He threw lavish housewarming parties each time a round of construction was completed at Carinhall, and changed costumes several times throughout the evenings.[159]
Göring was noted for his patronage of music, especially opera. He entertained frequently and sumptuously, and hosted elaborate birthday parties for himself.[160] Armaments minister Albert Speer recalled that guests brought expensive gifts such as gold bars, Dutch cigars, and valuable artwork. For his birthday in 1944, Speer gave Göring an oversize marble bust of Hitler.[161] As a member of the Prussian Council of State, Speer was required to donate a considerable portion of his salary towards the Council's birthday gift to Göring without even being asked. Generalfeldmarschall Erhard Milch told Speer that similar donations were required out of the Air Ministry's general fund.[162] For his birthday in 1940, Italian Foreign Minister Count Ciano decorated Göring with the coveted Collar of Annunziata. The award reduced him to tears.[163]
The design of the Reichsmarschall standard, on a light blue field, featured a gold German eagle grasping a wreath surmounted by two batons overlaid with a swastika. The reverse side of the flag had the Großkreuz des Eisernen Kreuzes (""Grand Cross of the Iron Cross"") surrounded by a wreath between four Luftwaffe eagles. The flag was carried by a personal standard-bearer at all public occasions.

Though he liked to be called ""der Eiserne"" (the Iron Man), the once dashing and muscular fighter pilot had become corpulent. He was one of the few Nazi leaders who did not take offence at hearing jokes about himself, ""no matter how rude"", taking them as a sign of popularity. Germans joked about his ego, saying that he would wear an admiral's uniform with rubber medals to take a bath, and his obesity, joking that ""he sits down on his stomach"".[164][165] One joke claimed that he had sent a wire to Hitler after his visit to the Vatican: ""Mission accomplished. Pope unfrocked. Tiara and pontifical vestments are a perfect fit.""[166]Joseph Goebbels and Heinrich Himmler were far more antisemitic than Göring, who mainly adopted that attitude because party politics required him to do so.[167] His deputy, Erhard Milch, had a Jewish parent. But Göring supported the Nuremberg Laws of 1935, and later initiated economic measures unfavourable to Jews.[167] He required the registration of all Jewish property as part of the Four Year Plan, and at a meeting held after Kristallnacht was livid that the financial burden for the Jewish losses would have to be made good by German-owned insurance companies. He proposed that the Jews be fined one billion marks.[168]
At the same meeting, options for the disposition of the Jews and their property were discussed. Jews would be segregated into ghettos or encouraged to emigrate, and their property would be seized in a programme of Aryanization. Compensation for seized property would be low, if any was given at all.[168] Detailed minutes of this meeting and other documents were read out at the Nuremberg trial, proving his knowledge of and complicity with the persecution of the Jews.[135] He told Gilbert that he would never have supported the anti-Jewish measures if he had known what was going to happen. ""I only thought we would eliminate Jews from positions in big business and government"", he claimed.[169]
In July 1941, Göring issued a memo to Reinhard Heydrich ordering him to organise the practical details of the Final Solution to the ""Jewish Question"". By the time that this letter was written, many Jews and others had already been killed in Poland, Russia, and elsewhere. At the Wannsee Conference, held six months later, Heydrich formally announced that genocide of the Jews was now official Reich policy. Göring did not attend the conference, but he was present at other meetings where the number of people killed was discussed.[170][171]
Göring directed anti-partisan operations by Luftwaffe security battalions in the Białowieża Forest between 1942 and 1944 that resulted in the murder of thousands of Jews and Polish civilians.[172]
Further reading
"
Signals intelligence - Wikipedia," 
Signals intelligence (SIGINT) is intelligence-gathering by interception of signals, whether communications between people (communications intelligence—abbreviated to COMINT) or from electronic signals not directly used in communication (electronic intelligence—abbreviated to ELINT). Signals intelligence is a subset of intelligence collection management.
As sensitive information is often encrypted, signals intelligence in turn involves the use of cryptanalysis to decipher the messages. Traffic analysis—the study of who is signaling whom and in what quantity—is also used to integrate information again.[citation needed]
Electronic interceptions appeared as early as 1900, during the Boer War of 1899–1902. The British Royal Navy had installed wireless sets produced by Marconi on board their ships in the late 1890s and the British Army used some limited wireless signalling. The Boers captured some wireless sets and used them to make vital transmissions.[citation needed] Since the British were the only people transmitting at the time, no special interpretation of the signals that were intercepted by the British was necessary.[1]
The birth of signals intelligence in a modern sense dates from the Russo-Japanese War of 1904–1905. As the Russian fleet prepared for conflict with Japan in 1904, the British ship HMS Diana stationed in the Suez Canal intercepted Russian naval wireless signals being sent out for the mobilization of the fleet, for the first time in history.[2]
Over the course of the First World War, the new method of signals intelligence reached maturity.[3] Failure to properly protect its communications fatally compromised the Russian Army in its advance early in World War I and led to their disastrous defeat by the Germans under Ludendorff and Hindenburg at the Battle of Tannenberg. In 1918, French intercept personnel captured a message written in the new ADFGVX cipher, which was cryptanalyzed by Georges Painvin. This gave the Allies advance warning of the German 1918 Spring Offensive.
The British in particular built up great expertise in the newly emerging field of signals intelligence and codebreaking. On the declaration of war, Britain cut all German undersea cables.[4] This forced the Germans to use either a telegraph line that connected through the British network and could be tapped, or through radio which the British could then intercept.[5] Rear-Admiral Henry Oliver appointed Sir Alfred Ewing to establish an interception and decryption service at the Admiralty; Room 40.[5] An interception service known as 'Y' service, together with the post office and Marconi stations grew rapidly to the point where the British could intercept almost all official German messages.[5]
The German fleet was in the habit each day of wirelessing the exact position of each ship and giving regular position reports when at sea. It was possible to build up a precise picture of the normal operation of the High Seas Fleet, to infer from the routes they chose where defensive minefields had been placed and where it was safe for ships to operate. Whenever a change to the normal pattern was seen, it immediately signalled that some operation was about to take place and a warning could be given. Detailed information about submarine movements was also available.[5]
The use of radio receiving equipment to pinpoint the location of the transmitter was also developed during the war.
Captain H.J. Round working for Marconi, began carrying out experiments with direction finding radio equipment for the army in France in 1915. By May 1915, the Admiralty was able to track German submarines crossing the North Sea. Some of these stations also acted as 'Y' stations to collect German messages, but a new section was created within Room 40 to plot the positions of ships from the directional reports.[5]
Room 40 played an important role in several naval engagements during the war, notably in detecting major German sorties into the North Sea. The battle of Dogger Bank was won in no small part due to the intercepts that allowed the Navy to position its ships in the right place.[6] It played a vital role in subsequent naval clashes, including at the Battle of Jutland as the British fleet was sent out to intercept them. The direction-finding capability allowed for the tracking and location of German ships, submarines and Zeppelins. The system was so successful, that by the end of the war over 80 million words, comprising the totality of German wireless transmission over the course of the war had been intercepted by the operators of the Y-stations and decrypted.[7] However its most astonishing success was in decrypting the Zimmermann Telegram, a telegram from the German Foreign Office sent via Washington to its ambassador Heinrich von Eckardt in Mexico.
With the importance of interception and decryption firmly established by the wartime experience, countries established permanent agencies dedicated to this task in the interwar period. In 1919, the British Cabinet's Secret Service Committee, chaired by Lord Curzon, recommended that a peace-time codebreaking agency should be created.[8] The Government Code and Cypher School (GC&CS) was the first peace-time codebreaking agency, with a public function ""to advise as to the security of codes and cyphers used by all Government departments and to assist in their provision"", but also with a secret directive to ""study the methods of cypher communications used by foreign powers"".[9] GC&CS officially formed on 1 November 1919, and produced its first decrypt on 19 October.[8][10] By 1940, GC&CS was working on the diplomatic codes and ciphers of 26 countries, tackling over 150 diplomatic cryptosystems.[11]
The US Cipher Bureau was established in 1919 and achieved some success at the Washington Naval Conference in 1921, through cryptanalysis by Herbert Yardley. Secretary of War Henry L. Stimson closed the US Cipher Bureau in 1929 with the words ""Gentlemen do not read each other's mail.""
The use of SIGINT had even greater implications during World War II. The combined effort of intercepts and cryptanalysis for the whole of the British forces in World War II came under the code name ""Ultra"" managed from Government Code and Cypher School at Bletchley Park. Properly used, the German Enigma and Lorenz ciphers should have been virtually unbreakable, but flaws in German cryptographic procedures, and poor discipline among the personnel carrying them out, created vulnerabilities which made Bletchley's attacks feasible.
Bletchley's work was essential to defeating the U-boats in the Battle of the Atlantic, and to the British naval victories in the Battle of Cape Matapan and the Battle of North Cape. In 1941, Ultra exerted a powerful effect on the North African desert campaign against German forces under General Erwin Rommel. General Sir Claude Auchinleck wrote that were it not for Ultra, ""Rommel would have certainly got through to Cairo"". ""Ultra"" decrypts featured prominently in the story of Operation SALAM, László Almásy's mission across the desert behind Allied lines in 1942.[12] Prior to the Normandy landings on D-Day in June 1944, the Allies knew the locations of all but two of Germany's fifty-eight Western-front divisions.
Winston Churchill was reported to have told King George VI: ""It is thanks to the secret weapon of General Menzies, put into use on all the fronts, that we won the war!"" Supreme Allied Commander, Dwight D. Eisenhower, at the end of the war, described Ultra as having been ""decisive"" to Allied victory.[13] Official historian of British Intelligence in World War II Sir Harry Hinsley, argued that Ultra shortened the war ""by not less than two years and probably by four years""; and that, in the absence of Ultra, it is uncertain how the war would have ended.[14]
The United States Department of Defense has defined the term ""signals intelligence"" as:
Being a broad field, SIGINT has many sub-disciplines. The two main ones are communications intelligence (COMINT) and electronic intelligence (ELINT).
A collection system has to know to look for a particular signal. ""System"", in this context, has several nuances. Targeting is an output of the process of developing collection requirements:
First, atmospheric conditions, sunspots, the target's transmission schedule and antenna characteristics, and other factors create uncertainty that a given signal intercept sensor will be able to ""hear"" the signal of interest, even with a geographically fixed target and an opponent making no attempt to evade interception. Basic countermeasures against interception include frequent changing of radio frequency, polarization, and other transmission characteristics. An intercept aircraft could not get off the ground if it had to carry antennas and receivers for every possible frequency and signal type to deal with such countermeasures.
Second, locating the transmitter's position is usually part of SIGINT. Triangulation and more sophisticated radio location techniques, such as time of arrival methods, require multiple receiving points at different locations. These receivers send location-relevant information to a central point, or perhaps to a distributed system in which all participate, such that the information can be correlated and a location computed.
Modern SIGINT systems, therefore, have substantial communications among intercept platforms. Even if some platforms are clandestine, there is still a broadcast of information telling them where and how to look for signals.[16] A United States targeting system under development in the late 1990s, PSTS, constantly sends out information that helps the interceptors properly aim their antennas and tune their receivers. Larger intercept aircraft, such as the EP-3 or RC-135, have the on-board capability to do some target analysis and planning, but others, such as the RC-12 GUARDRAIL, are completely under ground direction. GUARDRAIL aircraft are fairly small, and usually work in units of three to cover a tactical SIGINT requirement, where the larger aircraft tend to be assigned strategic/national missions.
Before the detailed process of targeting begins, someone has to decide there is a value in collecting information about something. While it would be possible to direct signals intelligence collection at a major sports event, the systems would capture a great deal of noise, news signals, and perhaps announcements in the stadium. If, however, an anti-terrorist organization believed that a small group would be trying to coordinate their efforts, using short-range unlicensed radios, at the event, SIGINT targeting of radios of that type would be reasonable. Targeting would not know where in the stadium the radios might be located, or the exact frequency they are using; those are the functions of subsequent steps such as signal detection and direction finding.
Once the decision to target is made, the various interception points need to cooperate, since resources are limited.
Knowing what interception equipment to use becomes easier when a target country buys its radars and radios from known manufacturers, or is given them as military aid. National intelligence services keep libraries of devices manufactured by their own country and others, and then use a variety of techniques to learn what equipment is acquired by a given country.
Knowledge of physics and electronic engineering further narrows the problem of what types of equipment might be in use. An intelligence aircraft flying well outside the borders of another country will listen for long-range search radars, not short-range fire control radars that would be used by a mobile air defense. Soldiers scouting the front lines of another army know that the other side will be using radios that must be portable and not have huge antennas.
Even if a signal is human communications (e.g., a radio), the intelligence collection specialists have to know it exists. If the targeting function described above learns that a country has a radar that operates in a certain frequency range, the first step is to use a sensitive receiver, with one or more antennas that listen in every direction, to find an area where such a radar is operating. Once the radar is known to be in the area, the next step is to find its location.
If operators know the probable frequencies of transmissions of interest, they may use a set of receivers, preset to the frequencies of interest. These are the frequency (horizontal axis) versus power (vertical axis) produced at the transmitter, before any filtering of signals that do not add to the information being transmitted. Received energy on a particular frequency may start a recorder, and alert a human to listen to the signals if they are intelligible (i.e., COMINT). If the frequency is not known, the operators may look for power on primary or sideband frequencies using a spectrum analyzer. Information from the spectrum analyzer is then used to tune receivers to signals of interest. For example, in this simplified spectrum, the actual information is at 800 kHz and 1.2 MHz.
Real-world transmitters and receivers usually are directional. In the figure to the left, assume that each display is connected to a spectrum analyzer connected to a directional antenna aimed in the indicated direction.
Spread-spectrum communications is an electronic counter-countermeasures (ECCM) technique to defeat looking for particular frequencies. Spectrum analysis can be used in a different ECCM way to identify frequencies not being jammed or not in use.
The earliest, and still common, means of direction finding is to use directional antennas as goniometers, so that a line can be drawn from the receiver through the position of the signal of interest. (See HF/DF.) Knowing the compass bearing, from a single point, to the transmitter does not locate it. Where the bearings from multiple points, using goniometry, are plotted on a map, the transmitter will be located at the point where the bearings intersect. This is the simplest case; a target may try to confuse listeners by having multiple transmitters, giving the same signal from different locations, switching on and off in a pattern known to their user but apparently random to the listener.
Individual directional antennas have to be manually or automatically turned to find the signal direction, which may be too slow when the signal is of short duration. One alternative is the Wullenweber array technique. In this method, several concentric rings of antenna elements simultaneously receive the signal, so that the best bearing will ideally be clearly on a single antenna or a small set. Wullenweber arrays for high-frequency signals are enormous, referred to as ""elephant cages"" by their users.
An alternative to tunable directional antennas, or large omnidirectional arrays such as the Wullenweber, is to measure the time of arrival of the signal at multiple points, using GPS or a similar method to have precise time synchronization. Receivers can be on ground stations, ships, aircraft, or satellites, giving great flexibility.
Modern anti-radiation missiles can home in on and attack transmitters; military antennas are rarely a safe distance from the user of the transmitter.
When locations are known, usage patterns may emerge, from which inferences may be drawn. Traffic analysis is the discipline of drawing patterns from information flow among a set of senders and receivers, whether those senders and receivers are designated by location determined through direction finding, by addressee and sender identifications in the message, or even MASINT techniques for ""fingerprinting"" transmitters or operators. Message content, other than the sender and receiver, is not necessary to do traffic analysis, although more information can be helpful.
For example, if a certain type of radio is known to be used only by tank units, even if the position is not precisely determined by direction finding, it may be assumed that a tank unit is in the general area of the signal. The owner of the transmitter can assume someone is listening, so might set up tank radios in an area where he wants the other side to believe he has actual tanks. As part of Operation Quicksilver, part of the deception plan for the invasion of Europe at the Battle of Normandy, radio transmissions simulated the headquarters and subordinate units of the fictitious First United States Army Group (FUSAG), commanded by George S. Patton, to make the German defense think that the main invasion was to come at another location. In like manner, fake radio transmissions from Japanese aircraft carriers, before the Battle of Pearl Harbor, were made from Japanese local waters, while the attacking ships moved under strict radio silence.
Traffic analysis need not focus on human communications. For example, if the sequence of a radar signal, followed by an exchange of targeting data and a confirmation, followed by observation of artillery fire, this may identify an automated counterbattery system. A radio signal that triggers navigational beacons could be a landing aid system for an airstrip or helicopter pad that is intended to be low-profile.
Patterns do emerge. Knowing a radio signal, with certain characteristics, originating from a fixed headquarters may be strongly suggestive that a particular unit will soon move out of its regular base. The contents of the message need not be known to infer the movement.
There is an art as well as science of traffic analysis. Expert analysts develop a sense for what is real and what is deceptive. Harry Kidder,[17] for example, was one of the star cryptanalysts of World War II, a star hidden behind the secret curtain of SIGINT.[18]
Generating an electronic order of battle (EOB) requires identifying SIGINT emitters in an area of interest, determining their geographic location or range of mobility, characterizing their signals, and, where possible, determining their role in the broader organizational order of battle. EOB covers both COMINT and ELINT.[19] The Defense Intelligence Agency maintains an EOB by location. The Joint Spectrum Center (JSC) of the Defense Information Systems Agency supplements this location database with five more technical databases:
For example, several voice transmitters might be identified as the command net (i.e., top commander and direct reports) in a tank battalion or tank-heavy task force. Another set of transmitters might identify the logistic net for that same unit. An inventory of ELINT sources might identify the medium- and long-range counter-artillery radars in a given area.
Signals intelligence units will identify changes in the EOB, which might indicate enemy unit movement, changes in command relationships, and increases or decreases in capability.
Using the COMINT gathering method enables the intelligence officer to produce an electronic order of battle by traffic analysis and content analysis among several enemy units. For example, if the following messages were intercepted:
This sequence shows that there are two units in the battlefield, unit 1 is mobile, while unit 2 is in a higher hierarchical level, perhaps a command post. One can also understand that unit 1 moved from one point to another which are distant from each 20 minutes with a vehicle. If these are regular reports over a period of time, they might reveal a patrol pattern. Direction-finding and radio frequency MASINT could help confirm that the traffic is not deception.
The EOB buildup process is divided as following:
Separation of the intercepted spectrum and the signals intercepted from each sensor must take place in an extremely small period of time, in order to separate the different signals to different transmitters in the battlefield. The complexity of the separation process depends on the complexity of the transmission methods (e.g., hopping or time division multiple access (TDMA)).
By gathering and clustering data from each sensor, the measurements of the direction of signals can be optimized and get much more accurate than the basic measurements of a standard direction finding sensor.[20] By calculating larger samples of the sensor's output data in near real-time, together with historical information of signals, better results are achieved.
Data fusion correlates data samples from different frequencies from the same sensor, ""same"" being confirmed by direction finding or radiofrequency MASINT. If an emitter is mobile, direction finding, other than discovering a repetitive pattern of movement, is of limited value in determining if a sensor is unique. MASINT then becomes more informative, as individual transmitters and antennas may have unique side lobes, unintentional radiation, pulse timing, etc.
Network build-up, or analysis of emitters (communication transmitters) in a target region over a sufficient period of time, enables creation of the communications flows of a battlefield.[21]
COMINT (Communications Intelligence) is a sub-category of signals intelligence that engages in dealing with messages or voice information derived from the interception of foreign communications. COMINT is commonly referred to as SIGINT, which can cause confusion when talking about the broader intelligence disciplines. The US Joint Chiefs of Staff defines it as ""Technical information and intelligence derived from foreign communications by other than the intended recipients"".[15]
COMINT, which is defined to be communications among people, will reveal some or all of the following:
A basic COMINT technique is to listen for voice communications, usually over radio but possibly ""leaking"" from telephones or from wiretaps. If the voice communications are encrypted, traffic analysis may still give information.
In the Second World War, for security the United States used Native American volunteer communicators known as code talkers, who used languages such as Navajo, Comanche and Choctaw, which would be understood by few people, even in the U.S. Even within these uncommon languages, the code talkers used specialized codes, so a ""butterfly"" might be a specific Japanese aircraft. British forces made limited use of Welsh speakers for the same reason.
While modern electronic encryption does away with the need for armies to use obscure languages, it is likely that some groups might use rare dialects that few outside their ethnic group would understand.
Morse code interception was once very important, but Morse code telegraphy is now obsolete in the western world, although possibly used by special operations forces. Such forces, however, now have portable cryptographic equipment. Morse code is still used by military forces of former Soviet Union countries.
Specialists scan radio frequencies for character sequences (e.g., electronic mail) and fax.
A given digital communications link can carry thousands or millions of voice communications, especially in developed countries. Without addressing the legality of such actions, the problem of identifying which channel contains which conversation becomes much simpler when the first thing intercepted is the signaling channel that carries information to set up telephone calls. In civilian and many military use, this channel will carry messages in Signaling System 7 protocols.
Retrospective analysis of telephone calls can be made from Call detail record (CDR) used for billing the calls.
More a part of communications security than true intelligence collection, SIGINT units still may have the responsibility of monitoring one's own communications or other electronic emissions, to avoid providing intelligence to the enemy. For example, a security monitor may hear an individual transmitting inappropriate information over an unencrypted radio network, or simply one that is not authorized for the type of information being given. If immediately calling attention to the violation would not create an even greater security risk, the monitor will call out one of the BEADWINDOW codes[22] used by Australia, Canada, New Zealand, the United Kingdom, the United States, and other nations working under their procedures. Standard BEADWINDOW codes (e.g., ""BEADWINDOW 2"") include:
In WWII, for example, the Japanese Navy, by poor practice, identified a key person's movement over a low-security cryptosystem. This made possible Operation Vengeance, the interception and death of the Combined Fleet commander, Admiral Isoroku Yamamoto.
Electronic signals intelligence (ELINT) refers to intelligence-gathering by use of electronic sensors. Its primary focus lies on non-communications signals intelligence. The Joint Chiefs of Staff define it as ""Technical and geolocation intelligence derived from foreign noncommunications electromagnetic radiations emanating from sources other than nuclear detonations or radioactive sources.""[15]
Signal identification is performed by analyzing the collected parameters of a specific signal, and either matching it to known criteria, or recording it as a possible new emitter. ELINT data are usually highly classified, and are protected as such.
The data gathered are typically pertinent to the electronics of an opponent's defense network, especially the electronic parts such as radars, surface-to-air missile systems, aircraft, etc. ELINT can be used to detect ships and aircraft by their radar and other electromagnetic radiation; commanders have to make choices between not using radar (EMCON), intermittently using it, or using it and expecting to avoid defenses. ELINT can be collected from ground stations near the opponent's territory, ships off their coast, aircraft near or in their airspace, or by satellite.
Combining other sources of information and ELINT allows traffic analysis to be performed on electronic emissions which contain human encoded messages. The method of analysis differs from SIGINT in that any human encoded message which is in the electronic transmission is not analyzed during ELINT. What is of interest is the type of electronic transmission and its location. For example, during the Battle of the Atlantic in World War II, Ultra COMINT was not always available because Bletchley Park was not always able to read the U-boat Enigma traffic. But high-frequency direction finding (""huff-duff"") was still able to detect U-boats by analysis of radio transmissions and the positions through triangulation from the direction located by two or more huff-duff systems. The Admiralty was able to use this information to plot courses which took convoys away from high concentrations of U-boats.
Other ELINT disciplines include intercepting and analyzing enemy weapons control signals, or the identification, friend or foe responses from transponders in aircraft used to distinguish enemy craft from friendly ones.
A very common area of ELINT is intercepting radars and learning their locations and operating procedures. Attacking forces may be able to avoid the coverage of certain radars, or, knowing their characteristics, electronic warfare units may jam radars or send them deceptive signals. Confusing a radar electronically is called a ""soft kill"", but military units will also send specialized missiles at radars, or bomb them, to get a ""hard kill"". Some modern air-to-air missiles also have radar homing guidance systems, particularly for use against large airborne radars.
Knowing where each surface-to-air missile and anti-aircraft artillery system is and its type means that air raids can be plotted to avoid the most heavily defended areas and to fly on a flight profile which will give the aircraft the best chance of evading ground fire and fighter patrols. It also allows for the jamming or spoofing of the enemy's defense network (see electronic warfare). Good electronic intelligence can be very important to stealth operations; stealth aircraft are not totally undetectable and need to know which areas to avoid. Similarly, conventional aircraft need to know where fixed or semi-mobile air defense systems are so that they can shut them down or fly around them.

Electronic support measures (ESM) or electronic surveillance measures are ELINT techniques using various electronic surveillance systems, but the term is used in the specific context of tactical warfare. ESM give the information needed for electronic attack (EA) such as jamming, or directional bearings (compass angle) to a target in signals intercept such as in the huff-duff radio direction finding (RDF) systems so critically important during the World War II Battle of the Atlantic. After WWII, the RDF, originally applied only in communications, was broadened into systems to also take in ELINT from radar bandwidths and lower frequency communications systems, giving birth to a family of NATO ESM systems, such as the shipboard US AN/WLR-1[23]—AN/WLR-6 systems and comparable airborne units. EA is also called electronic counter-measures (ECM). ESM provides information needed for electronic counter-counter measures (ECCM), such as understanding a spoofing or jamming mode so one can change one's radar characteristics to avoid them.
Meaconing[24] is the combined intelligence and electronic warfare of learning the characteristics of enemy navigation aids, such as radio beacons, and retransmitting them with incorrect information.
FISINT (Foreign instrumentation signals intelligence) is a sub-category of SIGINT, monitoring primarily non-human communication. Foreign instrumentation signals include (but not limited to) telemetry (TELINT), tracking systems, and video data links. TELINT is an important part of national means of technical verification for arms control.
Still at the research level are techniques that can only be described as counter-ELINT, which would be part of a SEAD campaign. It may be informative to compare and contrast counter-ELINT with ECCM.
Signals intelligence and measurement and signature intelligence (MASINT) are closely, and sometimes confusingly, related.[25]
The signals intelligence disciplines of communications and electronic intelligence focus on the information in those signals themselves, as with COMINT detecting the speech in a voice communication or ELINT measuring the frequency, pulse repetition rate, and other characteristics of a radar.
MASINT also works with collected signals, but is more of an analysis discipline. There are, however, unique MASINT sensors, typically working in different regions or domains of the electromagnetic spectrum, such as infrared or magnetic fields. While NSA and other agencies have MASINT groups, the Central MASINT Office is in the Defense Intelligence Agency (DIA).
Where COMINT and ELINT focus on the intentionally transmitted part of the signal, MASINT focuses on unintentionally transmitted information. For example, a given radar antenna will have sidelobes emanating from a direction other than that in which the main antenna is aimed. The RADINT (radar intelligence) discipline involves learning to recognize a radar both by its primary signal, captured by ELINT, and its sidelobes, perhaps captured by the main ELINT sensor, or, more likely, a sensor aimed at the sides of the radio antenna.
MASINT associated with COMINT might involve the detection of common background sounds expected with human voice communications. For example, if a given radio signal comes from a radio used in a tank, if the interceptor does not hear engine noise or higher voice frequency than the voice modulation usually uses, even though the voice conversation is meaningful, MASINT might suggest it is a deception, not coming from a real tank.
See HF/DF for a discussion of SIGINT-captured information with a MASINT flavor, such as determining the frequency to which a receiver is tuned, from detecting the frequency of the beat frequency oscillator of the superheterodyne receiver.
Since the invention of the radio, the international consensus has been that the radio-waves are no one's property, and thus the interception itself is not illegal.[26] There can however be national laws on who is allowed to collect, store and process radio traffic, and for what purposes.
Monitoring traffic in cables (i.e. telephone and Internet) is far more controversial, since it most of the time requires physical access to the cable and thereby violating ownership and expected privacy.
"
Navajo language - Wikipedia," Navajo or Navaho (/ˈnævəhoʊ, ˈnɑː-/;[3] Navajo: Diné bizaad [tìnépìz̥ɑ̀ːt] or Naabeehó bizaad [nɑ̀ːpèːhópìz̥ɑ̀ːt]) is a Southern Athabaskan language of the Na-Dené family, through which it is related to languages spoken across the western areas of North America. Navajo is spoken primarily in the Southwestern United States, especially on the Navajo Nation. It is one of the most widely spoken Native American languages and is the most widely spoken north of the Mexico–United States border, with almost 170,000 Americans speaking Navajo at home as of 2011. The language has struggled to keep a healthy speaker base, although this problem has been alleviated to some extent by extensive education programs on the Navajo Nation.
The language has a fairly large phoneme inventory; it includes several uncommon consonants that are not found in English. Its four basic vowels are distinguished for nasality, length, and tone. It has both agglutinative and fusional elements: it relies on affixes to modify verbs, and nouns are typically created from multiple morphemes, but in both cases these morphemes are fused irregularly and beyond easy recognition. Basic word order is subject–object–verb, though it is highly flexible to pragmatic factors. Verbs are conjugated for aspect and mood, and given affixes for the person and number of both subjects and objects, as well as a host of other variables.
The language's orthography, which was developed in the late 1930s after a series of prior attempts, is based on the Latin script. Most Navajo vocabulary is Athabaskan in origin, as the language has been conservative with loanwords since its early stages.
The word Navajo is an exonym: it comes from the Tewa word Navahu, which combines the roots nava ('field') and hu ('valley') to mean 'large field'. It was borrowed into Spanish to refer to an area of present-day northwestern New Mexico, and later into English for the Navajo tribe and their language.[4] The alternative spelling Navaho is considered antiquated; even anthropologist Berard Haile spelled it with a ""j"" in accordance with contemporary usage despite his personal objections.[5] The Navajo refer to themselves as the Diné ('People'), with their language known as Diné bizaad ('People's language')[6] or Naabeehó bizaad.
Navajo is an Athabaskan language; Navajo and Apache languages make up the southernmost branch of the family. Most of the other Athabaskan languages are located in Alaska and along the North American Pacific coast.
Most languages in the Athabaskan family have tone. However, this feature evolved independently in all subgroups; Proto-Athabaskan had no tones.[7] In each case, tone evolved from glottalic consonants at the ends of morphemes; however, the progression of these consonants into tones has not been consistent, with some related morphemes being pronounced with high tones in some Athabaskan languages and low tones in others. It has been posited that Navajo and Chipewyan, which have no common ancestor more recent than Proto-Athabaskan and possess many pairs of corresponding but opposite tones, evolved from different dialects of Proto-Athabaskan that pronounced these glottalic consonants differently.[8] Proto-Athabaskan diverged fully into separate languages circa 500 BCE.[9]
Navajo is most closely related to Western Apache, with which it shares a similar tonal scheme[10] and more than 92 percent of its vocabulary. It is estimated that the Apachean linguistic groups separated and became established as distinct societies, of which the Navajo were one, somewhere between 1300 and 1525.[11] As a member of the Western Apachean group, Navajo's closest relative is the Mescalero-Chiricahua language.[12] Navajo is generally considered mutually intelligible with all other Apachean languages.[13]
The Apachean languages, of which Navajo is one, are thought to have arrived in the American Southwest from the north by 1500 CE, probably passing through Alberta and Wyoming.[14][15] Archaeological finds considered to be proto-Navajo have been located in the far northern New Mexico around the La Plata, Animas and Pine rivers, dating to around 1500. In 1936, linguist Edward Sapir showed how the arrival of the Navajo people in the new arid climate among the corn agriculturalists of the Pueblo area was reflected in their language by tracing the changing meanings of words from Proto-Athabaskan to Navajo. For example, the word *dè:, which in Proto-Athabaskan meant ""horn"" and ""dipper made from animal horn"", in Navajo came to mean ""gourd"" or ""dipper made from gourd"". Likewise, the Proto-Athabaskan word *ɫ-yáxs ""snow lies on the ground"" in Navajo became sàs ""corn lies on the ground"". Similarly, the Navajo word for ""corn"" is nà:-dą:, derived from two Proto-Athabaskan roots meaning ""enemy"" and ""food"", suggesting that the Navajo originally considered corn to be ""food of the enemy"" when they first arrived among the Pueblo people.[16][17]
Navajo lands were initially colonized by the Spanish in the early nineteenth century, shortly after this area was ""annexed"" as part of the Spanish colony of Mexico. When the United States annexed these territories in 1848 following the Mexican–American War,[18] the English-speaking settlers allowed[citation needed] Navajo children to attend their schools. In some cases, the United States established separate schools for Navajo and other Native American children. In the late 19th century, it founded boarding schools, often operated by religious missionary groups. In efforts to acculturate the children, school authorities insisted that they learn to speak English and practice Christianity. Students routinely had their mouths washed out with lye soap as a punishment if they did speak Navajo.[19] Consequently, when these students grew up and had children of their own, they often did not teach them Navajo, in order to prevent them from being punished.[20]
Robert W. Young and William Morgan (Navajo), who both worked for the Navajo Agency of the Bureau of Indian Affairs, developed and published a practical orthography in 1937. It helped spread education among Navajo speakers.[21] In 1943 the men collaborated on The Navajo Language, a dictionary organized by the roots of the language.[22] In World War II, the United States military used speakers of Navajo as code talkers – to transmit top-secret military messages over telephone and radio in a code based on Navajo. The language was considered ideal because of its grammar, which differs strongly from that of German and Japanese, and because no published Navajo dictionaries existed at the time.[23]
Despite gaining new scholarly attention and being documented, the language declined in use. By the 1960s, indigenous languages of the United States had been declining in use for some time. Native American language use began to decline more quickly in this decade as paved roads were built and English-language radio was broadcast to tribal areas. Navajo was no exception, although its large speaker pool—larger than that of any other Native language in the United States—gave it more staying power than most.[24] Adding to the language's decline, federal acts passed in the 1950s to increase educational opportunities for Navajo children had resulted in pervasive use of English in their schools.[25]
In 1968, U.S. President Lyndon B. Johnson signed the Bilingual Education Act, which provided funds for educating young students who are not native English speakers. The Act had mainly been intended for Spanish-speaking children—particularly Mexican Americans—but it applied to all recognized linguistic minorities. Many Native American tribes seized the chance to establish their own bilingual education programs. However, qualified teachers who were fluent in Native languages were scarce, and these programs were largely unsuccessful.[24]
However, data collected in 1980 showed that 85 percent of Navajo first-graders were bilingual, compared to 62 percent of Navajo of all ages – early evidence of a resurgence of use of their traditional language among younger people.[26] In 1984, to counteract the language's historical decline, the Navajo Nation Council decreed that the Navajo language would be available and comprehensive for students of all grade levels in schools of the Navajo Nation.[24] This effort was aided by the fact that, largely due to the work of Young and Morgan, Navajo is one of the best-documented Native American languages. In 1980 they published a monumental expansion of their work on the language, organized by word (first initial of vowel or consonant) in the pattern of English dictionaries, as requested by Navajo students. The Navajo Language: A Grammar and Colloquial Dictionary also included a 400-page grammar, making it invaluable for both native speakers and students of the language. Particularly in its organization of verbs, it was oriented to Navajo speakers.[27] They expanded this work again in 1987, with several significant additions, and this edition continues to be used as an important text.[22]
The Native American language education movement has been met with adversity, such as by English-only campaigns in some areas in the late 1990s. However, Navajo-immersion programs have cropped up across the Navajo Nation. Statistical evidence shows that Navajo-immersion students generally do better on standardized tests than their counterparts educated only in English. Some educators have remarked that students who know their native languages feel a sense of pride and identity validation.[28] Since 1989, Diné College, a Navajo tribal community college, has offered an associate degree in the subject of Navajo.[29] This program includes language, literature, culture, medical terminology, and teaching courses and produces the highest number of Navajo teachers of any institution in the United States.
About 600 students attend per semester.[30] One major university that teaches classes in the Navajo language is Arizona State University.[31] In 1992, Young and Morgan published another major work on Navajo: Analytical Lexicon of Navajo, with the assistance of Sally Midgette (Navajo). This work is organized by root, the basis of Athabaskan languages.[22]
A 1991 survey of 682 preschoolers on the Navajo Reservation Head Start program found that 54 percent were monolingual English speakers, 28 percent were bilingual in English and Navajo, and 18 percent spoke only Navajo. This study noted that while the preschool staff knew both languages, they spoke English to the children most of the time. In addition, most of the children's parents spoke to the children in English more often than in Navajo. The study concluded that the preschoolers were in ""almost total immersion in English"".[32] An American Community Survey taken in 2011 found that 169,369 Americans spoke Navajo at home – 0.3 percent of Americans whose primary home language was not English. Of primary Navajo speakers, 78.8 percent reported they spoke English ""very well"", a fairly high percentage overall but less than among other Americans speaking a different Native American language (85.4 percent). Navajo was the only Native American language afforded its own category in the survey; domestic Navajo speakers represented 46.4 percent of all domestic Native language speakers (only 195,407 Americans have a different home Native language).[1] As of July 2014, Ethnologue classes Navajo as ""6b"" (In Trouble), signifying that few, but some, parents teach the language to their offspring and that concerted efforts at revitalization could easily protect the language. Navajo had a high population for a language in this category.[33] About half of all Navajo people live on Navajo Nation land, an area spanning parts of Arizona, New Mexico, and Utah; others are dispersed throughout the United States.[18] Under tribal law, fluency in Navajo is mandatory for candidates to the office of the President of the Navajo Nation.[34]
Both original and translated media have been produced in Navajo. The first works tended to be religious texts translated by missionaries, including the Bible. From 1943 to about 1957, the Navajo Agency of the BIA published Ádahooníłígíí (""Events""[35]), the first newspaper in Navajo and the only one to be written entirely in Navajo. It was edited by Robert W. Young and William Morgan, Sr. (Navajo). They had collaborated on The Navajo Language, a major language dictionary published that same year, and continued to work on studying and documenting the language in major works for the next few decades.[22] Today an AM radio station, KTNN, broadcasts in Navajo and English, with programming including music and NFL games;[36] AM station KNDN broadcasts only in Navajo.[37] When Super Bowl XXX was broadcast in Navajo in 1996, it was the first time a Super Bowl had been carried in a Native American language.[38] In 2013, the 1977 film Star Wars was translated into Navajo. It was the first major motion picture translated into any Native American language.[39][40]
On October 5, 2018, an early beta of a Navajo course was released on Duolingo.[41]
The Navajo Nation operates Tséhootsooí Diné Bi'ólta', a Navajo language immersion school for grades K-8 in Fort Defiance, Arizona. Located on the Arizona-New Mexico border in the southeastern quarter of the Navajo Reservation, the school strives to revitalize Navajo among children of the Window Rock Unified School District. Tséhootsooí Diné Bi'ólta' has thirteen Navajo language teachers who instruct only in the Navajo language, and no English, while five English language teachers instruct in the English language. Kindergarten and first grade are taught completely in the Navajo language, while English is incorporated into the program during third grade, when it is used for about 10% of instruction.[42]
Navajo has a fairly large consonant inventory. Its stop consonants exist in three laryngeal forms: aspirated, unaspirated, and ejective – for example, /tʃʰ/, /tʃ/, and /tʃʼ/.[43] Ejective consonants are those that are pronounced with a glottalic initiation. Navajo also has a simple glottal stop used after vowels,[44] and every word that would otherwise begin with a vowel is pronounced with an initial glottal stop.[45] Consonant clusters are uncommon, aside from frequent placing /d/ or /t/ before fricatives.[46]
The language has four vowel qualities: /a/, /e/, /i/, and /o/.[46] Each exists in both oral and nasalized forms, and can be either short or long.[47] Navajo also distinguishes for tone between high and low, with the low tone typically regarded as the default. However, some linguists have suggested that Navajo does not possess true tones, but only a pitch accent system similar to that of Japanese.[48] In general, Navajo speech also has a slower speech tempo than English does.[44]
Navajo is difficult to classify in terms of broad morphological typology: it relies heavily on affixes—mainly prefixes—like agglutinative languages,[49] but these affixes are joined in unpredictable, overlapping ways that make them difficult to segment, a trait of fusional languages.[50] In general, Navajo verbs contain more morphemes than do nouns (on average, 11 for verbs compared to 4–5 for nouns), but noun morphology is less transparent.[51] Navajo is sometimes classified as a fusional language[50][52] and sometimes as agglutinative or even polysynthetic.[20][53]
In terms of basic word order, Navajo has been classified as a subject–object–verb language.[54][55] However, some speakers order the subject and object based on ""noun ranking"". In this system, nouns are ranked in three categories—humans, animals, and inanimate objects—and within these categories, nouns are ranked by strength, size, and intelligence. Whichever of the subject and object has a higher rank comes first. As a result, the agent of an action may be syntactically ambiguous.[56] Other linguists such as Eloise Jelinek consider Navajo to be a discourse configurational language, in which word order is not fixed by syntactic rules, but determined by pragmatic factors in the communicative context.[57]
In Navajo, verbs are the main elements of their sentences, imparting a large amount of information. The verb is based on a stem, which is made of a root to identify the action and the semblance of a suffix to convey mode and aspect; however, this suffix is fused beyond separability.[58] The stem is given somewhat more transparent prefixes to indicate, in this order, the following information: postpositional object, postposition, adverb-state, iterativity, number, direct object, deictic information, another adverb-state, mode and aspect, subject, classifier (see later on), mirativity and two-tier evidentiality.  Some of these prefixes may be null; for example, there is only a plural marker (da/daa) and no readily identifiable marker for the other grammatical numbers.[59]
Navajo does not distinguish strict tense per se; instead, an action's position in time is conveyed through mode, aspect, but also time adverbials or context. Each verb has an inherent aspect and can be conjugated in up to seven modes.[60] These forms are as follows:
Modes:

Aspects:

For any verb, the usitative and repetitive modes share the same stem, as do the progressive and future modes; these modes are distinguished with prefixes. However, pairs of modes other than these may also share the same stem,[71] as illustrated in the following example, where the verb ""to play"" is conjugated into each of the five mode paradigms:
The basic set of subject prefixes for the imperfective mode, as well as the actual conjugation of the verb into these person and number categories, are as follows.[72]


The remaining piece of these conjugated verbs—the prefix na-—is called an ""outer"" or ""disjunct"" prefix. It is the marker of the Continuative aspect (to play about).[73]
Navajo distinguishes between the first, second, third, and fourth persons in the singular, dual, and plural numbers.[74] The fourth person is similar to the third person, but is generally used for indefinite, theoretical actors rather than defined ones.[75] Despite the potential for extreme verb complexity, only the mode/aspect, subject, classifier, and stem are absolutely necessary.[59] Furthermore, Navajo negates clauses by surrounding the verb with the circumclitic doo= ... =da (e.g. mósí doo nitsaa da 'the cat is not big'). Dooda, as a single word, corresponds to English no.[76]
Classificatory verbs are a set of verbal roots distinguishing eleven shapes and three classes of motion for each shape.[77][78] The motion classes are:
The shapes are listed here with their standard names and their corresponding handle root.[77]



For example, Navajo has no single verb that corresponds to the English 'give'. To say 'give me some hay', the Navajo verb níłjool (Non-Compact Matter) must be used, while for 'give me a cigarette' the verb nítįįh (Slender Stiff Object) must be used.
Navajo also contains a separate system of classifiers that generally marks for voice. There are four classifiers: Ø-, ł-, d-, and l-, placed between the personal prefixes and the verbal stem. The ł- classifier indicates causation (transitivity increase), e.g. yibéézh (yi-Ø-béézh) 'it's boiling'  vs. yiłbéézh (yi-ł-béézh) 'he's boiling it'. The d- and l- classifiers indicate passive voice (transitivity reduction), e.g. yizéés (yi-Ø-zéés) 'he's singing it' vs. yidéés (yi-d-zéés) 'it's being sung' . The d- classifier is used to detransitivize verbs with Ø-, while l- is used for verbs with ł-.[79]
Nouns are not required to form a complete Navajo sentence. Besides the extensive information that can be communicated with a verb, Navajo speakers may alternate between the third and fourth person to distinguish between two already specified actors, similarly to how speakers of languages with grammatical gender may repeatedly use pronouns.[80]
Most nouns are not inflected for number,[76] and plurality is usually encoded directly in the verb through the use of various prefixes or aspects, though this is by no means mandatory. In the following example, the verb on the right is used with the plural prefix da-  and switches to the distributive aspect.
Kinkinhouseáshłééh.á-Ø-sh-łééhmake-3.OBJ-1.SUBJ-make.MOM.IMPFKin áshłééh.kin á-Ø-sh-łééhhouse make-3.OBJ-1.SUBJ-make.MOM.IMPF'I build a house.'
Kinkinhouseádaashleʼ.á-da-Ø-sh-łeʼmake-PL-3.OBJ-1.SUBJ-make.DIST.IMPFKin ádaashleʼ.kin á-da-Ø-sh-łeʼhouse make-PL-3.OBJ-1.SUBJ-make.DIST.IMPF'I build houses.'
Some verbal roots encode number in their lexical definition (see classificatory verbs above). When available, the use of the correct verbal root is mandatory:
Béégashiicowsitį́.3.SUBJ-lie(1).PERFBéégashii sitį́.cow 3.SUBJ-lie(1).PERF'The (one) cow lies.'
Béégashiicowshitéézh.3.SUBJ-lie(2).PERFBéégashii shitéézh.cow 3.SUBJ-lie(2).PERF'The (two) cows lie.'
Béégashiicowshijééʼ.3.SUBJ-lie(3+).PERFBéégashii shijééʼ.cow 3.SUBJ-lie(3+).PERF'The (three or more) cows lie.'
Bilasáanabilasáanaappleshaash-aa1-toniʼaah.Ø-ni-ʼaah3.OBJ-2.SUBJ-give(SRO).MOM.PERFBilasáana shaa niʼaah.bilasáana sh-aa Ø-ni-ʼaahapple 1-to 3.OBJ-2.SUBJ-give(SRO).MOM.PERF'You give me an apple.'
Bilasáanabilasáanaappleshaash-aa1-toninííł.Ø-ni-nííł3.OBJ-2.SUBJ-give(PlO1).MOM.PERFBilasáana shaa ninííł.bilasáana sh-aa Ø-ni-nííłapple 1-to 3.OBJ-2.SUBJ-give(PlO1).MOM.PERF'You give me apples.'
Number marking on nouns occurs only for terms of kinship and age-sex groupings. Other prefixes that can be added to nouns include possessive markers (e.g. chidí 'car' - shichidí 'my car') and a few adjectival enclitics. Generally, an upper limit for prefixes on a noun is about four or five.[81]
Nouns are also not marked for case, this traditionally being covered by word order.[82]
Atʼéédgirlashkiiboyyiyiiłtsą́.3.OBJ-3.SUBJ-sawAtʼééd ashkii yiyiiłtsą́.girl boy 3.OBJ-3.SUBJ-saw'The girl saw the boy.'
Ashkiiboyatʼéédgirlyiyiiłtsą́.3.OBJ-3.SUBJ-sawAshkii atʼééd yiyiiłtsą́.boy girl 3.OBJ-3.SUBJ-saw'The boy saw the girl.'
Other parts of speech in Navajo are also relatively immutable, and tend to be short. These parts of speech include question particles, demonstrative adjectives, relative pronouns, interjections, conjunctions,[83] and adverbs (both unique ones and those based on verbs).[84] The Navajo numeral system is decimal, and some example numbers follow.[85]
1 – tʼááłáʼí
2 – naaki
3 – tááʼ
4 – dį́į́ʼ
5 – ashdlaʼ
6 – hastą́ą́
7 – tsostsʼid
8 – tseebíí
9 – náhástʼéí
10 – neeznáá
11 – łaʼtsʼáadah
12 – naakitsʼáadah
13 – tááʼtsʼáadah
14 – dį́į́ʼtsʼáadah
15 – ashdlaʼáadah
16 – hastą́ʼáadah
17 – tsostsʼidtsáadah
20 – naadiin
300 – táadi neeznádiin
4,567 – dį́į́di mííl dóó baʼaan ashdladi neeznádiin dóó baʼaan hastą́diin dóó baʼaan tsostsʼid
Navajo does not contain a single part of speech analogous to adjectives; rather, some verbs describe static qualitative attributes (e.g. nitsaa 'he/she/it is large'), and demonstrative adjectives (e.g. díí 'this/these') are their own part of speech. However, these verbs, known as ""neuter verbs"", are distinguished by only having the imperfective mode, as they describe continuous states of being.[86]
The vast majority of Navajo vocabulary is of Athabaskan origin.[87] However, the vocabulary size is still fairly small; one estimate counted 6,245 noun bases and 9,000 verb bases, with most of these nouns being derived from verbs.[81] Prior to the European colonization of the Americas, Navajo did not borrow much from other languages, including from other Athabaskan and even Apachean languages. The Athabaskan family is fairly diverse in both phonology and morphology due to its languages' prolonged relative isolation.[87] Even the Pueblo peoples, with whom the Navajo interacted with for centuries and borrowed cultural customs, have lent few words to the Navajo language. After Spain and Mexico took over Navajo lands, the language did not incorporate many Spanish words, either.[88]
This resistance to word absorption extended to English, at least until the mid-twentieth century. Around this point, the Navajo language began importing some, though still not many, English words, mainly by young schoolchildren exposed to English.[25]
Navajo has expanded its vocabulary to include Western technological and cultural terms through calques and Navajo descriptive terms. For example, the phrase for English tank is chidí naa'naʼí beeʼeldǫǫhtsoh bikááʼ dah naaznilígíí 'vehicle that crawls around, by means of which big explosions are made, and that one sits on at an elevation'. This language purism also extends to proper nouns, such as the names of U.S. states (e.g. Hoozdo 'Arizona' and Yootó 'New Mexico'; see also hahoodzo 'state') and languages (naakaii 'Spanish').
Only one Navajo word has been fully absorbed into the English language: hogan (from Navajo hooghan) – a term referring to the traditional houses.[89] Another word with limited English recognition is chindi (an evil spirit of the deceased).[90] The taxonomic genus name Uta may be of Navajo origin.[91] It has been speculated that English-speaking settlers were reluctant to take on more Navajo loanwords compared to many other Native American languages, including the Hopi language, because the Navajo were among the most violent resisters to colonialism.[92]
Early attempts at a Navajo orthography were made in the late nineteenth and early twentieth centuries. One such attempt was based on the Latin alphabet, particularly the English variety, with some additional letters and diacritics.  Anthropologists were frustrated by Navajo's having several sounds that are not found in English and lack of other sounds that are.[93] Finally, the current Navajo orthography was developed between 1935 and 1940[21] by Young and Morgan.
An apostrophe (ʼ) is used to mark ejective consonants (e.g. chʼ, tłʼ)[94] as well as mid-word or final glottal stops. However, initial glottal stops are usually not marked.[45] The voiceless glottal fricative (/h/) is normally written as h, but appears as x after the consonants s, z, and digraphs ending in h to avoid phonological ambiguity.[94][95] The voiced velar fricative is written as y before i and e (where it is palatalized /ʝ/), as w before o (where it is labialized /ɣʷ/), and as gh before a.[96]
Navajo represents nasalized vowels with an ogonek ( ˛ ), sometimes described as a reverse cedilla; and represents the voiceless alveolar lateral fricative (/ɬ/) with a barred L (capital Ł, lowercase ł).[97] The ogonek is placed centrally under a vowel, but it was imported from Polish and Lithuanian, which do not use it under certain vowels such as o or any vowels with accent marks.  For example, in proper Navajo writing, the ogonek below lowercase a is written centered below the letter,[citation needed] whereas fonts for a with ogonek intended for Polish and Lithuanian such as those used in common Web browsers render the ogonek connected to the bottom right of the letter. As of 2017,[update] no Unicode font has been developed to properly accommodate Navajo typography. Google is working to correct this oversight with Noto fonts.
The first Navajo-capable typewriter was developed in preparation for a Navajo newspaper and dictionary created in the 1940s. The advent of early computers in the 1960s necessitated special fonts to input Navajo text, and the first Navajo font was created in the 1970s.[97] Navajo virtual keyboards were made available for iOS devices in November 2012 and Android devices in August 2013.[98]
This is the first paragraph of a Navajo short story.[99]
Navajo original: Ashiiké tʼóó diigis léiʼ tółikaní łaʼ ádiilnííł dóó nihaa nahidoonih níigo yee hodeezʼą́ jiní. Áko tʼáá ałʼąą chʼil naʼatłʼoʼii kʼiidiilá dóó hááhgóóshį́į́ yinaalnishgo tʼáá áłah chʼil naʼatłʼoʼii néineestʼą́ jiní. Áádóó tółikaní áyiilaago tʼáá bíhígíí tʼáá ałʼąą tłʼízíkágí yiiʼ haidééłbįįd jiní. ""Háadida díí tółikaní yígíí doo łaʼ ahaʼdiidził da,"" níigo ahaʼdeetʼą́ jiníʼ. Áádóó baa nahidoonih biniiyé kintahgóó dah yidiiłjid jiníʼ ...
English translation: Some crazy boys decided to make some wine to sell, so they each planted grapevines and, working hard on them, they raised them to maturity. Then, having made wine, they each filled a goatskin with it. They agreed that at no time would they give each other a drink of it, and they then set out for town lugging the goatskins on their backs ...
"
Barbareño language - Wikipedia," Barbareño is one of the extinct Chumashan languages, a group of Native American languages, which was spoken in the area of Santa Barbara, California. The closely related Ineseño may have been a dialect of the same language. Barbareño became extinct in 1965 with the death of Mary Yee.[1]
As of 2013, the Barbareno Chumash Council is engaged in ongoing efforts to revive the language. Two of its members are language apprentices and teachers.[4][5]
Wishtoyo Chumash Village, in Malibu, California, announced the opening of its Šmuwič Language School in 2010.[6][7]
The Ineseño community now call their language Samala.  In 2008 Richard Applegate compiled a grammar and dictionary of Ineseño based on Harrington's work in the early 1900s with one of the last fluent speakers, Maria Solares.[8] Applegate and Nakia Zavalla, Cultural Director for the Santa Ynez Band of Chumash and a descendant of Solares, have begun an effort to revitalize the language. Applegate began teaching Ineseño in 2003, and Zavalla has started an immersion-based language apprentice program.[9] As of 2008, Applegate had five students, though none had reached fluency.[10]
"
Category:Wikipedia articles incorporating text from the National Park Service - Wikipedia," The following 200 pages are in this category, out of  200 total. This list may not reflect recent changes (learn more).
"
Grand Canyon - Wikipedia," 
The Grand Canyon (Hopi: Ongtupqa,[2] Yavapai: Wi:kaʼi:la,  Navajo: Bidááʼ Haʼaztʼiʼ Tsékooh,[3][4] Spanish: Gran Cañón) is a steep-sided canyon carved by the Colorado River in Arizona, United States. The Grand Canyon is 277 miles (446 km) long, up to 18 miles (29 km) wide and attains a depth of over a mile (6,093 feet or 1,857 meters).[5]
The canyon and adjacent rim are contained within Grand Canyon National Park, the Kaibab National Forest, Grand Canyon–Parashant National Monument, the Hualapai Indian Reservation, the Havasupai Indian Reservation and the Navajo Nation. President Theodore Roosevelt was a major proponent of preservation of the Grand Canyon area and visited it on numerous occasions to hunt and enjoy the scenery.
Nearly two billion years of Earth's geological history have been exposed as the Colorado River and its tributaries cut their channels through layer after layer of rock while the Colorado Plateau was uplifted.[6] While some aspects about the history of incision of the canyon are debated by geologists,[7] several recent studies support the hypothesis that the Colorado River established its course through the area about 5 to 6 million years ago.[1][8][9] Since that time, the Colorado River has driven the down-cutting of the tributaries and retreat of the cliffs, simultaneously deepening and widening the canyon.
For thousands of years, the area has been continuously inhabited by Native Americans, who built settlements within the canyon and its many caves. The Pueblo people considered the Grand Canyon a holy site, and made pilgrimages to it.[10] The first European known to have viewed the Grand Canyon was García López de Cárdenas from Spain, who arrived in 1540.[11]
The Grand Canyon is a river valley in the Colorado Plateau that exposes uplifted Proterozoic and Paleozoic strata, and is also one of the six distinct physiographic sections of the Colorado Plateau province. Even though It is not the deepest canyon in the world (Kali Gandaki Gorge in Nepal is much deeper), the Grand Canyon is known for its visually overwhelming size and its intricate and colorful landscape. Geologically, it is significant because of the thick sequence of ancient rocks that are well preserved and exposed in the walls of the canyon. These rock layers record much of the early geologic history of the North American continent.
Uplift associated with mountain formation later moved these sediments thousands of feet upward and created the Colorado Plateau. The higher elevation has also resulted in greater precipitation in the Colorado River drainage area, but not enough to change the Grand Canyon area from being semi-arid.[12] The uplift of the Colorado Plateau is uneven, and the Kaibab Plateau that the Grand Canyon bisects is over one thousand feet (300 m) higher at the North Rim than at the South Rim. Almost all runoff from the North Rim (which also gets more rain and snow) flows toward the Grand Canyon, while much of the runoff on the plateau behind the South Rim flows away from the canyon (following the general tilt). The result is deeper and longer tributary washes and canyons on the north side and shorter and steeper side canyons on the south side.
Temperatures on the North Rim are generally lower than those on the South Rim because of the greater elevation (averaging 8,000 feet or 2,400 metres above sea level).[13] Heavy rains are common on both rims during the summer months. Access to the North Rim via the primary route leading to the canyon (State Route 67) is limited during the winter season due to road closures.[14]
The Grand Canyon is part of the Colorado River basin which has developed over the past 70 million years,[15]  in part based on apatite (U-Th)/He thermochronometry showing that Grand Canyon reached a depth near to the modern depth by 20 Ma.[16]  A recent study examining caves near Grand Canyon places their origins beginning about 17 million years ago. Previous estimates had placed the age of the canyon at 5–6 million years.[17]  The study, which was published in the journal Science in 2008, used uranium-lead dating to analyze calcite deposits found on the walls of nine caves throughout the canyon.[18] There is a substantial amount of controversy because this research suggests such a substantial departure from prior widely supported scientific consensus.[19] In December 2012, a study published in the journal Science claimed new tests had suggested the Grand Canyon could be as old as 70 million years[20].[21] However, this study has been criticized by those who support the ""young canyon"" age of around six million years as ""[an] attempt to push the interpretation of their new data to their limits without consideration of the whole range of other geologic data sets.""[17]
The canyon is the result of erosion which exposes one of the most complete geologic columns on the planet.
The major geologic exposures in the Grand Canyon range in age from the 2-billion-year-old Vishnu Schist at the bottom of the Inner Gorge to the 270-million-year-old Kaibab Limestone on the Rim. There is a gap of about a billion years between the 500-million-year-old stratum and the level below it, which dates to about 1.5 billion years ago. This large unconformity indicates a long period for which no deposits are present.
Many of the formations were deposited in warm shallow seas, near-shore environments (such as beaches), and swamps as the seashore repeatedly advanced and retreated over the edge of a proto-North America. Major exceptions include the Permian Coconino Sandstone, which contains abundant geological evidence of aeolian sand dune deposition. Several parts of the Supai Group also were deposited in non–marine environments.
The great depth of the Grand Canyon and especially the height of its strata (most of which formed below sea level) can be attributed to 5–10 thousand feet (1,500 to 3,000 m) of uplift of the Colorado Plateau, starting about 65 million years ago (during the Laramide Orogeny). This uplift has steepened the stream gradient of the Colorado River and its tributaries, which in turn has increased their speed and thus their ability to cut through rock (see the elevation summary of the Colorado River for present conditions).
Weather conditions during the ice ages also increased the amount of water in the Colorado River drainage system. The ancestral Colorado River responded by cutting its channel faster and deeper.
The base level and course of the Colorado River (or its ancestral equivalent) changed 5.3 million years ago when the Gulf of California opened and lowered the river's base level (its lowest point). This increased the rate of erosion and cut nearly all of the Grand Canyon's current depth by 1.2 million years ago. The terraced walls of the canyon were created by differential erosion.[22]
Between 100,000 and 3 million years ago, volcanic activity deposited ash and lava over the area which at times completely obstructed the river. These volcanic rocks are the youngest in the canyon.
The Ancestral Puebloans were a Native American culture centered on the present-day Four Corners area of the United States. They were the first people known to live in the Grand Canyon area. The cultural group has often been referred to in archaeology as the Anasazi, although the term is not preferred by the modern Puebloan peoples.[23] The word ""Anasazi"" is Navajo for ""Ancient Ones"" or ""Ancient Enemy"".[24][unreliable source?]
Archaeologists still debate when this distinct culture emerged. The current consensus, based on terminology defined by the Pecos Classification, suggests their emergence was around 1200 BCE during the Basketmaker II Era. Beginning with the earliest explorations and excavations, researchers have believed that the Ancestral Puebloans are ancestors of the modern Pueblo peoples.[24]
In addition to the Ancestral Puebloans, a number of distinct cultures have inhabited the Grand Canyon area. The Cohonina lived to the west of the Grand Canyon, between 500 and 1200 CE.[25][26] The Cohonina were ancestors of the Yuman, Havasupai, and Hualapai peoples who inhabit the area today.[27]
The Sinagua were a cultural group occupying an area to the southeast of the Grand Canyon, between the Little Colorado River and the Salt River, between approximately 500 and 1425 CE. The Sinagua may have been ancestors of several Hopi clans.
By the time of the arrival of Europeans in the 16th century, newer cultures had evolved. The Hualapai inhabit a 100-mile (160 km) stretch along the pine-clad southern side of the Grand Canyon. The Havasupai have been living in the area near Cataract Canyon since the beginning of the 13th century, occupying an area the size of Delaware.[28] The Southern Paiutes live in what is now southern Utah and northern Arizona. The Navajo, or Diné, live in a wide area stretching from the San Francisco Peaks eastwards towards the Four Corners. Archaeological and linguistic evidence suggests the Navajo descended from the Athabaskan people near Great Slave Lake, Canada, who migrated after the 11th century.[29] In the mythology of some Third Mesa Hopi communities, the Grand Canyon was the location humankind arose out of the Third World from a sipapu.[30]
In September 1540, under orders from the conquistador Francisco Vázquez de Coronado to search for the fabled Seven Cities of Cibola, Captain García López de Cárdenas, along with Hopi guides and a small group of Spanish soldiers, traveled to the south rim of the Grand Canyon between Desert View and Moran Point. Pablo de Melgrossa, Juan Galeras, and a third soldier descended some one third of the way into the canyon until they were forced to return because of lack of water. In their report, they noted that some of the rocks in the canyon were ""bigger than the great tower of Seville, Giralda ""[31] It is speculated that their Hopi guides likely knew routes to the canyon floor, but may have been reluctant to lead the Spanish to the river. No Europeans visited the canyon again for more than two hundred years.
Fathers Francisco Atanasio Domínguez and Silvestre Vélez de Escalante were two Spanish priests who, with a group of Spanish soldiers, explored southern Utah and traveled along the north rim of the canyon in Glen and Marble Canyons in search of a route from Santa Fe to California in 1776. They eventually found a crossing, formerly known as the ""Crossing of the Fathers,"" that today lies under Lake Powell.
Also in 1776, Fray Francisco Garces, a Franciscan missionary, spent a week near Havasupai, unsuccessfully attempting to convert a band of Native Americans to Christianity. He described the canyon as ""profound"".[31]
James Ohio Pattie, along with a group of American trappers and mountain men, may have been the next European to reach the canyon, in 1826.[32]
Jacob Hamblin, a Mormon missionary, was sent by Brigham Young in the 1850s to locate suitable river crossing sites in the canyon. Building good relations with local Hualapai and white settlers, he found the Crossing of the Fathers, and the locations that would become Lees Ferry in 1858 and Pearce Ferry (later operated by, and named for, Harrison Pearce) – only the latter two sites suitable for ferry operation.[citation needed] He also acted as an advisor to John Wesley Powell, before his second expedition to the Grand Canyon, serving as a diplomat between Powell and the local native tribes to ensure the safety of his party.
In 1857, Edward Fitzgerald Beale was superintendent of an expedition to survey a wagon road along the 35th parallel from Fort Defiance, Arizona to the Colorado River. He led a small party of men in search of water on the Coconino Plateau near the canyon's south rim. On September 19, near present-day National Canyon, they came upon what May Humphreys Stacey described in his journal as ""...a wonderful canyon four thousand feet deep. Everyone (in the party) admitted that he never before saw anything to match or equal this astonishing natural curiosity.""
Also in 1857, the U.S. War Department asked Lieutenant Joseph Ives to lead an expedition to assess the feasibility of an up-river navigation from the Gulf of California. Also in a stern wheeler steamboat Explorer, after two months and 350 miles (560 km) of difficult navigation, his party reached Black Canyon some two months after George Johnson.[citation needed] The Explorer struck a rock and was abandoned. Ives led his party east into the canyon — they may have been the first Europeans to travel the Diamond Creek drainage and traveled eastwards along the south rim. In his ""Colorado River of the West"" report to the Senate in 1861 he states that ""One or two trappers profess to have seen the canyon.""
According to the San Francisco Herald, in a series of articles run in 1853, Captain Joseph R. Walker in January 1851 with his nephew James T. Walker and six men, traveled up the Colorado River to a point where it joined the Virgin River and continued east into Arizona, traveling along the Grand Canyon and making short exploratory side trips along the way. Walker is reported to have said he wanted to visit the ""Moqui"" Indians, as the Hopi were then called by Europeans. He had met these people briefly in previous years, thought them exceptionally interesting and wanted to become better acquainted. The Herald reporter then stated, ""We believe that Captain Joe Walker is the only white man in this country that has ever visited this strange people.""[citation needed]
In 1858, John Strong Newberry became probably the first geologist to visit the Grand Canyon.[33]
In 1869, Major John Wesley Powell led the first expedition down the canyon. Powell set out to explore the Colorado River and the Grand Canyon. Powell ordered a shipwright to build four reinforced Whitewall rowboats from Chicago and had them shipped east on the newly completed Continental railroad. He hired nine men, including his brother Walter, and collected provisions for ten months. They set out from Green River, Wyoming on May 24. Passing through (or portaging around) a series of dangerous rapids, the group passed down the Green River to its confluence with the Colorado River, near present-day Moab, Utah. Most of their food spoiled after getting wet in the waves or by heavy rains. Beaten up by ferocious whitewater and nearly out of food, three men left the expedition in the Grand Canyon, electing to walk 75 miles (121 km) out across a desert to a Mormon settlement. They were never seen again, and their disappearance remains one of the most enduring mysteries of American western history. The remaining members completed the journey through the Grand Canyon on August 13, 1869.[34][35] In 1871 Powell first used the term ""Grand Canyon""; previously it had been called the ""Big Canyon"".[36]
In 1889, Frank M. Brown wanted to build a railroad along the Colorado River to carry coal. He, his chief engineer Robert Brewster Stanton, and 14 others started to explore the Grand Canyon in poorly designed cedar wood boats, with no life preservers. Brown drowned in an accident near Marble Canyon: Stanton made new boats and proceeded to explore the Colorado all of the way to the Gulf of California.[37]
The Grand Canyon[38] became an official national monument in 1908 and a national park in 1919.
U.S. President Theodore Roosevelt visited the Grand Canyon in 1903.  An avid outdoorsman and staunch conservationist, Roosevelt established the Grand Canyon Game Preserve on November 28, 1906. Livestock grazing was reduced, but predators such as mountain lions, eagles, and wolves were eradicated. Roosevelt along with other members of his conservation group, the Boone and Crockett Club helped form the National Parks Association, which in turn lobbied for the Antiquities Act of 1906 which gave Roosevelt the power to create national monuments. Once the act was passed, Roosevelt immediately added adjacent national forest lands and redesignated the preserve a U.S. National Monument on January 11, 1908.[39] Opponents such as land and mining claim holders blocked efforts to reclassify the monument as a U.S. National Park for 11 years. Grand Canyon National Park was finally established as the 17th U.S. National Park by an Act of Congress signed into law by President Woodrow Wilson on February 26, 1919.[25]
The federal government administrators who manage park resources face many challenges. These include issues related to the recent reintroduction into the wild of the highly endangered California condor, air tour overflight noise levels, water rights disputes with various tribal reservations that border the park, and forest fire management. Federal officials started floods in the Grand Canyon in hopes of restoring its ecosystem in 1996, 2004 and 2008. The canyon's ecosystem was permanently changed after the construction of the Glen Canyon Dam in 1963.[40]
Between 2003 and 2011, 2,215 mining claims had been requested that are adjacent to the canyon, including claims for uranium mines. Mining has been suspended since 2009, when U.S. Interior Secretary Ken Salazar withdrew 1 million acres (4,000 km2) from the permitting process, pending assessment of the environmental impact of mining. Critics of the mines are concerned that, once mined, the uranium will leach into the water of the Colorado River and contaminate the water supply for up to 18 million people.[41] Salazar's so-called ""Northern Arizona Withdrawal"" is a 20-year moratorium on new mines, but allows existing mines to continue. In 2012, the federal government stopped new mines in the area, which was upheld by the U.S. District Court for Arizona in 2014, but appealed by the National Mining Association, joined by the state of Arizona under Attorney General Mark Brnovich as well as Utah, Montana and Nevada. National Mining Association v. Jewell is pending before the Ninth Circuit Court of Appeals as of September 2015.[42]
There are several historic buildings located along the South Rim with most in the vicinity of Grand Canyon Village.
Weather in the Grand Canyon varies according to elevation. The forested rims are high enough to receive winter snowfall, but along the Colorado River in the Inner Gorge, temperatures are similar to those found in Tucson and other low elevation desert locations in Arizona. Conditions in the Grand Canyon region are generally dry, but substantial precipitation occurs twice annually, during seasonal pattern shifts in winter (when Pacific storms usually deliver widespread, moderate rain and high-elevation snow to the region from the west) and in late summer (due to the North American Monsoon, which delivers waves of moisture from the southeast, causing dramatic, localized thunderstorms fueled by the heat of the day).[45] Average annual precipitation on the South Rim is less than 16 inches (41 cm), with 60 inches (150 cm) of snow; the higher North Rim usually receives 27 inches (69 cm) of moisture, with a typical snowfall of 144 inches (370 cm); and Phantom Ranch, far below the canyon's rims along the Colorado River at 2,500 feet (762 m) gets just 8 inches (20 cm) of rain, and snow is a rarity.
Temperatures vary wildly throughout the year, with summer highs within the Inner Gorge commonly exceeding 100 °F (37.8 °C) and winter minimum temperatures sometimes falling below zero degrees Fahrenheit (−17.8 °C) along the canyon's rims.[45] Visitors are often surprised by these potentially extreme conditions, and this, along with the high altitude of the canyon's rims, can lead to unpleasant side effects such as dehydration, sunburn, and hypothermia.
Weather conditions can greatly affect hiking and canyon exploration, and visitors should obtain accurate forecasts because of hazards posed by exposure to extreme temperatures, winter storms and late summer monsoons. While the park service posts weather information at gates and visitor centers, this is a rough approximation only, and should not be relied upon for trip planning. For accurate weather in the canyon, hikers should consult the National Weather Service's NOAA weather radio or the official National Weather Service website.[46]
The National Weather Service has had a cooperative station on the South Rim since 1903. The record high temperature on the South Rim was 105 °F (41 °C) on June 26, 1974, and the record low temperature was −20 °F (−29 °C) on January 1, 1919, February 1, 1985, and December 23, 1990.[47][48][49]
The Grand Canyon area has some of the cleanest air in the United States.[51]:p.5–2
[52]
However, at times the air quality can be considerably affected by events such as forest fires and dust storms in the Southwest.
What effect there is on air quality and visibility in the canyon has been mainly from sulfates, soils, and organics. The sulfates largely result from urban emissions in southern California, borne on the prevailing westerly winds throughout much of the year, and emissions from Arizona's copper smelter region, borne on southerly or southeasterly winds during the monsoon. Airborne soils originate with windy conditions and road dust. Organic particles result from vehicle emissions, long-range transport from urban areas, and forest fires, as well as from VOCs emitted by vegetation in the surrounding forests. Nitrates, carried in from urban areas, stationary sources, and vehicle emissions; as well as black carbon from forest fires and vehicle emissions, also contribute to a lesser extent.
[52]
[53]:p.26, 49–51
A number of actions have been taken to preserve and further improve air quality and visibility at the canyon.
In 1990, amendments to the Clean Air Act established the Grand Canyon Visibility Transport Commission (GCVTC) to advise the US EPA on strategies for protecting visual air quality on the Colorado Plateau. The GCVTC released its final report in 1996 and initiated the Western Regional Air Partnership (WRAP), a partnership of state, tribal and federal agencies to help coordinate implementation of the Commission's recommendations.
[54]
[55]
In 1999, the Regional Haze Rule established a goal of restoring visibility in national parks and wilderness areas (Class 1 areas), such as the Grand Canyon, to natural background levels by 2064. Subsequent revisions to the rule provide specific requirements for making reasonable progress toward that goal.
[56]
In the early 1990s, studies indicated that emissions of SO2, a sulfate precursor, from the Navajo Generating Station affected visibility in the canyon mainly in the winter, and which if controlled would improve wintertime visibility by 2 to 7%.
[57]:p.C-2,C-6
As a result, scrubbers were added to the plant's three units in 1997 through 1999, reducing SO2 emissions by more than 90%. The plant also installed low-NOx SOFA burners in 2009–2011, reducing emissions of NOx, a nitrate precursor, by 40%.
The plant shut down completely in 2019. Emissions from the Mohave Generating Station to the west were similarly found to affect visibility in the canyon. The plant was required to have installed SO2 scrubbers, but was instead shut down in 2005, completely eliminating its emissions.
[58]
Prescribed fires are typically conducted in the spring and fall in the forests adjacent to the canyon to reduce the potential for severe forest fires and resulting smoke conditions. Although prescribed fires also affect air quality, the controlled conditions allow the use of management techniques to minimize their impact.
[59]
[60]:p.86,93
There are approximately 1,737 known species of vascular plants, 167 species of fungi, 64 species of moss and 195 species of lichen found in Grand Canyon National Park.[61] This variety is largely due to the 8,000 foot (2,400 m) elevation change from the Colorado River up to the highest point on the North Rim.[61] Grand Canyon boasts a dozen endemic plants (known only within the Park's boundaries) while only ten percent of the Park's flora is exotic.[61] Sixty-three plants found here have been given special status by the U.S. Fish and Wildlife Service.[61]
The Mojave Desert influences the western sections of the canyon, Sonoran Desert vegetation covers the eastern sections, and ponderosa and pinyon pine forests grow on both rims.[62]
Natural seeps and springs percolating out of the canyon walls are home to 11% of all the plant species found in the Grand Canyon.[62] The canyon itself can act as a connection between the east and the west by providing corridors of appropriate habitat along its length.[62] The canyon can also be a genetic barrier to some species, like the tassel-eared squirrel.[62]
The aspect, or direction a slope faces, also plays a major role in adding diversity to the Grand Canyon. North-facing slopes receive about one-third the normal amount of sunlight, so plants growing there are similar to plants found at higher elevations, or in more northern latitudes.[62] The south-facing slopes receive the full amount of sunlight and are covered in vegetation typical of the Sonoran Desert.[62]
Of the 90 mammal species found along the Colorado River corridor, 18 are rodents and 22 are bats.[63]
The Park contains several major ecosystems.[13] Its great biological diversity can be attributed to the presence of five of the seven life zones and three of the four desert types in North America.[13] The five life zones represented are the Lower Sonoran, Upper Sonoran, Transition, Canadian, and Hudsonian.[13] This is equivalent to traveling from Mexico to Canada. Differences in elevation and the resulting variations in climate are the major factors that form the various life zones and communities in and around the canyon. Grand Canyon National Park contains 129 vegetation communities, and the composition and distribution of plant species is influenced by climate, geomorphology and geology.[61]
The Lower Sonoran life zone spans from the Colorado River up to 3,500 feet (1,100 m). Along the Colorado River and its perennial tributaries, a riparian community exists.[61] Coyote willow, arrowweed, seep-willow, western honey mesquite, catclaw acacia, and exotic tamarisk (saltcedar) are the predominant species.[61] Hanging gardens, seeps and springs often contain rare plants such as the white-flowering western redbud, stream orchid, and Flaveria mcdougallii.[61] Endangered fish in the river include the humpback chub and the razorback sucker.[64]
The three most common amphibians in these riparian communities are the canyon tree frog, red-spotted toad, and Woodhouse's Rocky Mountain toad.[65] Leopard frogs are very rare in the Colorado River corridor, they have undergone major declines and have not been seen in the Canyon in several years.[65] There are 33 crustacean species found in the Colorado River and its tributaries within Grand Canyon National Park. Of these 33, 16 are considered true zooplankton organisms.[66]
Only 48 bird species regularly nest along the river, while others use the river as a migration corridor or as overwintering habitat. The bald eagle is one species that uses the river corridor as winter habitat.[67]
River otters may have disappeared from the park in the late 20th century, and muskrats are extremely rare.[63] Beavers cut willows, cottonwoods, and shrubs for food, and can significantly affect the riparian vegetation.[63] Other rodents, such as antelope squirrels and pocket mice, are mostly omnivorous, using many different vegetation types.[63] Grand Canyon bats typically roost in desert uplands, but forage on the abundance of insects along the river and its tributaries.[63] In addition to bats, coyotes, ringtails, and spotted skunks are the most numerous riparian predators and prey on invertebrates, rodents, and reptiles.[63]
Raccoons, weasels, bobcats, gray foxes, and mountain lions are also present, but are much more rare.[63] Mule deer and desert bighorn sheep are the ungulates that frequent the river corridor. Since the removal of 500 feral burros in the early 1980s, bighorn sheep numbers have rebounded.[63] Mule deer are generally not permanent residents along the river, but travel down from the rim when food and water resources there become scarce.[63]
The insect species commonly found in the river corridor and tributaries are midges, caddis flies, mayflies, stoneflies, black flies, mites, beetles, butterflies, moths, and fire ants.[68] Numerous species of spiders and several species of scorpions including the bark scorpion and the giant desert hairy scorpion inhabit the riparian zone.[68]
Eleven aquatic and 26 terrestrial species of mollusks have been identified in and around Grand Canyon National Park.[69] Of the aquatic species, two are bivalves (clams) and nine are gastropods (snails).[69] Twenty-six species of terrestrial gastropods have been identified, primarily land snails and slugs.[69]
There are approximately 41 reptile species in Grand Canyon National Park. Ten are considered common along the river corridor and include lizards and snakes.[70] Lizard density tends to be highest along the stretch of land between the water's edge and the beginning of the upland desert community.[70] The two largest lizards in the canyon are gila monsters and chuckwallas.[70] Many snake species, which are not directly dependent on surface water, may be found both within the inner gorge and the Colorado River corridor. Six rattlesnake species have been recorded in the park.[70]
Above the river corridor a desert scrub community, composed of North American desert flora, thrives. Typical warm desert species such as creosote bush, white bursage, brittlebush, catclaw acacia, ocotillo, mariola, western honey mesquite, four-wing saltbush, big sagebrush, blackbrush and rubber rabbitbrush grow in this community.[61] The mammalian fauna in the woodland scrub community consists of 50 species, mostly rodents and bats.[63] Three of the five Park woodrat species live in the desert scrub community.[63]
Except for the western (desert) banded gecko, which seems to be distributed only near water along the Colorado River, all of the reptiles found near the river also appear in the uplands, but in lower densities.[70] The desert gopher tortoise, a threatened species, inhabits the desert scrublands in the western end of the park.[70]
Some of the common insects and animals found at elevations above 2,000 feet (610 m) are orange paper wasps, honey bees, black flies, tarantula hawks, stink bugs, beetles, black ants, and monarch and swallowtail butterflies.[68] Solifugids, wood spiders, garden spiders, black widow spiders, peacocks, and tarantulas can be found in the desert scrub and higher elevations.[68]
The Upper Sonoran Life Zone includes most of the inner canyon and South Rim at elevations from 3,500 to 7,000 feet (1,100 to 2,100 m).[62] This zone is generally dominated by blackbrush, sagebrush, and pinyon-juniper woodlands. Elevations of 3,500 to 4,000 feet (1,100 to 1,200 m) are in the Mojave Desert Scrub community of the Upper Sonoran. This community is dominated by the four-winged saltbush and creosote bush; other important plants include Utah agave, narrowleaf mesquite, ratany, catclaw acacia, and various cacti species.[62]
Approximately 30 bird species breed primarily in the desert uplands and cliffs of the inner canyon.[67] Virtually all bird species present breed in other suitable habitats throughout the Sonoran and Mohave deserts.[67] The abundance of bats, swifts, and riparian birds provides ample food for peregrines, and suitable eyrie sites are plentiful along the steep canyon walls. Also, several critically endangered California condors that were re-introduced to the Colorado Plateau on the Arizona Strip, have made the eastern part of the Park their home.[67]
The conifer forests provide habitat for 52 animal species.[62] Porcupines, shrews, red squirrels, tassel eared Kaibab and Abert's squirrels, Indian peacocks, black bear, mule deer, and elk are found at the park's higher elevations on the Kaibab Plateau.[63]
Above the desert scrub and up to 6,200 feet (1,900 m) is a pinyon pine forest and one seed juniper woodland.[61] Within this woodland one can find big sagebrush, snakeweed, Mormon tea, Utah agave, banana and narrowleaf Yucca, winterfat, Indian ricegrass, dropseed, and needlegrass.[61] There are a variety of snakes and lizards here, but one species of reptile, the mountain short-horned lizard, is a particularly abundant inhabitant of the piñon-juniper and ponderosa pine forests.[70]
Ponderosa pine forests grow at elevations between 6,500 and 8,200 feet (2,000 and 2,500 m), on both North and South rims in the Transition life zone.[61] The South Rim includes species such as gray fox, mule deer, bighorn sheep, rock squirrels, pinyon pine and Utah juniper.[62] Additional species such as Gambel oak, New Mexico locust, mountain mahogany, elderberry, creeping mahonia, and fescue have been identified in these forests.[61] The Utah tiger salamander and the Great Basin spadefoot toad are two amphibians that are common in the rim forests.[65] Of the approximately 90 bird species that breed in the coniferous forests, 51 are summer residents and at least 15 of these are known to be neotropical migrants.[67]
Elevations of 8,200 to 9,000 feet (2,500 to 2,700 m) are in the Canadian Life Zone, which includes the North Rim and the Kaibab Plateau.[62] Spruce-fir forests characterized by Engelmann spruce, blue spruce, Douglas fir, white fir, aspen, and mountain ash, along with several species of perennial grasses, groundsels, yarrow, cinquefoil, lupines, sedges, and asters, grow in this sub-alpine climate.[61] Mountain lions, Kaibab squirrels, and northern goshawks are found here.[62]
Montane meadows and subalpine grassland communities of the Hudsonian life zone are rare and located only on the North Rim.[61] Both are typified by many grass species. Some of these grasses include blue and black grama, big galleta, Indian ricegrass and three-awns.[61] The wettest areas support sedges and forbs.[61]
Grand Canyon National Park is one of the world's premier natural attractions, attracting about five million visitors per year. Overall, 83% were from the United States: California (12%), Arizona (9%), Texas (5%), Florida (3%) and New York (4%) represented the top domestic visitors. Seventeen percent of visitors were from outside the United States; the most prominently represented nations were the United Kingdom (3%), Canada (4%), Japan (2%), Germany (2%) and The Netherlands (1%).[71]
The South Rim is open all year round weather permitting.  The North Rim is generally open mid-May to mid-October.[72]
Aside from casual sightseeing from the South Rim (averaging 7,000 feet [2,100 m] above sea level), rafting, hiking, running, and helicopter tours are popular. The Grand Canyon Ultra Marathon is a 78-mile (126 km) race over 24 hours. The floor of the valley is accessible by foot, muleback, or by boat or raft from upriver. Hiking down to the river and back up to the rim in one day is discouraged by park officials because of the distance, steep and rocky trails, change in elevation, and danger of heat exhaustion from the much higher temperatures at the bottom. Rescues are required annually of unsuccessful rim-to-river-to-rim travelers. Nevertheless, hundreds of fit and experienced hikers complete the trip every year.[citation needed]
Camping on the North and South rims is generally restricted to established campgrounds and reservations are highly recommended, especially at the busier South Rim. There is at large camping available along many parts of the North Rim managed by Kaibab National Forest. North Rim campsites are only open seasonally due to road closures from weather and winter snowpack. All overnight camping below the rim requires a backcountry permit from the Backcountry Office (BCO).[73] Each year Grand Canyon National Park receives approximately 30,000 requests for backcountry permits. The park issues 13,000 permits, and close to 40,000 people camp overnight.[73] The earliest a permit application is accepted is the first of the month, four months before the proposed start month.
Tourists wishing for a more vertical perspective can go skydiving, board helicopters and small airplanes in Boulder, Las Vegas, Phoenix and Grand Canyon National Park Airport (seven miles from the South Rim) for canyon flyovers. Scenic flights are no longer allowed to fly within 1,500 feet (460 m) of the rim within the national park because of a late 1990s crash.[74] The last aerial video footage from below the rim was filmed in 1984. However, some helicopter flights land on the Havasupai and Hualapai Indian Reservations within Grand Canyon (outside of the park boundaries).
In 2007, the Hualapai Tribe opened the glass-bottomed Grand Canyon Skywalk on their property, Grand Canyon West. The Skywalk is about 250 miles (400 km) by road from Grand Canyon Village at the South Rim.[75] 
The skywalk has attracted ""thousands of visitors a year, most from Las Vegas"".[76]
In 2016, skydiving at the Grand Canyon become possible with the first Grand Canyon Skydiving operation opening up at the Grand Canyon National Park Airport, on the South Rim.
In 2014, a developer announced plans to build a multimedia complex on the canyon's rim called the Grand Canyon Escalade. On 420 acres (170 ha) there would be shops, an IMAX theater, hotels and an RV park. A gondola would enable easy visits to the canyon floor where a ""riverwalk"" of ""connected walkways, an eatery, a tramway station, a seating area and a wastewater package plant"" would be situated. Navajo Nation President Ben Shelly has indicated agreement; the tribe would have to invest $65 million for road, water and communication facilities for the $1 billion complex. One of the developers is Navajo and has cited an 8 to 18 percent share of the gross revenue for the tribe as an incentive.[77]
Lipan Point is a promontory located on the South Rim. This point is located to the east of the Grand Canyon Village along the Desert View Drive. There is a parking lot for visitors to Lipan Point. The trailhead to the Tanner Trail is located just before the parking lot. The view from Lipan Point shows a wide array of rock strata and the Unkar Delta area in the inner canyon.[78]
About 770 deaths have occurred between the mid 1800s and 2015.[79][80] Of the fatalities that occurred from 1869 to 2001, some were as follows: 53 resulted from falls; 65 were attributable to environmental causes, including heat stroke, cardiac arrest, dehydration, and hypothermia; 7 were caught in flash floods; 79 were drowned in the Colorado River; 242 perished in airplane and helicopter crashes (128 of them in the 1956 disaster mentioned below); 25 died in freak errors and accidents, including lightning strikes and rock falls; and 23 were the victims of homicides.[81]
In 1956, the Grand Canyon was the site of the deadliest commercial aviation disaster in history at the time.
On the morning of June 30, 1956, a TWA Lockheed Super Constellation and a United Airlines Douglas DC-7 departed Los Angeles International Airport within three minutes of one another on eastbound transcontinental flights. Approximately 90 minutes later, the two propeller-driven airliners collided above the canyon while both were flying in unmonitored airspace.
The wreckage of both planes fell into the eastern portion of the canyon, on Temple and Chuar Buttes, near the confluence of the Colorado and Little Colorado rivers. The disaster killed all 128 passengers and crew members aboard both planes.
This accident led to the institution of high-altitude airways and direct radar observation of aircraft (known as positive control) by en route ground controllers.
In Over the Edge: Death in Grand Canyon, Thomas M. Myers, a journalist and author, documents every death in the Grand Canyon.[82][83]
On October 3, 2020, former Major League Baseball player Charlie Haeger was found dead from a self-inflicted gunshot wound on a canyon trail. He was under investigation for the murder of his ex-girlfriend which had taken place the day before in Scottsdale.[84]
History
Travel and sites
Multimedia
"
Agua Fria National Monument - Wikipedia," Agua Fria National Monument is in the U.S. state of Arizona, approximately 40 miles (64 km) north of downtown Phoenix, Arizona. Created by Presidential proclamation on January 11, 2000, the 72,344-acre (113 sq mi; 293 km2)[1] monument is managed by the Bureau of Land Management, an agency within the U.S. Department of the Interior. The Bureau of Land Management already managed the lands; however, under monument status the level of protection and preservation of resources within the new monument have been enhanced.
The monument is a unit of the BLM's National Landscape Conservation System. Over 450 distinct Native American structures have been recorded in the monument, some of large pueblos containing more than 100 rooms each. The enhanced protection status also provides greater habitat protection for the numerous plant and animal communities.
Petroglyphs are scattered across the numerous puebloan ruins, which were built between 1250 and 1450 C.E. when several thousand Native Americans, known as the Perry Mesa Tradition, inhabited the region. The petroglyphs depict animals, geometric figures and abstract symbols and are found by the thousands. Numerous ruins of agricultural terraces and irrigation devices indicate that farming was widespread during this period. Other historical entities that are found include 19th century mining features and Basque sheep camps.
Situated between 2,150 feet (660 m) and 4,600 feet (1,400 m) in elevation, the monument is primarily composed of semi-desert grassland but also contains extensive riparian stands of cottonwoods and willows which are tied to the Agua Fria River. More than 140 bird species have been recorded at the monument. Notable species of reptiles and amphibians, including the leopard frog, the garter snake, and the desert tortoise, can be seen at the monument. Mammals such as the pronghorn, mule deer, white-tail deer and javelina are relatively common. The elk, black bears and mountain lions are also found in the monument, but are much less common. Native fish including the longfin dace, the Gila mountain sucker, speckled dace, and three endangered native fish including the Gila intermedia, charalito, and desert pupfish exist in the 129-mile (208 km)-long Agua Fria River and its tributaries.
In late 2004, the BLM and the Sierra Club helped spark the formation of the Friends of the Agua Fria National Monument, a non-profit organization created to assist the federal agency in monument protection, management, and outreach.[2][3][4][5]
"
Ironwood Forest National Monument - Wikipedia," Ironwood Forest National Monument is located in the Sonoran Desert of Arizona. Created by Bill Clinton by Presidential Proclamation 7320 on June 9, 2000, the monument is managed by the Bureau of Land Management, an agency within the United States Department of the Interior. The monument covers 129,055 acres (52,227 ha),[2] of which 59,573 acres (24,108 ha) are non-federal and include private land holdings and Arizona State School Trust lands.
A significant concentration of ironwood (also known as desert ironwood, Olneya tesota) trees is found in the monument, along with two federally recognized endangered animal and plant species. More than 200 Hohokam and Paleo-Indian archaeological sites have been identified in the monument, dated between 600 and 1450.
An array of flora are present in the Ironwood Forest National Monument. The higher elevations have the pinyon-juniper woodland plant community. The lower elevations are in the Sonoran Desert ecoregion. One of the notable trees native here is the elephant tree (Bursera microphylla).[3] Small populations of the endangered Nichols turk's head cactus, although not found among ironwood trees, occur in very localized limestone-rich areas within the monument.
The desert ironwood (Olneya tesota) is a very long-lived tree, with some specimens estimated to be more than 800 years old.[4] Desert ironwood is a keystone species because it provides a nursery environment of shade and protection that enables young seedlings of other species to become established despite the harsh desert climate, where daytime high temperatures can exceed 105 °F (41 °C). The ironwood also provides shade and roosting area habitats for birds. Its smoky lavender-colored blossoms provide nectar for bees and other insects, as well as forage for animals. The blossoms produce bean pods which also provide food for desert animals.
Lists of dominant plants in the prehistoric ecology and plant community of the Waterman Mountains area in the monument have been published in a sequence that currently dates back to the last glacial period, the Late Wisconsin glacial period. Dominant trees of that era, based upon pollen records, were Utah juniper (Juniperus osteosperma), single-leaf pinyon (Pinus monophylla), and redberry juniper (Juniperus pinchotii), and understory plants included Monardella arizonica.[3]
According to Proclamation 7320, 674 plant and animal species have been identified in the Silver Bell Mountains within the monument, including 64 species of mammals and 57 species of birds, although the Bureau of Land Management has been unable to verify those claims.  Recent studies by the Arizona-Sonora Desert Museum, however, have documented 560 plant species.[5]  Resident birdwatchers have documented more than 80 species of migratory and non-migratory birds.[6]
One specimen of the endangered lesser long-nosed bat (Leptonycteris curasoae) and a night roost were documented within the monument by bat researchers Karen Krebbs and Yar Petryszyn. They concluded that while the monument may be an important feeding stopover during spring migrations, the presence of L. curasoae in the monument is probably low or incidental.[7] Leptonycteris curasoae is one of only a few bat species that migrate long distances, coming from as far south as Jalisco, Mexico, more than 1,600 miles (2,570 km).
The Arizona desert bighorn sheep herd located within the monument is the last remaining relict population of desert bighorn sheep in southeastern Arizona, having first migrated into North America during the Pleistocene epoch.   One or two specimens of the cactus ferruginous pygmy owl, which  was listed as an endangered species in March 1997 and delisted by court order April 14, 2006, have been found within and near the monument by licensed surveyors.[8]
The Hohokam people were the first miners in the area.  They mined andesite, which was useful for making agave knives.  Andesite knives that originated from Hohokham mines within the monument perimeter have been found as far south as the Gulf of California in Mexico.
Silver and copper mining began in the Silver Bell Mountains around 1850 and continues today. Bighorn sheep ewes prefer mine tailings for lambing grounds because the high, steep and open terrain enables them to see and escape from predators.
The Ironwood Forest National Monument is managed for multiple uses including recreation, cattle grazing and mining, although new mining claims and motorized off-road travel are prohibited by the establishing Proclamation.  Livestock grazing, which has occurred continuously for at least the last 125 years within the monument, is currently managed at very light or conservative levels of approximately one cow per every 300 to 400 acres (1.6 km2). Domestic sheep and goats are prohibited as a protection to the bighorn sheep. The monument offers almost no surface water but contains sufficient groundwater resources.
The cattle ranchers maintain more than 80 individual man-made water sources within the monument, in addition to the 14 water sources maintained by the Arizona Game and Fish Department and the Arizona Desert Bighorn Sheep Society. The presence of human-supplied water supports the exceptional abundance of birds, mule deer, coyotes, foxes, bobcats, mountain lions and other wildlife found in the monument.[9]
The Silver Bell Cemetery in Ironwood Forest National Monument.
Silver Bell Mine is surrounded by ironwood forest to the north and the Tohono O'odham Indian Reservation to the south.
Hohokam ruins in the Los Robles Archaeological District.
Hohokam petroglyphs at Cocoraque Butte.
Saguaro forest with Ragged Top in the background.
"
"Category:Protected areas of Pinal County, Arizona - Wikipedia"," This category has the following 4 subcategories, out of 4 total.
The following 12 pages are in this category, out of  12 total. This list may not reflect recent changes (learn more).
"
Adamsville A.O.U.W. Cemetery - Wikipedia," The Adamsville Cemetery is a historic cemetery located in the Arizona ghost town of Adamsville in Pinal County. The Pioneers' Cemetery Association (PCA) defines a ""historic cemetery"" as one which has been in existence for more than fifty years.[2]
Adamsville was one of the first two towns formed in Pinal County, Arizona. The town was located at an elevation is 1,450 feet, on the south bank of the Gila River, west of Florence, Arizona. It was named for its original settler in 1866, Fred A. Adams.[3] in 1900, the Gila River overflowed and wiped out most of the town. Those who survived the flood moved to the town of Florence.[4] The Adamsville A.O.U.W. Cemetery (Ancient Order of United Workmen) was deeded on May 31, 1894. It is among the few original remains of the town of Adamsville.
The historic cemetery is now within the jurisdiction of the town of Florence. In 1996, V. Phil Hawkins, cleaned, repaired, and identified graves in the cemetery as part of his Eagle Scout Project. Hawkins was able to identify the graves of 54 of those who are interned in the cemetery.[5]
Among those interned in the cemetery and whose graves are pictured are [6]
Cemetery trail
Entrance
Rock formation in the middle of the cemetery
Grave of Fred A. Adams (1844–1910)
Grave of Judge H. B. Summers (1823–1895)
Grave of Capt. Granville Henderson Oury (1825–1891)
Graves of Felix Grunde Hardwick (1831–1908) and his wife Martha Angeline Hardwick (1831–1896)
The Stevens family: Olnorah Stevens (1858–1893); Carmen Sarah Stevens (1888–1889) and Taylor Stevens (1898–1898)
Unknown grave
"
"List of historic Structures in Oatman, Arizona - Wikipedia"," This is a list with images of some of the structures in the historic mining town of Oatman, Arizona which is located in the Black Mountains of Mohave County. Two of the structures are listed in the National Register of Historic Places. Also, included is the gas station of Cool Springs, which served automobile travelers on the historic Route 66 in the vicinity of Oatman.
The Black Mountains of northwest Arizona are an extensive long mountain range. The Hualapai Native-American Tribe inhabited the area. Tensions between the Hualapai people and settlers began with encroachment of Indian lands. Between 1867 and 1869, the Hualapai were at war with the American settlers from the east coast who lived in this area after the death of the prominent Yavapai leader Anasa. The United States Army arrived in the defense of the settlers in what now is known as the Hualapai War. Skirmishing continued for almost two more years in the area after the majority of the Hualapais surrendered due to an outbreak of dysentery and whooping cough in 1869.[1][2]
John Thomas Moss was a frontiersman and prospector from Utica, New York who learned the languages of many of the tribes in the area. In 1862, he discovered gold in the Black Mountains and staked a claim in what became known as the Moss Mine. In 1863, he went into the Hualapai Mountains and organized the profitable Wauba Yuma Mining District.[3][4]
Soon after a tent city was established and named ""Vivian"" nestled in the southern portion of the Black Mountain range.[5] The tent city of Vivian grew as miners and prospectors continued to arrive in the area. Burros, which is the Spanish word for Donkey, were used by the miners to carry essential supplies, including rock and metals in those days. The wild burros, roaming the streets of Oatman today, are the offspring of the burros that were let lose after they became useless to the miners as resources ran out and mines closed.[6]
In 1902, John Durlin built the Drulin Hotel which provided shelter and food to many of the miners in the area. Currently, the hotel, which was renamed in 1960 ""Oatman Hotel"" is the only historic two-story adobe building in Mohave County.[7] By 1904, the Vivian Mining Company began operations and a Post Office was established. Between 1903 and 1905, the town and mines were served by a narrow gauge rail line. The narrow gauge trail line ran 17 miles to the Colorado River near Needles, California.[8]
In 1909, Vivian was formally named Oatman in honor of Olive Oatman. a young 14-year-old girl whose family was attacked by a small group from a Native American tribe believed to be Tolkepayas (Western Yavapai). All were killed except for three of the children: Lorenzo, age 15 (who was left for dead), Olive, and Mary Ann, age 7, who were taken to be slaves for the Yavapais.[9] Olive and Mary Ann were sold as slaves to the Mohave who tattooed both girls on their chins and arms.[10] Olive survived the ordeal, but her younger sister Mary Ann died while in captivity. Eventually. Olive was released when the Mohaves were told by a messenger that the whites would destroy the Mohaves if they did not release her. The trade items included were blankets and a white horse.[11][12]
In 1915, another boom was provided to the settlement when two miners struck a gold find. However, in 1921, a fire of huge proportions consumed many of the structures in the town. The townspeople worked hard to rebuild the small town. Five years later the main mining company, United Eastern Mines, shut down operations for good.[8][7]  The town continued to survive due to the travelers who passed through the old U.S. Route 66 which was built in 1920. The town's economy was once again affected when the route became what is now Interstate 40 and was completely bypassed in 1953.[8][7]
In 1995, the Gold Road Mine was reopened, however the decline of gold prices resulted in its closure in 1998. A renewed interest in the town and travel in Route 66 began with the growth of the nearby gaming town of Laughlin, Nevada. The hotels in Laughlin promotes visits to the town. Oatman is now a tourist attraction.[13][8][7]
Before reaching Oatman through the historic Route 66, there is a gas station, the Cool Springs camp and service station, which originally was built in 1920. The original building burned to the ground in the 1960s and a new one was rebuilt in its place. The pillars are the only items which remained from the original structure. Part of the movie ""Universal Soldier"" was filmed there.[14] Between the station and Oatman there is a stone bridge on the narrow Route 66. The stone bridge is located in what is known as ""Bloody 66"" in Sitgreaves Pass. According to the book “The Big Roads”, authored by Earl Swift,  the 18-foot stone bridge that crossed the wash was too narrow for two speeding cars to pass safely in opposite directions. One car would hit another or worse impale itself, and sometimes its driver, on the wooden guard rail.[15]
Among the structures listed in the National Register of Historic Places are the following:
Pictured are the following structures:
Also pictured:
Oatman on Route 66
Oatman Main Street
Oatman Theatre
The Oatman Drug Company Building
Different view of the Oatman Drug Company Building
Oatman Jail
Oatman Jail cell
Old Building
The Olive Oatman Restaurant and Saloon
Fast Fanny's Place
Oatman Memorial on Main Street
Entrance of the Gold Road Mine
Oatman Hotel originally the Durlin Hotel
Inside the Oatman Hotel
Second floor of the hotel
Hotel rooms in the second floor
The Clark Gable and Carole Lombard honeymoon suite.
Rebuilt Cool Springs camp and service station
Narrow stone bridge on ""Bloody 66"" leading to Oatman
"
File:Oatman-Oatman Hotel-1902-4.jpg - Wikipedia," Original file ‎(4,608 × 3,456 pixels, file size: 3.89 MB, MIME type: image/jpeg)
 
https://creativecommons.org/licenses/by-sa/4.0
CC BY-SA 4.0 
Creative Commons Attribution-Share Alike 4.0 
truetrue
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.

If the file has been modified from its original state, some details may not fully reflect the modified file."
Color balance - Wikipedia," In photography and image processing, color balance is the global adjustment of the intensities of the colors (typically red, green, and blue primary colors).  An important goal of this adjustment is to render specific colors – particularly neutral colors – correctly. Hence, the general method is sometimes called gray balance, neutral balance, or white balance.  Color balance changes the overall mixture of colors in an image and is used for color correction. Generalized versions of color balance are used to correct colors other than neutrals or to deliberately change them for effect.
Image data acquired by sensors – either film or electronic image sensors – must be transformed from the acquired values to new values that are appropriate for color reproduction or display.  Several aspects of the acquisition and display process make such color correction essential – including that the acquisition sensors do not match the sensors in the human eye, that the properties of the display medium must be accounted for, and that the ambient viewing conditions of the acquisition differ from the display viewing conditions.
The color balance operations in popular image editing applications usually operate directly on the red, green, and blue channel pixel values,[1][2] without respect to any color sensing or reproduction model.  In film photography, color balance is typically achieved by using color correction filters over the lights or on the camera lens.[3]
Sometimes the adjustment to keep neutrals neutral is called white balance, and the phrase color balance refers to the adjustment that in addition makes other colors in a displayed image appear to have the same general appearance as the colors in an original scene.[4] It is particularly important that neutral (gray, neutral, white) colors in a scene appear neutral in the reproduction. [5]
Humans relate to flesh tones more critically than other colors. Trees, grass and sky can all be off without concern, but if human flesh tones are 'off' then the human subject can look sick or dead. To address this critical color balance issue, the tri-color primaries themselves are formulated to not balance as a true neutral color. The purpose of this color primary imbalance is to more faithfully reproduce the flesh tones through the entire brightness range.
Most digital cameras have means to select color correction based on the type of scene lighting, using either manual lighting selection, automatic white balance, or custom white balance.[6] The algorithms for these processes perform generalized chromatic adaptation.
Many methods exist for color balancing.  Setting a button on a camera is a way for the user to indicate to the processor the nature of the scene lighting. Another option on some cameras is a button which one may press when the camera is pointed at a gray card or other neutral colored object. This captures an image of the ambient light, which enables a digital camera to set the correct color balance for that light.
There is a large literature on how one might estimate the ambient lighting from the camera data and then use this information to transform the image data. A variety of algorithms have been proposed, and the quality of these has been debated. A few examples and examination of the references therein will lead the reader to many others. Examples are Retinex, an artificial neural network[7] or a Bayesian method.[8]
Color balancing an image affects not only the neutrals, but other colors as well. An image that is not color balanced is said to have a color cast, as everything in the image appears to have been shifted towards one color.[9][page needed] Color balancing may be thought in terms of removing this color cast.
Color balance is also related to color constancy. Algorithms and techniques used to attain color constancy are frequently used for color balancing, as well. Color constancy is, in turn, related to chromatic adaptation. Conceptually, color balancing consists of two steps: first, determining the illuminant under which an image was captured; and second, scaling the components (e.g., R, G, and B) of the image or otherwise transforming the components so they conform to the viewing illuminant.
Viggiano found that white balancing in the camera's native RGB color model tended to produce less color inconstancy (i.e., less distortion of the colors) than in monitor RGB for over 4000 hypothetical sets of camera sensitivities.[10] This difference typically amounted to a factor of more than two in favor of camera RGB. This means that it is advantageous to get color balance right at the time an image is captured, rather than edit later on a monitor. If one must color balance later, balancing the raw image data will tend to produce less distortion of chromatic colors than balancing in monitor RGB.
Color balancing is sometimes performed on a three-component image (e.g., RGB) using a 3x3 matrix. This type of transformation is appropriate if the image was captured using the wrong white balance setting on a digital camera, or through a color filter.
In principle, one wants to scale all relative luminances in an image so that objects which are believed to be neutral appear so. If, say, a surface with 



R
=
240


{\displaystyle R=240}

 was believed to be a white object, and if 255 is the count which corresponds to white, one could multiply all red values by 255/240. Doing analogously for green and blue would result, at least in theory, in a color balanced image. In this type of transformation the 3x3 matrix is a diagonal matrix.
where 



R


{\displaystyle R}

, 



G


{\displaystyle G}

, and 



B


{\displaystyle B}

 are the color balanced red, green, and blue components of a pixel in the image; 




R
′



{\displaystyle R'}

, 




G
′



{\displaystyle G'}

, and 




B
′



{\displaystyle B'}

 are the red, green, and blue components of the image before color balancing, and 




R

w

′



{\displaystyle R'_{w}}

, 




G

w

′



{\displaystyle G'_{w}}

, and 




B

w

′



{\displaystyle B'_{w}}

 are the red, green, and blue components of a pixel which is believed to be a white surface in the image before color balancing. This is a simple scaling of the red, green, and blue channels, and is why color balance tools in Photoshop and the GIMP have a white eyedropper tool.  It has been demonstrated that performing the white balancing in the phosphor set assumed by sRGB tends to produce large errors in chromatic colors, even though it can render the neutral surfaces perfectly neutral.[10]
If the image may be transformed into CIE XYZ tristimulus values, the color balancing may be performed there.  This has been termed a “wrong von Kries” transformation.[11][12] Although it has been demonstrated to offer usually poorer results than balancing in monitor RGB, it is mentioned here as a bridge to other things. Mathematically, one computes:
where 



X


{\displaystyle X}

, 



Y


{\displaystyle Y}

, and 



Z


{\displaystyle Z}

 are the color-balanced tristimulus values; 




X

w




{\displaystyle X_{w}}

, 




Y

w




{\displaystyle Y_{w}}

, and 




Z

w




{\displaystyle Z_{w}}

 are the tristimulus values of the viewing illuminant (the white point to which the image is being transformed to conform to); 




X

w

′



{\displaystyle X'_{w}}

, 




Y

w

′



{\displaystyle Y'_{w}}

, and 




Z

w

′



{\displaystyle Z'_{w}}

 are the tristimulus values of an object believed to be white in the un-color-balanced image, and 




X
′



{\displaystyle X'}

, 




Y
′



{\displaystyle Y'}

, and 




Z
′



{\displaystyle Z'}

 are the tristimulus values of a pixel in the un-color-balanced image. If the tristimulus values of the monitor primaries are in a matrix 




P



{\displaystyle \mathbf {P} }

 so that:
where 




L

R




{\displaystyle L_{R}}

, 




L

G




{\displaystyle L_{G}}

, and 




L

B




{\displaystyle L_{B}}

 are the un-gamma corrected monitor RGB, one may use:
Johannes von Kries, whose theory of rods and three color-sensitive cone types in the retina has survived as the dominant explanation of color sensation for over 100 years, motivated the method of converting color to the LMS color space, representing the effective stimuli for the Long-, Medium-, and Short-wavelength cone types that are modeled as adapting independently.  A 3x3 matrix converts RGB or XYZ to LMS, and then the three LMS primary values are scaled to balance the neutral; the color can then be converted back to the desired final color space:[13]
where 



L


{\displaystyle L}

, 



M


{\displaystyle M}

, and 



S


{\displaystyle S}

 are the color-balanced LMS cone tristimulus values; 




L

w

′



{\displaystyle L'_{w}}

, 




M

w

′



{\displaystyle M'_{w}}

, and 




S

w

′



{\displaystyle S'_{w}}

 are the tristimulus values of an object believed to be white in the un-color-balanced image, and 




L
′



{\displaystyle L'}

, 




M
′



{\displaystyle M'}

, and 




S
′



{\displaystyle S'}

 are the tristimulus values of a pixel in the un-color-balanced image.
Matrices to convert to LMS space were not specified by von Kries, but can be derived from CIE color matching functions and LMS color matching functions when the latter are specified; matrices can also be found in reference books.[13]
By Viggiano's measure, and using his model of gaussian camera spectral sensitivities, most camera RGB spaces performed better than either monitor RGB or XYZ.[10]  If the camera's raw RGB values are known, one may use the 3x3 diagonal matrix:
and then convert to a working RGB space such as sRGB or Adobe RGB after balancing.
Comparisons of images balanced by diagonal transforms in a number of different RGB spaces have identified several such spaces that work better than others, and better than camera or monitor spaces, for chromatic adaptation, as measured by several color appearance models; the systems that performed statistically as well as the best on the majority of the image test sets used were the ""Sharp"", ""Bradford"", ""CMCCAT"", and ""ROMM"" spaces.[14]
The best color matrix for adapting to a change in illuminant is not necessarily a diagonal matrix in a fixed color space.  It has long been known that if the space of illuminants can be described as a linear model with N basis terms, the proper color transformation will be the weighted sum of N fixed linear transformations, not necessarily consistently diagonalizable.[15]
"
Chromophore - Wikipedia," A chromophore is the part of a molecule responsible for its color.[2]
The color that is seen by our eyes is the one not absorbed within a certain wavelength spectrum of visible light. The chromophore is a region in the molecule where the energy difference between two separate molecular orbitals falls within the range of the visible spectrum. Visible light that hits the chromophore can thus be absorbed by exciting an electron from its ground state into an excited state.  In biological molecules that serve to capture or detect light energy, the chromophore is the moiety that causes a conformational change of the molecule when hit by light.
Just like how two adjacent p-orbitals in a molecule will form a pi-bond, three or more adjacent p-orbitals in a molecule can form a conjugated pi-system.  In a conjugated pi-system, electrons are able to capture certain photons as the electrons resonate along a certain distance of p-orbitals - similar to how a radio antenna detects photons along its length.  Typically, the more conjugated (longer) the pi-system is, the longer the wavelength of photon can be captured. In other words, with every added adjacent double bond we see in a molecule diagram, we can predict the system will be progressively more likely to appear yellow to our eyes as it is less likely to absorb yellow light and more likely to absorb red light. (""Conjugated systems of fewer than eight conjugated double bonds absorb only in the ultraviolet region and are colorless to the human eye"", ""Compounds that are blue or green typically do not rely on conjugated double bonds alone."")[4]
In the conjugated chromophores, the electrons jump between energy levels that are extended pi orbitals, created by a series of alternating single and double bonds, often in aromatic systems.  Common examples include retinal (used in the eye to detect light), various food colorings, fabric dyes (azo compounds), pH indicators, lycopene, β-carotene, and anthocyanins.  Various factors in a chromophore's structure go into determining at what wavelength region in a spectrum the chromophore will absorb.  Lengthening or extending a conjugated system with more unsaturated (multiple) bonds in a molecule will tend to shift absorption to longer wavelengths.  Woodward–Fieser rules can be used to approximate ultraviolet-visible maximum absorption wavelength in organic compounds with conjugated pi-bond systems.
Some of these are metal complex chromophores, which contain a metal in a coordination complex with ligands. Examples are chlorophyll, which is used by plants for photosynthesis and hemoglobin, the oxygen transporter in the blood of vertebrate animals.  In these two examples, a metal is complexed at the center of a tetrapyrrole macrocycle ring: the metal being iron in the heme group (iron in a porphyrin ring) of hemoglobin, or magnesium complexed in a chlorin-type ring in the case of chlorophyll.  The highly conjugated pi-bonding system of the macrocycle ring absorbs visible light.  The nature of the central metal can also influence the absorption spectrum of the metal-macrocycle complex or properties such as excited state lifetime.[5][6][7]  The tetrapyrrole moiety in organic compounds which is not macrocyclic but still has a conjugated pi-bond system still acts as a chromophore.  Examples of such compounds include bilirubin and urobilin, which exhibit a yellow color.
An auxochrome is a functional group of atoms attached to the chromophore which modifies the ability of the chromophore to absorb light, altering the wavelength or intensity of the absorption.
Halochromism occurs when a substance changes color as the pH changes.  This is a property of pH indicators, whose molecular structure changes upon certain changes in the surrounding pH.  This change in structure affects a chromophore in the pH indicator molecule.  For example, phenolphthalein is a pH indicator whose structure changes as pH changes as shown in the following table:
In a pH range of about 0-8, the molecule has three aromatic rings all bonded to a tetrahedral sp3 hybridized carbon atom in the middle which does not make the π-bonding in the aromatic rings conjugate. Because of their limited extent, the aromatic rings only absorb light in the ultraviolet region, and so the compound appears colorless in the 0-8 pH range.  However, as the pH increases beyond 8.2, that central carbon becomes part of a double bond becoming sp2 hybridized and leaving a p orbital to overlap with the π-bonding in the rings. This makes the three rings conjugate together to form an extended chromophore absorbing longer wavelength visible light to show a fuchsia color.[8]  At pH ranges outside 0-12, other molecular structure changes result in other color changes; see Phenolphthalein  details.

"
Grey - Wikipedia," Grey or gray (American English alternative; see spelling differences) is an intermediate color between black and white. It is a neutral color or achromatic color, meaning literally that it is a color ""without color"", because it can be composed of black and white.[2] It is the color of a cloud-covered sky, of ash and of lead.[3]
The first recorded use of grey as a color name in the English language was in AD 700.[4] Grey is the dominant spelling in European and Commonwealth English, although gray remained in common usage in the UK until the second half of the 20th century.[5] Gray has been the preferred American spelling since approximately 1825,[6] although grey is an accepted variant.[7][8]
In Europe and North America, surveys show that grey is the color most commonly associated with neutrality, conformity, boredom, uncertainty, old age, indifference, and modesty. Only one percent of respondents chose it as their favorite color.[9]
Grey comes from the Middle English grai or grei, from the Anglo-Saxon grǣġ, and is related to the Dutch grauw and German grau.[10] The first recorded use of grey as a color name in the English language was in AD 700.[4]
In antiquity and the Middle Ages, grey was the color of undyed wool, and thus was the color most commonly worn by peasants and the poor. It was also the color worn by Cistercian monks and friars of the Franciscan and Capuchin orders as a symbol of their vows of humility and poverty. Franciscan friars in England and Scotland were commonly known as the grey friars, and that name is now attached to many places in Great Britain.
During the Renaissance and the Baroque, grey began to play an important role in fashion and art. Black became the most popular color of the nobility, particularly in Italy, France, and Spain, and grey and white were harmonious with it.
Grey was also frequently used for the drawing of oil paintings, a technique called grisaille. The painting would first be composed in grey and white, and then the colors, made with thin transparent glazes, would be added on top. The grisaille beneath would provide the shading, visible through the layers of color. Sometimes the grisaille was simply left uncovered, giving the appearance of carved stone.
Grey was a particularly good background color for gold and for skin tones. It became the most common background for the portraits of Rembrandt Van Rijn and for many of the paintings of El Greco, who used it to highlight the faces and costumes of the central figures. The palette of Rembrandt was composed almost entirely of somber colors. He composed his warm greys out of black pigments made from charcoal or burnt animal bones, mixed with lead white or a white made of lime, which he warmed with a little red lake color from cochineal or madder. In one painting, the portrait of Margaretha de Geer (1661), one part of a grey wall in the background is painted with a layer of dark brown over a layer of orange, red, and yellow earths, mixed with ivory black and some lead white. Over this he put an additional layer of glaze made of mixture of blue smalt, red ochre, and yellow lake. Using these ingredients and many others, he made greys which had, according to art historian Philip Ball, ""an incredible subtlety of pigmentation"".[11] The warm, dark and rich greys and browns served to emphasize the golden light on the faces in the paintings.
Grey became a highly fashionable color in the 18th century, both for women's dresses and for men's waistcoats and coats. It looked particularly luminous coloring the silk and satin fabrics worn by the nobility and wealthy.
Women's fashion in the 19th century was dominated by Paris, while men's fashion was set by London. The grey business suit appeared in the mid-19th century in London; light grey in summer, dark grey in winter; replacing the more colorful palette of men's clothing early in the century.
The clothing of women working in the factories and workshops of Paris in the 19th century was usually grey. This gave them the name of grisettes. ""Gris"" or grey also meant drunk, and the name ""grisette"" was also given to the lower class of Parisian prostitutes.
Grey also became a common color for military uniforms; in an age of rifles with longer range, soldiers in grey were less visible as targets than those in blue or red. Grey was the color of the uniforms of the Confederate Army during the American Civil War, and of the Prussian Army for active service wear from 1910 onwards.
Several artists of the mid-19th century used tones of grey to create memorable paintings; Jean-Baptiste-Camille Corot used tones of green-grey and blue grey to give harmony to his landscapes, and James McNeill Whistler created a special grey for the background of the portrait of his mother, and for his own self-portrait.
Whistler's arrangement of tones of grey had an effect on the world of music, on the French composer Claude Debussy. In 1894, Debussy wrote to violinist Eugène Ysaÿe describing his Nocturnes as ""an experiment in the combinations that can be obtained from one color – what a study in grey would be in painting"".[12]
In the late 1930s, grey became a symbol of industrialization and war. It was the dominant color of Pablo Picasso's celebrated painting about the horrors of the Spanish Civil War, Guernica.[13]
After the war, the grey business suit became a metaphor for uniformity of thought, popularized in such books as The Man in the Gray Flannel Suit (1955), which became a successful film in 1956.[14]
The whiteness or darkness of clouds is a function of their depth. Small, fluffy white clouds in summer look white because the sunlight is being scattered by the tiny water droplets they contain, and that white light comes to the viewer's eye. However, as clouds become larger and thicker, the white light cannot penetrate through the cloud, and is reflected off the top. Clouds look darkest grey during thunderstorms, when they can be as much as 20,000 to 30,000 feet high.
Stratiform clouds are a layer of clouds that covers the entire sky, and which have a depth of between a few hundred to a few thousand feet thick. The thicker the clouds, the darker they appear from below, because little of the sunlight is able to pass through. From above, in an airplane, the same clouds look perfectly white, but from the ground the sky looks gloomy and gray.[15]
The color of a person's hair is created by the pigment melanin, found in the core of each hair. Melanin is also responsible for the color of the skin and of the eyes. There are only two types of pigment: dark (eumelanin) or light (phaeomelanin). Combined in various combinations, these pigments create all natural hair colors.
Melanin itself is the product of a specialized cell, the melanocyte, which is found in each hair follicle, from which the hair grows. As hair grows, the melanocyte injects melanin into the hair cells, which contain the protein keratin and which makes up our hair, skin, and nails. As long as the melanocytes continue injecting melanin into the hair cells, the hair retains its original color. At a certain age, however, which varies from person to person, the amount of melanin injected is reduced and eventually stops. The hair, without pigment, turns grey and eventually white. The reason for this decline of production of melanocytes is uncertain. In the February 2005 issue of Science, a team of Harvard scientists suggested that the cause was the failure of the melanocyte stem cells to maintain the production of the essential pigments, due to age or genetic factors, after a certain period of time. For some people, the breakdown comes in their twenties; for others, many years later.[16] According to the site of the magazine Scientific American, ""Generally speaking, among Caucasians 50 percent are 50 percent grey by age 50.""[17] Adult male gorillas also develop silver hair, but only on their backs - see Physical characteristics of gorillas.
Christine Lagarde, head of the International Monetary Fund
Actor Donald Sutherland
Over the centuries, artists have traditionally created grey by mixing black and white in various proportions. They added a little red to make a warmer grey, or a little blue for a cooler grey. Artists could also make a grey by mixing two complementary colors, such as orange and blue.
Today the grey on televisions, computer displays, and telephones is usually created using the RGB color model. Red, green, and blue light combined at full intensity on the black screen makes white; by lowering the intensity, it is possible to create shades of grey.
In printing, grey is usually obtained with the CMYK color model, using cyan, magenta, yellow, and black. Grey is produced either by using black and white, or by combining equal amounts of cyan, magenta, and yellow. Most greys have a cool or warm cast to them, as the human eye can detect even a minute amount of color saturation. Yellow, orange, and red create a ""warm grey"". Green, blue, and violet create a ""cool grey"".[18] When no color is added, the color is ""neutral grey"", ""achromatic grey"", or simply ""grey"". Images consisting wholly of black, white and greys are called monochrome, black-and-white, or greyscale.
There are several tones of grey available for use with HTML and Cascading Style Sheets (CSS) as named colors, while 254 true greys are available by specification of a hex triplet for the RGB value. All are spelled gray, using the spelling grey can cause errors. This spelling was inherited from the X11 color list. Internet Explorer's Trident browser engine does not recognize grey and renders it green. Another anomaly is that gray is in fact much darker than the X11 color marked darkgray; this is because of a conflict with the original HTML gray and the X11 gray, which is closer to HTML's silver. The three slategray colors are not themselves on the greyscale, but are slightly saturated toward cyan (green + blue). Since there are an even (256, including black and white) number of unsaturated tones of grey, there are two grey tones straddling the midpoint in the 8-bit greyscale. The color name gray has been assigned the lighter of the two shades (128, also known as #808080), due to rounding up.
Until the 19th century, artists traditionally created grey by simply combining black and white. Rembrandt Van Rijn, for instance, usually used lead white and either carbon black or ivory black, along with touches of either blues or reds to cool or warm the grey.
In the early 19th century, a new grey, Payne's grey, appeared on the market. Payne's grey is a dark blue-gray, a mixture of ultramarine and black or of ultramarine and sienna. It is named after William Payne, a British artist who painted watercolors in the late 18th century. The first recorded use of Payne's grey as a color name in English was in 1835.[19]
Grey is a very common color for animals, birds, and fish, ranging in size from whales to mice. It provides a natural camouflage and allows them to blend with their surroundings.
The substance that composes the brain is sometimes referred to as grey matter, or ""the little grey cells"", so the color grey is associated with things intellectual. However, the living human brain is actually pink in color; it only turns grey when dead.
Grey goo is a hypothetical end-of-the-world scenario, also known as ecophagy: out-of-control self-replicating nanobots consume all living matter on Earth while building more of themselves.[20]
In sound engineering, grey noise is random noise subjected to a psychoacoustic equal loudness curve, such as an inverted A-weighting curve, over a given range of frequencies, giving the listener the perception that it is equally loud at all frequencies.
In the Christian religion, grey is the color of ashes, and so a biblical symbol of mourning and repentance, described as sackcloth and ashes. It can be used during Lent or on special days of fasting and prayer. As the color of humility and modesty, grey is worn by friars of the Order of Friars Minor Capuchin and Franciscan order as well as monks of the Cistercian order.[21] Grey cassocks are worn by clergy of the Brazilian Catholic Apostolic Church.
Buddhist monks and priests in Japan and Korea will often wear a sleeved grey, brown, or black outer robe.
Taoist priests in China also often wear grey.
Grey is rarely used as a color by political parties, largely because of its common association with conformity, boredom and indecision. An example of a political party using grey as a color are the German Grey Panthers.
The term ""grey power"" or ""the grey vote"" is sometimes used to describe the influence of older voters as a voting bloc. In the United States, older people are more likely to vote, and usually vote to protect certain social benefits, such as Social Security.[22][23]
Greys is a term sometimes used pejoratively by environmentalists in the green movement to describe those who oppose environmental measures and supposedly prefer the grey of concrete and cement.
During the American Civil War, the soldiers of the Confederate Army wore grey uniforms. At the beginning of the war, the armies of the North and of the South had very similar uniforms; some Confederate units wore blue, and some Union units wore grey. There naturally was confusion, and sometimes soldiers fired by mistake at soldiers of their own army. On June 6, 1861, the Confederate government issued regulations standardizing the army uniform and establishing cadet grey as the uniform color. This was (and still is) the color of the uniform of cadets at the United States Military Academy at West Point, and cadets at the Virginia Military Institute, which produced many officers for the Confederacy.
The new uniforms were designed by Nicola Marschall, a German-American artist, who also designed the original Confederate flag. He closely followed the design of contemporary French and Austrian military uniforms.[24] Grey was not chosen for its camouflage value; this was not appreciated for several more decades; but because the South did not have a major dye industry and grey dyes were inexpensive and easy to manufacture. While some units had uniforms colored with good-quality dyes, which were a solid bluish-grey, others had uniforms colored with vegetable dyes made from sumac or logwood, which quickly faded in sunshine to the yellowish color of butternut squash.
The German Army wore grey uniforms from 1907 until 1945, during both the First World War and Second World War. The color chosen was a grey-green called field grey (German: feldgrau). It was chosen because it was less visible at a distance than the previous German uniforms, which were Prussian blue. It was one of the first uniform colors to be chosen for its camouflage value, important in the new age of smokeless powder and more accurate rifles and machine guns. It gave the Germans a distinct advantage at the beginning of the First World War, when the French soldiers were dressed in blue jackets and red trousers. The Finnish Army also began using grey uniforms on the German model.
Some of the more recent uniforms of the German Army and East German Army were field grey, as were some uniforms of the Swedish army. The formal dress (M/83) of the Finnish Army is grey. The Army of Chile wears field grey today.
During the 19th century, women's fashions were largely dictated by Paris, while London set fashions for men. The intent of a business suit was above all to show seriousness, and to show one's position in business and society. Over the course of the century, bright colors disappeared from men's fashion, and were largely replaced by a black or dark charcoal grey frock coat in winter, and lighter greys in summer. In the early 20th century, the frock coat was gradually replaced by the lounge suit, a less formal version of evening dress, which was also usually black or charcoal grey. In the 1930s the English suit style was called the drape suit, with wide shoulders and a nipped waist, usually dark or light grey. After World War II, the style changed to a slimmer fit called the continental cut, but the color remained grey.[25]
In America and Europe, grey is one of the least popular colors; In a European survey, only one percent of men said it was their favorite color, and thirteen percent called it their least favorite color; the response from women was almost the same. According to color historian Eva Heller, ""grey is too weak to be considered masculine, but too menacing to be considered a feminine color. It is neither warm nor cold, neither material or spiritual. With grey, nothing seems to be decided.""[27] It also denotes undefinedness, as in a grey area.
Grey is the color most commonly associated in many cultures with the elderly and old age, because of the association with grey hair; it symbolizes the wisdom and dignity that come with experience and age. The New York Times is sometimes called The Grey Lady because of its long history and esteemed position in American journalism.[28]
Grey is the color most often associated in Europe and America with modesty.[29]
"
White - Wikipedia," 
White is the lightest color and is achromatic (having no hue). It is the color of fresh snow, chalk and milk, and is the opposite of black. White objects fully reflect and scatter all the visible wavelengths of light.  White on television and computer screens is created by a mixture of red, blue and green light.  In everyday life, whiteness is often conferred with white pigments, especially titanium dioxide, of which is produced more than 3,000,000 tons per year.[1]
In ancient Egypt and ancient Rome, priestesses wore white as a symbol of purity, and Romans wore white togas as symbols of citizenship. In the Middle Ages and Renaissance a white unicorn symbolized chastity, and a white lamb sacrifice and purity. It was the royal color of the kings of France, and of the monarchist movement that opposed the Bolsheviks during the Russian Civil War (1917–1922). Greek and Roman temples were faced with white marble, and beginning in the 18th century, with the advent of neoclassical architecture, white became the most common color of new churches, capitols and other government buildings, especially in the United States. It was also widely used in 20th century modern architecture as a symbol of modernity and simplicity.
According to surveys in Europe and the United States, white is the color most often associated with perfection, the good, honesty, cleanliness, the beginning, the new, neutrality, and exactitude.[2] White is an important color for almost all world religions. The pope, the head of the Roman Catholic Church, has worn white since 1566, as a symbol of purity and sacrifice.  In Islam, and in the Shinto religion of Japan, it is worn by pilgrims. In Western cultures and in Japan, white is the most common color for wedding dresses, symbolizing purity and virginity. In many Asian cultures, white is also the color of mourning.[3]
The word white continues Old English hwīt, ultimately from a Common Germanic *χwītaz also reflected in OHG (h)wîz, ON hvítr, Goth. ƕeits. The root is ultimately from Proto-Indo-European language *kwid-, surviving also in Sanskrit śveta ""to be white or bright""[4] and Slavonic světŭ ""light"".[5][6] The Icelandic word for white, hvítur, is directly derived from the Old Norse form of the word hvítr. Common Germanic also had the word *blankaz (""white, bright, blinding""), borrowed into Late Latin as *blancus, which provided the source for Romance words for ""white"" (Catalan, Occitan and French blanc, Spanish blanco, Italian bianco, Galician-Portuguese branco, etc.). The antonym of white is black.
Some non-European languages have a wide variety of terms for white. The Inuit language has seven different words for seven different nuances of white. Sanskrit has specific words for bright white, the white of teeth, the white of sandalwood, the white of the autumn moon, the white of silver, the white of cow's milk, the white of pearls, the white of a ray of sunlight, and the white of stars.  Japanese has six different words, depending upon brilliance or dullness, or if the color is inert or dynamic.[7]
White was one of the first colors used in art. The Lascaux Cave in France contains drawings of bulls and other animals drawn by paleolithic artists between 18,000 and 17,000 years ago. Paleolithic artists used calcite or chalk, sometimes as a background, sometimes as a highlight, along with charcoal and red and yellow ochre in their vivid cave paintings.[8][9]
In ancient Egypt, white was connected with the goddess Isis. The priests and priestesses of Isis dressed only in white linen, and it was used to wrap mummies.[10]
In Greece and other ancient civilizations, white was often associated with mother's milk. In Greek mythology, the chief god Zeus was nourished at the breast of the nymph Amalthea.  In the Talmud, milk was one of four sacred substances, along with wine, honey, and the rose.[11]
The ancient Greeks saw the world in terms of darkness and light, so white was a fundamental color.  According to Pliny the Elder in his Natural History, Apelles (4th century BC) and the other famous painters of ancient Greece used only four colors in their paintings; white, red, yellow and black;[12] For painting, the Greeks used the highly toxic pigment lead white, made by a long and laborious process.[13]
A plain white toga, known as a toga virilis, was worn for ceremonial occasions by all Roman citizens over the age of 14–18. Magistrates and certain priests wore a toga praetexta, with a broad purple stripe.
In the time of the Emperor Augustus, no Roman man was allowed to appear in the Roman forum without a toga.
The ancient Romans had two words for white; albus, a plain white, (the source of the word albino); and candidus, a brighter white. A man who wanted public office in Rome wore a white toga brightened with chalk, called a toga candida, the origin of the word candidate. The Latin word candere meant to shine, to be bright. It was the origin of the words candle and candid.[14]
In ancient Rome, the priestesses of the goddess Vesta dressed in white linen robes, a white palla or shawl, and a white veil. They protected the sacred fire and the penates of Rome. White symbolized their purity, loyalty, and chastity.[10]
Prehistoric paintings in Chauvet Cave, France (30,000 to 32,000 BC)
Painting of the goddess Isis (1380–1385 BC). The priests of her cult wore white linen.
Paintings of women in white from a tomb (1448–1422 BC).
Statue of the chief Vestal Virgin, wearing a white palla and a white veil.
The early Christian church adopted the Roman symbolism of white as the color of purity, sacrifice and virtue. It became the color worn by priests during Mass, the color worn by monks of the Cistercian Order, and, under Pope Pius V, a former monk of the Dominican Order, it became the official color worn by the pope himself. Monks of the Order of Saint Benedict dressed in the white or gray of natural undyed wool, but later changed to black, the color of humility and penitence.
Postclassical history art, the white lamb became the symbol of the sacrifice of Christ on behalf of mankind. John the Baptist described Christ as the lamb of God, who took the sins of the world upon himself. The white lamb was the center of one of the most famous paintings of the Medieval period, the Ghent Altarpiece by Jan van Eyck.[15]
White was also the symbolic color of the transfiguration. The Gospel of Saint Mark describes Jesus' clothing in this event as ""shining, exceeding white as snow."" Artists such as Fra Angelico used their skill to capture the whiteness of his garments. In his painting of the transfiguration at the Convent of Saint Mark in Florence, Fra Angelico emphasized the white garment by using a light gold background, placed in an almond-shaped halo.[16]
The white unicorn was a common subject of Postclassical history manuscripts, paintings and tapestries. It was a symbol of purity, chastity and grace, which could only be captured by a virgin. It was often portrayed in the lap of the Virgin Mary.[17]
During the Postclassical history, painters rarely ever mixed colors; but in the Renaissance, the influential humanist and scholar Leon Battista Alberti encouraged artists to add white to their colors to make them lighter, brighter, and to add hilaritas, or gaiety. Many painters followed his advice, and the palette of the Renaissance was considerably brighter.[18]
Until the 16th century, white was commonly worn by widows as a color of mourning. The widows of the kings of France wore white until Anne of Brittany in the 16th century. A white tunic was also worn by many knights, along with a red cloak, which showed the knights were willing to give their blood for the king or Church.
The monks of the order of Saint Benedict (circa 480–542) first dressed in undyed white or gray wool robes, here shown in painting by Sodoma on the life of Saint Benedict (1504). They later changed to black robes, the color of humility and penitence.
Under Pope Pius V (1504–1572), a former monk of the Dominican Order, white became the official color worn by the Pope.
The white lamb in the Ghent Altarpiece by Jan van Eyck. (1432)
The Transfiguration by Fra Angelico (1440–1442)
Mary Stuart wore white in mourning for her husband, King Francis II of France, who died in 1560.
White was the dominant color of architectural interiors in the Baroque period and especially the Rococo style that followed it in the 18th century. Church interiors were designed to show the power, glory and wealth of the church. They seemed to be alive, filled with curves, asymmetry, mirrors, gilding, statuary and reliefs, unified by white.
White was also a fashionable color for both men and women in the 18th century. Men in the aristocracy and upper classes wore powdered white wigs and white stockings, and women wore elaborate embroidered white and pastel gowns.
After the French Revolution, a more austere white (blanc cassé) became the most fashionable color in women's costumes which were modeled after the outfits of Ancient Greece and Republican Rome. Because of the rather revealing design of these dresses, the ladies wearing them were called les merveilleuses (the marvellous) by French men of that era.[19] The Empire style under Emperor Napoléon I was modeled after the more conservative outfits of Ancient Imperial Rome. The dresses were high in fashion but low in warmth considering the more severe weather conditions of northern France; in 1814 the former wife of Napoleon, Joséphine de Beauharnais, caught pneumonia and died after taking a walk in the cold night air with Tsar Alexander I of Russia.[20]
White was the universal color of both men and women's underwear and of sheets in the 18th and 19th centuries. It was unthinkable to have sheets or underwear of any other color. The reason was simple; the manner of washing linen in boiling water caused colors to fade. When linen was worn out, it was collected and turned into high-quality paper.[21]
The 19th-century American painter James McNeill Whistler (1834–1903), working at the same time as the French impressionists, created a series of paintings with musical titles where he used color to create moods, the way composers used music. His painting Symphony in White No. 1 – The White Girl, which used his mistress Joanna Hiffernan as a model, used delicate colors to portray innocence and fragility, and a moment of uncertainty.[22]
A highly theatrical white Rococo interior from the 18th century, at the Basilica at Ottobeuren, in Bavaria.
White gown of Marie Antoinette, painted by Elisabeth Vigée-Lebrun in 1783.
President George Washington in a white powdered wig. The first five Presidents of the United States wore dark suits with powdered wigs for formal occasions.
Portrait of Joséphine de Beauharnais in a classic Empire gown, modeled after the clothing of ancient Rome. (1801), by François Gérard. (The State Hermitage Museum).
Symphony in White No. 1 – The White Girl, by James McNeill Whistler (1862).
The White movement was the opposition that formed against the Bolsheviks during the Russian Civil War, which followed the Russian Revolution in 1917. It was finally defeated by the Bolsheviks in 1921–22, and many of its members emigrated to Europe.
At the end of the 19th century, lead white was still the most popular pigment; but between 1916 and 1918, chemical companies in Norway and the United States began to produce titanium white, made from titanium oxide. It had first been identified in the 18th century by the German chemist Martin Klaproth, who also discovered uranium. It had twice the covering power of lead white, and was the brightest white pigment known. By 1945, 80 percent of the white pigments sold were titanium white.[23]
The absoluteness of white appealed to modernist painters. It was used in its simplest form by the Russian suprematist painter Kazimir Malevich in his 1917 painting 'the white square,' the companion to his earlier 'black square.' It was also used by the Dutch modernist painter Piet Mondrian. His most famous paintings consisted of a pure white canvas with grid of vertical and horizontal black lines and rectangles of primary colors.
Black and white also appealed to modernist architects, such as Le Corbusier (1887–1965). He said a house was ""a machine for living in"" and called for a ""calm and powerful architecture"" built of reinforced concrete and steel, without any ornament or frills.[24] Almost all the buildings of contemporary architect Richard Meier, such as his museum in Rome to house the ancient Roman Ara Pacis, or Altar of Peace, are stark white, in the tradition of Le Corbusier.
Poster for the White Army during the Russian Civil War (1917–22). The poster says: ""for a United Russia.""
The Villa Savoye (1928–31) by Le Corbusier; Le Corbusier called for a ""calm and powerful"" architecture built of steel and reinforced concrete, without color or ornament.
Light is perceived by the human visual system as white when the incoming light to the eye stimulates all three types of color sensitive cone cells in the eye in roughly equal amounts.[25] Materials that do not emit light themselves appear white if their surfaces reflect back most of the light that strikes them in a diffuse way.
In the RGB color model, used to create colors on TV and computer screens, white is made by mixing red, blue and green light at full intensity.
White light refracted in a prism revealing the color components.
In 1666, Isaac Newton demonstrated that white light could be broken up into its composite colors by passing it through a prism, then using a second prism to reassemble them.  Before Newton, most scientists believed that white was the fundamental color of light.[26]
White light can be generated by the sun, by stars, or by earthbound sources such as fluorescent lamps, white LEDs and incandescent bulbs. On the screen of a color television or computer, white is produced by mixing the primary colors of light: red, green and blue (RGB) at full intensity, a process called additive mixing (see image below). White light can be fabricated using light with only two wavelengths, for instance by mixing light from a red and cyan laser or yellow and blue lasers. This light will however have very few practical applications since color rendering of objects will be greatly distorted.
The fact that light sources with vastly different spectral power distributions can result in a similar sensory experience is due to the way the light is processed by the visual system. One color that arises from two different spectral power distributions is called a metamerism.
Many of the light sources that emit white light emit light at almost all visible wavelengths (sun light, incandescent lamps of various Color temperatures). This has led to the notion that white light can be defined as a mixture of ""all colors"" or ""all visible wavelengths"".[27][28] This widespread idea is a misconception,[citation needed]  and might originally stem from the fact that Newton discovered that sunlight is composed of light with wavelengths across the visible spectrum. Concluding that since ""all colors"" produce white light then white must be made up of ""all colors"" is a common logical error called affirming the consequent, which might be the cause of the misunderstanding.
A range of spectral distributions of light sources can be perceived as white—there is no single, unique specification of ""white light"".  For example, when you buy a ""white"" light bulb, you might buy one labeled 2700K, 6000K, etc., which produce light having very different spectral distributions, and yet this will not prevent you from identifying the color of objects that they illuminate.[29]
Color vision allows us to distinguish different objects by their color. In order to do so, color constancy can keep the perceived color of an object relatively unchanged when the illumination changes among various broad (whitish) spectral distributions of light.[29]
The same principle is used in photography and cinematography where the choice of white point determines a transformation of all other color stimuli. Changes in or manipulation of the white point can be used to explain some optical illusions such as The dress.
While there is no single, unique specification of ""white light"", there is indeed a unique specification of ""white object"", or, more specifically, ""white surface"".
A perfectly white surface diffusely reflects (scatters) all visible light that strikes it, without absorbing any, irrespective of the light's wavelength or spectral distribution.[30][31]
Since it does not absorb any of the incident light, white is the lightest possible color.
If the reflection is not diffuse but rather specular, this describes a mirror rather than a white surface.[32][30]
Reflection of 100% of incident light at all wavelengths is a form of uniform reflectance, so white is an achromatic color, meaning a color without hue.[33][34] The color stimulus produced by the perfect diffuser is usually considered to be an achromatic stimulus for all illuminants, except for those whose light sources appear to be highly chromatic.[35]
Color constancy is achieved by chromatic adaptation.  The International Commission on Illumination defines white (adapted) as ""a color stimulus that an observer who is [chromatically] adapted to the viewing environment would judge to be perfectly achromatic and to have a luminance factor of unity.  The color stimulus that is considered to be the adapted white may be different at different locations within a scene.[36]
Snow is composed of ice and air; it scatters or reflects sunlight without absorbing other colors of the spectrum.
Cumulus clouds look white because the water droplets reflect and scatter the sunlight without absorbing other colors.
The White Cliffs of Dover, made of limestone
Hyams Beach, New South Wales appears white because the sunlight is reflected or scattered by the quartz or limestone sand
Beaches with sand containing high amounts of quartz or eroded limestone also appear white, since quartz and limestone reflect or scatter sunlight, rather than absorbing it. Tropical white sand beaches may also have a high quantity of white calcium carbonate from tiny bits of seashells ground to fine sand by the action of the waves.[37]
The White Cliffs of Dover take their white color from the large amount of chalk, made of limestone, which they contain, which reflects the sunlight.
Snow is a mixture of air and tiny ice crystals. When white sunlight enters snow, very little of the spectrum is absorbed; almost all of the light is reflected or scattered by the air and water molecules, so the snow appears to be the color of sunlight, white. Sometimes the light bounces around inside the ice crystals before being scattered, making the snow seem to sparkle.[38]
In the case of glaciers, the ice is more tightly pressed together and contains little air. As sunlight enters the ice, more light of the red spectrum is absorbed, so the light scattered will be bluish.[39]
Clouds are white for the same reason as ice. They are composed of water droplets or ice crystals mixed with air, very little light that strikes them is absorbed, and most of the light is scattered, appearing to the eye as white. Shadows of other clouds above can make clouds look gray, and some clouds have their own shadow on the bottom of the cloud.[40]
Many mountains with winter or year-round snow cover are named accordingly: Mauna Kea means white mountain in Hawaiian, Mont Blanc means white mountain in French. Changbai Mountains literally meaning perpetually white mountains, marks the border between China and Korea.
Titanium white, made with titanium dioxide, is the brightest white paint available. It also colors most toothpaste and sunscreen.
Zinc white is made from zinc oxide. Zinc oxide is used in paints, suntan lotion, and some foods.
Chalk is a kind of limestone, made of the mineral calcite, or calcium carbonate. It was originally deposited under the sea as the scales or plates of tiny micro-organisms called Coccolithophore. It was the first white pigment used by prehistoric artists in cave paintings. The chalk used on blackboards today is usually made of gypsum or calcium sulphate, a powder pressed into sticks.
Bianco di San Giovanni is a pigment used in the Renaissance, which was described by the painter Cennino Cennini in the 15th century. It is similar to chalk, made of calcium carbonate with calcium hydroxide. It was made of dried lime which was made into a powder, then soaked in water for eight days, with the water changed each day. It was then made into cakes and dried in the sun.[41]
Lead white was being produced during the 4th century BC; the process is described is Pliny the Elder, Vitruvius and the ancient Greek author Theophrastus. Pieces of lead were put into clay pots which had a separate compartment filled with vinegar. The pots in turn were piled on shelves close to cow dung. The combined fumes of the vinegar and the cow dung caused the lead to corrode into lead carbonate. It was a slow process which could take a month or more. It made an excellent white and was used by artists for centuries, but it was also toxic. It was replaced in the 19th century by zinc white and titanium white.[42]
Titanium white is the most popular white for artists today; it is the brightest available white pigment, and has twice the coverage of lead white. It first became commercially available in 1921. It is made out of titanium dioxide, from the minerals brookite, anatase, rutile, or ilmenite, currently the major source. Because of its brilliant whiteness, it is used as a colorant for most toothpaste and sunscreen.[43]
Zinc white is made from zinc oxide. It is similar to but not as opaque as titanium white. It is added to some foods to enrich them with zinc, an important nutrient.[44] Chinese white is a variety of zinc white made for artists.[45]
Some materials can be made to look ""whiter than white"", this is achieved using optical brightener agents (OBA). These are chemical compounds that absorb light in the ultraviolet and violet region (usually 340–370 nm) of the electromagnetic spectrum, and re-emit light in the blue region (typically 420–470 nm). OBAs are often used in paper and clothing to create an impression of very bright white. This is due to the fact that the materials actually send out more visible light than they receive.
Bleaching is a process for whitening fabrics which has been practiced for thousands of years. Sometimes it was simply a matter of leaving the fabric in the sun, to be faded by the bright light. In the 18th century several scientists developed varieties of chlorine bleach, including sodium hypochlorite and calcium hypochlorite (bleaching powder).[46] Bleaching agents that do not contain chlorine most often are based on peroxides, such as hydrogen peroxide, sodium percarbonate and sodium perborate. While most bleaches are oxidizing agents, a fewer number are reducing agents such as sodium dithionite.
Bleaches attack the chromophores, the part of a molecule which absorbs light and causes fabrics to have different colors. An oxidizing bleach works by breaking the chemical bonds that make up the chromophore. This changes the molecule into a different substance that either does not contain a chromophore, or contains a chromophore that does not absorb visible light. A reducing bleach works by converting double bonds in the chromophore into single bonds. This eliminates the ability of the chromophore to absorb visible light.[47]
Sunlight acts as a bleach through a similar process. High energy photons of light, often in the violet or ultraviolet range, can disrupt the bonds in the chromophore, rendering the resulting substance colorless.[48]
Some detergents go one step further; they contain fluorescent chemicals which glow, making the fabric look literally whiter than white.[49]
A white dwarf is a stellar remnant composed mostly of electron-degenerate matter. They are very dense; a white dwarf's mass is comparable to that of the Sun and its volume is comparable to that of the Earth. Its faint luminosity comes from the emission of stored thermal energy. A white dwarf is very hot when it is formed, but since it has no source of energy, it will gradually radiate away its energy and cool down. This means that its radiation, which initially has a high color temperature, will lessen and redden with time. Over a very long time, a white dwarf will cool to temperatures at which it will no longer emit significant heat or light, and it will become a cold black dwarf.[50] However, since no white dwarf can be older than the Age of the universe (approximately 13.8 billion years),[51] even the oldest white dwarfs still radiate at temperatures of a few thousand kelvins, and no black dwarfs are thought to exist yet.
An A-type main-sequence star (A V) or A dwarf star is a main-sequence (hydrogen-burning) star of spectral type A and luminosity class V. These stars have spectra which are defined by strong hydrogen Balmer absorption lines.[52][53] They have masses from 1.4 to 2.1 times the mass of the Sun and surface temperatures between 7600 and 11 500  K.[54]
White animals use their color as a form of camouflage in winter.
The dove is an international symbol of peace.
The ermine, or stoat. Once considered the most noble of animals because it would rather die than dirty its fur.
The Beluga whale lives in Arctic and sub-arctic waters, where its color is an effective camouflage
A Polar Bear in Alaska.  Its color is a form of camouflage
Thousands of pilgrims in white gather in Mecca for the beginning of their pilgrimage, or Hajj.
Pope Francis at the Vatican.  Popes have traditionally worn white since 1566.
A pilgrim in Japan.
In the Shinto religion of Japan brides traditionally wear a white wedding kimono.
The Buddhist deity Tara is often depicted with white skin.
White is an important symbolic color in most religions and cultures, usually because of its association with purity.
In the Roman Catholic Church, white is associated with Jesus Christ, innocence and sacrifice. Since the Middle Ages, priests wear a white cassock in many of the most important ceremonies and religious services connected with events in the life of Christ. White is worn by priests at Christmas, during Easter, and during celebrations connected with the other events of the life of Christ, such as Corpus Christi Sunday, and Trinity Sunday. It is also worn at the services dedicated to the Virgin Mary, and to those Saints who were not martyred, as well as other special occasions, such as the ordination of priests and the installation of new bishops. Within the hierarchy of the church, the lighter the color, the higher the rank. Ordinary priests wear black; bishops wear violet, cardinals wear red, and outside a church, only the Pope will wear white.[55] Popes occasionally wore white in the Middle Ages, but usually wore red. Popes have worn white regularly since 1566, when Pope Pius V, a member of the Dominican Order, began the practice.  White is the color of the Dominican Order.
In Islam, white clothing is worn during required pilgrimage to Mecca, or Ihram pilgrimage (Hajj).Hajj. Called Ihram clothing, men's garments often consist of two white un-hemmed sheets (usually towelling material). The top (the riḍā) is draped over the torso and the bottom (the izār) is secured by a belt; plus a pair of sandals. Women's clothing varies considerably and reflects regional as well as religious influences. Ihram is typically worn during Dhu al-Hijjah, the last month in the Islamic calendar.
White also has a long history of use as a religious and political symbol in Islam, beginning with the white banner that tradition ascribes to the Quraysh, the tribe to which Muhammad belonged. The Umayyad dynasty also used white as its dynastic color, following the personal banner of its founder, Mu'awiya I, while the Shi'ite Fatimids also chose white to highlight their opposition to the Sunni Abbasid Caliphate, whose color was black.[56]
In Judaism, during the rituals of Yom Kippur, the ceremony of atonement, the rabbi dresses in white, as do the members of the congregation, to restore the bonds between God and his followers.
In the traditional Japanese religion of Shinto, an area of white gravel or stones marks a sacred place, called a niwa. These places were dedicated to the kami, spirits which had descended from the heavens or had come across the sea. Later, temples of Zen Buddhism in Japan often featured a Zen garden, where white sand or gravel was carefully raked to resemble rivers or streams, designed as objects of meditation.[57]
In the temples of The Church of Jesus Christ of Latter-day Saints (LDS Church or also known as Mormon), White clothing is worn inside once they have been officially dedicated, due to white symbolizing purity.[58]
Many religions symbolize heaven by using a sky with white clouds. This phenomenon is not limited to western culture; in Yoruba religion, the orisha Obatala in the Ifá tradition is represented by white. Obatala is associated with calmness, morality, old age, and purity.
In Theosophy and similar religions, the deities called the Great White Brotherhood are said to have white auras.[59]
In some Asian and Slavic cultures, white is considered to be a color that represents death.[60] White also represented death in ancient Egypt, representing the lifeless desert that covered much of the country; black was held to be the color of life, representing the mud-covered fertile lands created by the flooding of the Nile and giving the country its name (Kemet, or ""black land"").
In China, Korea, and some other Asian countries, white, or more precisely, the whitish color of undyed linen, is the color of mourning and funerals.[61]
In traditional China, undyed linen clothing is worn at funerals. As time passes, the bereaved can gradually wear clothing dyed with colors, then with darker colors. Small sacks of quicklime, one for each year of the life of the deceased are placed around the body to protect it against impurity in the next world, and white paper flowers are placed around the body.[62]
In China and other Asian countries, white is the color of reincarnation, showing that death is not a permanent separation from the world.[63]
In China, white is associated with the masculine (the yang of the yin and yang); with the unicorn and tiger; with the fur of an animal; with the direction of west; with the element metal; and with the autumn season.[64]
In Japan, undyed linen white robes are worn by pilgrims for rituals of purification, and bathing in sacred rivers. In the mountains, pilgrims wear costumes of undyed jute to symbolize purity. A white kimono is often placed in the casket with the deceased for the journey to the other world, as white represents death sometimes.[65] Condolence gifts, or kooden, are tied with black and white ribbons and wrapped in white paper, protecting the contents from the impurities of the other world.[66]
In India, it is the color of purity, divinity, detachment and serenity. In Hindi, the name Sweta means white.
In Tibetan Buddhism, white robes were reserved for the lama of a monastery.
In the Bedouin and some other pastoral cultures, there is a strong connection between milk and white, which is considered the color of gratitude, esteem, joy, good fortune and fertility.[67]
White is often associated with monarchism. The association originally came from the white flag of the Bourbon dynasty of France. White became the banner of the royalist rebellions against the French Revolution (see Revolt in the Vendée).
During the Civil War which followed the Russian Revolution of 1917, the White Army, a coalition of monarchists, nationalists and liberals, fought unsuccessfully against the Red Army of the Bolsheviks. A similar battle between reds and whites took place during the Civil War in Finland in the same period.
The Ku Klux Klan is a racist and anti-immigrant organization which flourished in the Southern United States after the American Civil War. They wore white robes and hoods, burned crosses and violently attacked and murdered black Americans.
In Iran, the White Revolution was a series of social and political reforms launched in 1963 by the last Shah of Iran before his downfall.
White is also associated with peace and passive resistance. The white ribbon is worn by movements denouncing violence against women and the White Rose was a non-violent resistance group in Nazi Germany.
White is a common color in national flags, though its symbolism varies widely.  The white in the flag of the United States and flag of the United Kingdom comes from traditional red St George's Cross on a white background of the historic flag of England. The white in the flag of France represents either the monarchy or ""white, the ancient French color"" according to the Marquis de Lafayette.
Many flags in the Arab world use the colors of the flag of the Arab Revolt of 1916; red, white, green and black. These include the flags of Egypt, Palestine, Jordan, Syria, Kuwait and Iraq.
Philippines also use white as their symbol for unity in their flag.
Flag of the Bourbons, royal family of France until the French Revolution and during the restoration of the monarchy afterwards.
The Flag of Vatican City (1929).  The white and gold colors symbolize the colors of the keys to heaven given by Jesus Christ to Saint Peter: the gold of spiritual power, the white of worldly power. The keys have been the Papal symbol since the 13th century.
The flag of the Netherlands (1572) was the first red, white and blue national flag. Peter the Great adopted the colors for the flag of Russia.
The flag of India (1947).  White represents ""light, the path of truth"".[68]
The flag of Ireland. According to the Irish government press office, ""The green represents the older Gaelic tradition while the orange represents the supporters of William of Orange. The white in the centre signifies a lasting truce between the 'Orange' and the 'Green'. ""[69]
In Western culture, white is the color most often associated with innocence, or purity.[74] In the Bible and in Temple Judaism, white animals such as lambs were sacrificed to expiate sins. The white lily is considered the flower of purity and innocence, and is often associated with the Virgin Mary.
White is the color in Western culture most often associated with beginnings. In Christianity, children are baptized and first take communion wearing white. Christ after the Resurrection is traditionally portrayed dressed in white.
Queen Elizabeth II wears white when she opens each session of British Parliament. In high society, debutantes traditionally wear white for their first ball.
White has long been the traditional color worn by brides at royal weddings, but the white wedding gown for ordinary people appeared in the 19th century. Before that time, most brides wore their best Sunday clothing, of whatever color.[75] The white lace wedding gown of Queen Victoria in 1840 had a large impact on the color and fashion of wedding dresses in both Europe and America down to the present day.
The wedding dress of Queen Victoria (1840) set the fashion for wedding dresses of the Victorian era and for the 20th century.
The Barong Tagalog is a traditional folk costume of the Philippines; this attire is worn on formal gatherings and weddings.
White is the color most associated with cleanliness. Objects which are expected to be clean, such as refrigerators and dishes, toilets and sinks, bed linen and towels, are traditionally white. White was the traditional color of the coats of doctors, nurses, scientists and laboratory technicians, though nowadays a pale blue or green is often used. White is also the color most often worn by chefs, bakers, and butchers, and the color of the aprons of waiters in French restaurants.[76]
White is the color associated with ghosts and phantoms. In the past the dead were traditionally buried in a white shroud.  Ghosts are said to be the spirits of the dead who, for various reasons, are unable to rest or enter heaven, and so walk the earth in their white shrouds. White is also connected with the paleness of death. A common expression in English is ""pale as a ghost.""[77]
The woman in white, Weiße Frau, or dame blanche is a familiar figure in English, German and French ghost stories. She is a spectral apparition of a female clad in white, in most cases the ghost of an ancestor, sometimes giving warning about death and disaster. The most notable Weiße Frau is the legendary ghost of the German Hohenzollern dynasty.
Seeing a white horse in a dream is said to be presentiment of death.[78] In the Book of Revelation, the last book in the New Testament of the Bible, the Four Horsemen of the Apocalypse are supposed to announce the Apocalypse before the Last Judgement. The man on a white horse with a bow and arrow. according to different interpretations, represents either War and Conquest, the Antichrist, or Christ himself, cleansing the world of sin. Death rides a horse whose color is described in ancient Greek as khlōros (χλωρός) in the original Koine Greek,[79] which can mean either green/greenish-yellow or pale/pallid.[80]
Black and white often represent the contrast between light and darkness, day and night, male and female, good and evil.
In taoism, the two complementary natures of the universe, yin and yang, are often symbolized in black and white, Ancient games of strategy, such as go and chess, use black and white to represent the two sides.
In the French monarchy, white symbolized the King and his power par la grâce de Dieu (""by the grace of God"") and in contrast black was the color of the queen who according to the Salic Law which excluded women from the throne (and thus from power) could never become the ruling monarch.
Black and white also often represent formality and seriousness, as in the costumes of judges and priests, business suits, of formal evening dress. Monks of the Dominican Order wear a black cloak over a white habit. Until 1972 agents of the Federal Bureau of Investigation were informally required by FBI Director J. Edgar Hoover to wear white shirts with their suits, to project the correct image of the FBI.[81]
White is the source of more names for women in western countries than any other color.[82] Names taken from white include Alba, Albine (Latin). Blandine, Blanche and Blanchette (French); Bianca (Italian); Jennifer (Celt); Genevieve, Candice (from Latin Candida); Fenela, Fiona and Finola (Irish); Gwendoline, Gwenael, Nol(g)wen (white woman) (Celt), Nives (Spanish) and Zuria (Basque).[83]
In addition many names come from white flowers: Camille, Daisy, Lily, Lili, Magnolie, Jasmine, Yasemine, Leila, Marguerite, Rosalba, and others.
Other names come from the white pearl; Pearl, Margarita (Latin), Margaret, Margarethe, Marga, Grete, Rita, Gitta, Marjorie, Margot.
Since ancient times, temples, churches, and many government buildings in many countries have traditionally been white, the color associated with religious and civic virtue. The Parthenon and other ancient temples of Greece, and the buildings of the Roman Forum were mostly made of or clad in white marble, though it is now known that some of these ancient buildings were actually brightly painted.[84] The Roman tradition of using white stone for government buildings and churches was revived in the Renaissance and especially in the neoclassic style of the 18th and 19th centuries. White stone became the material of choice for government buildings in Washington D.C. and other American cities. European cathedrals were also usually built of white or light-colored stone, though many darkened over the centuries from smoke and soot.
The Renaissance architect and scholar Leon Battista Alberti wrote in 1452 that churches should be plastered white on the inside, since white was the only appropriate color for reflection and meditation.[85] After the Reformation, Calvinist churches in the Netherlands were whitewashed and sober inside, a tradition that was also followed in the Protestant churches of New England, such as Old North Church in Boston.
Although the Parthenon in Athens (5th century BC) is white today, it was originally painted with many colors
The Cathedral of Milan (1386–1965)
Dutch Reformed Church interior in Delft, the Netherlands (16th century)
Interior of Old North Church, Boston (1723)
The White House (1801), Washington D.C.
People of the Caucasian race are often referred to simply as white. The United States Census Bureau defines white people as those ""having origins in any of the original peoples of Europe, the Middle East, or North Africa. It includes people who reported ""white"" or wrote in entries such as Irish, German, Italian, Lebanese, Near Easterner, Arab, or Polish.""[86] White people constitute the majority of the U.S. population, with a total of 223,553,265 or 72% of the population in the 2010 United States Census.
A white flag has long been used to represent either surrender or a request for a truce. It is believed to have originated in the 15th century, during the Hundred Years' War between France and England, when multicolored flags, as well as firearms, came into common use by European armies. The white flag was officially recognized as a request to cease hostilities by the Geneva Convention of 1949.[87]
In English heraldry, white or silver signified brightness, purity, virtue, and innocence.[88]
"
Sand - Wikipedia," 

Sand is a granular material composed of finely divided rock and mineral particles. It is defined by size, being finer than gravel and coarser than silt. Sand can also refer to a textural class of soil or soil type; i.e., a soil containing more than 85 percent sand-sized particles by mass.[2]
The composition of sand varies, depending on the local rock sources and conditions, but the most common constituent of sand in inland continental settings and non-tropical coastal settings is silica (silicon dioxide, or SiO2), usually in the form of quartz. The second most common type of sand is calcium carbonate, for example, aragonite, which has mostly been created, over the past half billion years, by various forms of life, like coral and shellfish. For example, it is the primary form of sand apparent in areas where reefs have dominated the ecosystem for millions of years like the Caribbean.  Somewhat more rarely, sand may be composed of calcium sulfate, such as gypsum and selenite, as is found in places like White Sands National Park and Salt Plains National Wildlife Refuge in the U.S.  
Sand is a non-renewable resource over human timescales, and sand suitable for making concrete is in high demand.[3] Desert sand, although plentiful, is not suitable for concrete. 50 billion tons of beach sand and fossil sand is used each year for construction.[4]
The exact definition of sand varies. The scientific Unified Soil Classification System used in engineering and geology corresponds to US Standard Sieves,[5] and defines sand as particles with a diameter of between 0.074 and 4.75 millimeters. By another definition, in terms of particle size as used by geologists, sand particles range in diameter from 0.0625 mm (or ​1⁄16 mm) to 2 mm. An individual particle in this range size is termed a sand grain. Sand grains are between gravel (with particles ranging from 2 mm up to 64 mm by the latter system, and from 4.75 mm up to 75 mm in the former) and silt (particles smaller than 0.0625 mm down to 0.004 mm). The size specification between sand and gravel has remained constant for more than a century, but particle diameters as small as 0.02 mm were considered sand under the Albert Atterberg standard in use during the early 20th century. The grains of sand in Archimedes' The Sand Reckoner written around 240 BCE, were 0.02 mm in diameter. A 1938 specification of the United States Department of Agriculture was 0.05 mm.[6] A 1953 engineering standard published by the American Association of State Highway and Transportation Officials set the minimum sand size at 0.074 mm. Sand feels gritty when rubbed between the fingers. Silt, by comparison, feels like flour.
ISO 14688 grades sands as fine, medium, and coarse with ranges 0.063 mm to 0.2 mm to 0.63 mm to 2.0 mm. In the United States, sand is commonly divided into five sub-categories based on size: very fine sand (​1⁄16 – ​1⁄8 mm diameter), fine sand (​1⁄8 mm – ​1⁄4 mm), medium sand (​1⁄4 mm – ​1⁄2 mm), coarse sand (​1⁄2 mm – 1 mm), and very coarse sand (1 mm – 2 mm). These sizes are based on the Krumbein phi scale, where size in Φ = -log2D; D being the particle size in mm. On this scale, for sand the value of Φ varies from −1 to +4, with the divisions between sub-categories at whole numbers.
The most common constituent of sand, in inland continental settings and non-tropical coastal settings, is silica (silicon dioxide, or SiO2), usually in the form of quartz, which, because of its chemical inertness and considerable hardness, is the most common mineral resistant to weathering.
The composition of mineral sand is highly variable, depending on the local rock sources and conditions. The bright white sands found in tropical and subtropical coastal settings are eroded limestone and may contain coral and shell fragments in addition to other organic or organically derived fragmental material, suggesting that sand formation depends on living organisms, too.[7] The gypsum sand dunes of the White Sands National Park in New Mexico are famous for their bright, white color. Arkose is a sand or sandstone with considerable feldspar content, derived from weathering and erosion of a (usually nearby) granitic rock outcrop. Some sands contain magnetite, chlorite, glauconite, or gypsum. Sands rich in magnetite are dark to black in color, as are sands derived from volcanic basalts and obsidian. Chlorite-glauconite bearing sands are typically green in color, as are sands derived from basaltic lava with a high olivine content. Many sands, especially those found extensively in Southern Europe, have iron impurities within the quartz crystals of the sand, giving a deep yellow color. Sand deposits in some areas contain garnets and other resistant minerals, including some small gemstones.
Rocks erode or weather over a long period of time, mainly by water and wind, and their sediments are transported downstream. These sediments continue to break apart into smaller pieces until they become fine grains of sand. The type of rock the sediment originated from and the intensity of the environment gives different compositions of sand. The most common rock to form sand is granite, where the feldspar minerals dissolve faster than the quartz, causing the rock to break apart into small pieces. In high energy environments rocks break apart much faster than in more calm settings. For example, Granite rocks this means more Feldspar minerals in the sand because it wouldn't have had time to dissolve. The term for sand formed by weathering is epiclastic.[8]
Sand from rivers are collected either from the river itself or its flood plain and accounts for the majority of the sand used in the construction industry. Because of this, many small rivers have been depleted, causing environmental concern and economic losses to adjacent land. The rate of sand mining in such areas greatly outweighs the rate the sand can replenish, making it a non-renewable resource.[9]
Sand dunes are a consequence of dry conditions or wind deposition. The Sahara Desert is very dry because of its geographic location and is known for its vast sand dunes. They exist here because very little vegetation is able to grow and there's not a lot of water. Over time, wind blows away all the fine particles, such as clay and dead organic matter, leaving only sand and larger rocks. Only 15% of the Sahara is sand dunes, while 70% is bare rock.[10] The wind is responsible for creating these different environments and shaping the sand to be round and smooth. These properties make desert sand unusable for construction.[11]
Beach sand is also formed by erosion. Over thousands of years, rocks are eroded near the shoreline from the constant motion of waves and the sediments build up. Weathering and river deposition also accelerate the process of creating a beach, along with marine animals interacting with rocks, such as eating the algae off of them. Once there is a sufficient amount of sand, the beach acts as a barrier to keep the land from eroding any further. This sand is ideal for construction as it is angular and of various sizes.[12]
Marine sand (or ocean sand) comes from sediments transported into the ocean and the erosion of ocean rocks. The thickness of the sand layer varies, however it is common to have more sand closer to land; this type of sand is ideal for construction and is a very valuable commodity. Europe is the main miners of marine sand, which greatly hurts ecosystems and local fisheries.[9]
The study of individual grains can reveal much historical information as to the origin and kind of transport of the grain.[13] Quartz sand that is recently weathered from granite or gneiss quartz crystals will be angular. It is called grus in geology or sharp sand in the building trade where it is preferred for concrete, and in gardening where it is used as a soil amendment to loosen clay soils. Sand that is transported long distances by water or wind will be rounded, with characteristic abrasion patterns on the grain surface. Desert sand is typically rounded.
People who collect sand as a hobby are known as arenophiles. Organisms that thrive in sandy environments are psammophiles.[14]
Only some sands are suitable for the construction industry, for example for making concrete. Because of the growth of population and of cities and the consequent construction activity there is a huge demand for these special kinds of sand, and natural sources are running low. In 2012 French director Denis Delestrac made a documentary called ""Sand Wars"" about the impact of the lack of construction sand. It shows the ecological and economic effects of both legal and illegal trade in construction sand.[17][18][19]
To retrieve the sand, the method of hydraulic dredging is used. This works by pumping the top few meters of sand out of the water and filling it into a boat, which is then transported back to land for processing. Unfortunately, all marine life mixed in with the extracted sand is killed and the ecosystem can continue to suffer for years after the mining is complete. Not only does this affect marine life, but also the local fishing industries because of the loss of life, and communities living close to the water's edge. When sand is taken out of the water it increases the risk of landslides, which can lead to loss of agricultural land and/or damage to dwellings.[20]
Sand's many uses require a significant dredging industry, raising environmental concerns over fish depletion, landslides, and flooding.[21] Countries such as China, Indonesia, Malaysia, and Cambodia ban sand exports, citing these issues as a major factor.[22] It is estimated that the annual consumption of sand and gravel is 40 billion tons and sand is a US$70 billion global industry.[23]
The global demand for sand in 2017 was 9.55 billion tons as part of a $99.5 billion industry.[24]
While sand is generally non-toxic, sand-using activities such as sandblasting require precautions. Bags of silica sand used for sandblasting now carry labels warning the user to wear respiratory protection to avoid breathing the resulting fine silica dust. Safety data sheets for silica sand state that ""excessive inhalation of crystalline silica is a serious health concern"".[25]
In areas of high pore water pressure, sand and salt water can form quicksand, which is a colloid hydrogel that behaves like a liquid. Quicksand produces a considerable barrier to escape for creatures caught within, who often die from exposure (not from submersion) as a result.
Manufactured sand (M sand) is sand made from rock by artificial processes, usually for construction purposes in cement or concrete. It differs from river sand by being more angular, and has somewhat different properties.[26]
In Dubai, United Arab Emirates, the use of sand has been very demanding in the construction of infrastructure and creating new islands. They used up their own reserves and also imported sand from Australia. There have been three projects to create artificial islands needing more than 835 million tonnes of sand, which cost more than US$26 billion.[27]
"
Singing sand - Wikipedia," Singing sand, also called whistling sand or barking sand, is sand that produces sound. The sound emission may be caused by wind passing over dunes or by walking on the sand.
Certain conditions have to come together to create singing sand:
The most common frequency emitted seems to be close to 450 Hz.
There are various theories about the singing sand mechanism. It has been proposed that the sound frequency is controlled by the shear rate. Others have suggested that the frequency of vibration is related to the thickness of the dry surface layer of sand. The sound waves bounce back and forth between the surface of the dune and the surface of the moist layer, creating a resonance that increases the sound's volume.  The noise may be generated by friction between the grains or by the compression of air between them.[1]
Other sounds that can be emitted by sand have been described as ""roaring"" or ""booming"".
Singing sand dunes, an example of the phenomenon of singing sand, produce a sound described as roaring, booming, squeaking, or the ""Song of Dunes"". This is a natural sound phenomenon of up to 105 decibels, lasting as long as several minutes, that occurs in about 35 desert locations around the world. The sound is similar to a loud low-pitch rumble.  It emanates from crescent-shaped dunes, or barchans. The sound emission accompanies a slumping or avalanching movement of sand, usually triggered by wind passing over the dune or by someone walking near the crest.
Examples of singing sand dunes include California's Kelso Dunes and Eureka Dunes; AuTrain Beach in Northern Michigan; sugar sand beaches and Warren Dunes in southwestern Michigan; Sand Mountain in Nevada; the Booming Dunes in the Namib Desert, Africa; Porth Oer (also known as Whistling Sands) near Aberdaron in Wales; Indiana Dunes in Indiana; Barking Sands in Hawaiʻi; Ming Sha Shan in Dunhuang, China; Kotogahama Beach in Odashi, Japan; Singing Beach in Manchester-by-the-Sea, Massachusetts; near Mesaieed in Qatar; and Gebel Naqous, near el-Tor, South Sinai, Egypt.
The phenomena also inspired a song called ""The Singing Sands of Alamosa"" in Bing Crosby's album Drifting and Dreaming(1947).[2][3]
On some beaches around the world, dry sand will make a singing, squeaking, whistling, or screaming sound if a person scuffs or shuffles their feet with sufficient force.[4][5] The phenomenon is not completely understood scientifically, but it has been found that quartz sand will do this if the grains are very well-rounded and highly spherical.[6] It is believed by some that the sand grains must be of similar size, so the sand must be well sorted by the actions of wind and waves, and that the grains should be close to spherical and have dust-, pollution-, and organic-matter-free surfaces. The ""singing"" sound is then believed to be produced by shear, as each layer of sand grains slides over the layer beneath it. The similarity in size, the uniformity, and the cleanness means that grains move up and down in unison over the layer of grains below them. Even small amounts of pollution on the sand grains reduce the friction enough to silence the sand.[5]
Others believe that the sound is produced by the friction of grain against grain that have been coated with dried salt, in a way that is analogous to the way that the rosin on the bow produces sounds from a violin string.  It has also been speculated that thin layers of gas trapped and released between the grains act as ""percussive cushions"" capable of vibration, and so produce the tones heard.[7]
Not all sands sing, whistle or bark alike. The sounds heard have a wide frequency range that can be different for each patch of sand. Fine sands, where individual grains are barely visible to the naked eye, produce only a poor, weak sounding bark. Medium-sized grains can emit a range of sounds, from a faint squeak or a high-pitched sound, to the best and loudest barks when scuffed enthusiastically.[5]
Water also influences the effect. Wet sands are usually silent because the grains stick together instead of sliding past each other, but small amounts of water can actually raise the pitch of the sounds produced. The most common part of the beach on which to hear singing sand is the dry upper beach above the normal high tide line, but singing has been reported on the lower beach near the low tide line as well.[5]
Singing sand has been reported on 33 beaches in the British Isles,[8] including in the north of Wales and on the little island of Eigg in the Scottish Hebrides. It has also been reported at a number of beaches along North America's Atlantic coast. Singing sands can be found at Souris, on the eastern tip of Prince Edward Island, at the Singing Sands beach in Basin Head Provincial Park;[6] on Singing Beach in Manchester-by-the-Sea, Massachusetts,[4] as well as in the fresh waters of Lake Superior[9] and Lake Michigan[10] and in other places.
"
"Manchester-by-the-Sea, Massachusetts - Wikipedia"," Manchester-by-the-Sea (also known simply as Manchester, its name prior to 1989) is a town on Cape Ann, in Essex County, Massachusetts, United States. The town is known for scenic beaches and vista points. At the 2010 census, the population was 5,136.[1]
Manchester was first settled by English colonists in 1629 and was officially incorporated in 1645. It was formed from territory taken from Salem (that portion since given to Beverly) and Gloucester.
The community thrived primarily as a fishing community for more than 200 years. Beginning in 1845, it started to attract summer residents from the Boston area after poet Richard Dana built a house in the town. Over the next fifty years, development of summer houses along the coastline established the community as Boston society's community of choice for summer residency. The trend continued with designs of houses by architects, such as ""Sunny Waters"", designed by John Hubbard Sturgis for his older brother, Russell, in 1862.
The best known of these ""summer cottages"" was Kragsyde, built on Smith's Point in 1883. Commissioned by George Nixon Black, the Peabody and Stearns-designed residence has been hailed as the zenith of the Shingle style substyle of the Queen Anne style of architecture. It was demolished in 1929.
To prevent confusion with the nearby and much larger city of Manchester, New Hampshire, the name of the town was officially changed in 1989 following a close town meeting vote that year. This was ratified by an act of the state legislature passed on September 25, 1989.[2]
The name change was driven by Edward Corley, a longtime resident of Manchester.[3] All town documents, and the town seal, now use the name ""Manchester-by-the-Sea"". As a result of some minor resident activism, so do the majority of public and private lists of Massachusetts cities and towns, including that of the state government.[citation needed]
According to the United States Census Bureau, the town has a total area of 18.3 square miles (47.3 km2), of which 9.2 square miles (23.9 km2) is land and 9.0 square miles (23.4 km2), or 49.47%, is water.  The town lies along the North Shore of Massachusetts Bay, which in turn leads to the Atlantic Ocean.  There are seven beaches lining the coast, and several small islands dot the coast, the largest being Kettle Island and House Island.
Several small coves edge the coast, the largest being Manchester Harbor, which is fed by Sawmill Brook and other small bodies of water. There are several protected areas within town, including Cedar Swamp Conservation Area, Cheever Commons Conservation Area, Coolidge Reservation, Dexter Pond, Owl's Nest Nature Preservation Land, Powder House Hill Reservation, and Wyman Hill Conservation Area.
Manchester-by-the-Sea is bordered by Beverly and Wenham to the west, Hamilton to the northwest, Essex to the north, and Gloucester to the east. The town is located 9 miles (14 km) northeast of Salem and 24 miles (39 km) northeast of Boston.
Manchester-by-the-Sea lies along Massachusetts Route 128, the inner of two beltways around Greater Boston. Route 128 has two exits within town as it passes from Beverly to Gloucester, with a small portion crossing through the corner of Essex. Route 127 also passes from west to east through town, traveling through the center of town.  There is no bus service directly into town, with service passing east via the Cape Ann Transportation Authority in Gloucester, and an MBTA bus route providing service to Beverly.
The town is served by a stop along the Newburyport/Rockport Line of the MBTA Commuter Rail, providing service from Rockport along the North Shore to Boston's North Station.  The nearest airport is the Beverly Municipal Airport, with the nearest national and international service at Boston's Logan International Airport.
As of the census of 2010,[14] there were 5,136 people, 2,147 households, and 1,444 families residing in the town.  The population density was 562.7 people per square mile (217.3/km2). The racial makeup of the town was 97.6% (5,012) White, 0.1% (5) African American, 0.2% (10) Native American, 0.9% (46) Asian, 0.1% from other races, and 1.1% from two or more races. Hispanic or Latino of any race were 1.5% (77) of the population. The median income for a household in the town was $95,243, and the median income for a family was $109,760. About 4.5% of families and 5.1% of the population were below the poverty line, including 3.9% of those under age 18 and 3.7% of those age 65 or over.[14]
The local newspaper, The Manchester Cricket, is published weekly. Within the Cricket, there is a special section dedicated to the neighboring town, Essex. This section is called The Essex Echo. The town is also served by a regional newspaper, the Gloucester Daily Times.
One mile from the town center is Singing Beach, so named because the sand comprising the beach squeaks when walked upon. The sand is an iridescent color when the sun sets.[15] This beach is quite popular during summer months in particular, because it is easily accessible from Boston by a half-mile walk from the MBTA train station. Also located on this historic beach is the famous tourist attraction ""Eaglehead"", a rock composite that is the focal point of rock climbing and other recreation activities.
Kragsyde Mansion (1885, demolished 1929)
Town Hall
Old Burial Ground
The town provided the backdrop for these films:
It was also featured in a season of the TV series This Old House, and was featured in a ""Main Streets and Back Roads"" episode of Chronicle, a newsmagazine program in New England.
"
"Sitka, Alaska - Wikipedia"," 
The City and Borough of Sitka (Tlingit: Sheetʼká, Russian: Ситка), formerly Novo-Arkhangelsk (or New Archangel) under Russian rule (Russian: Ново-Архангельск or Новоaрхангельск, t Novoarkhangelsk), is a unified city-borough in the southeast portion of the U.S. state of Alaska.  The city is situated on the west side of Baranof Island and the south half of Chichagof Island in the Alexander Archipelago of the Pacific Ocean (part of the Alaska Panhandle).  As of the 2010 census, Sitka had a population of 8,881.[5]
With a consolidated land area of 2,870.3 square miles (7,434 square kilometres) and total area (including water) of 4,811.4 square miles (12,461 square kilometres), Sitka is the largest city-borough by total area in the U.S.
The current name Sitka (derived from Sheet’ká, a contraction of the Tlingit Shee At'iká)[6] means ""People on the Outside of Baranof Island,"" whose Tlingit name is Sheet’-ká X'áat'l (here contracted to Shee).
The area now known as the downtown area of Sitka was originally settled by the Tlingit people over 10,000 years ago.[citation needed]
Russian explorers settled Old Sitka in 1799, naming it Fort of Archangel Michael (Russian: форт Архангела Михаила, t Fort Arkhangela Mikhaila). The governor of Russian America, Alexander Baranov, arrived under the auspices of the Russian-American Company, a colonial trading company chartered by Tsar Paul I. In June 1802, Tlingit warriors destroyed the original settlement, killing many of the Russians, with only a few managing to escape.[7]:37–39 Baranov was forced to levy 10,000 rubles in ransom for the safe return of the surviving settlers.[8]
Baranov returned to Sitka in August 1804 with a large force, including Yuri Lisyansky's Neva. The ship bombarded the Tlingit fortification on the 20th, but was not able to cause significant damage. The Russians then launched an attack on the fort and were repelled. Following two days of bombardment, the Tlingit ""hung out a white flag"" on the 22nd, deserting the fort on the 26th.[7]:44–49
Following their victory at the Battle of Sitka, the Russians established the settlement ""New Archangel"", named after Arkhangelsk.  As a permanent settlement, New Archangel became the largest city in the region. The Tlingit re-established their fort on the Chatham Strait side of Peril Strait to enforce a trade embargo with the Russian establishment. In 1808, with Baranov still governor, Sitka was designated the capital of Russian America.[citation needed]
Bishop Innocent lived in Sitka after 1840. He was known for his interest in education, and his house, parts of which served as a schoolhouse, the Russian Bishop's House has since been restored by the National Park Service as part of the Sitka National Historical Park.
The original Cathedral of Saint Michael was built in Sitka in 1848 and became the seat of the Russian Orthodox bishop of Kamchatka, the Kurile and Aleutian Islands, and Alaska. The original church burned to the ground in 1966.  Although the church was restored to its original appearance, one exception was its clock face which is black in photographs taken before 1966, but white in subsequent photos.[citation needed]
Swedes, Finns and other nationalities of Lutherans worked for the Russian-American Company,[9] which led to the creation of a Lutheran congregation.  The Sitka Lutheran Church building was built in 1840 and was  the first Protestant church on the Pacific coast. After the transition to American control, following the purchase of Alaska from Russia by the United States in 1867, the influence of other Protestant religions increased, and Saint-Peter's-by-the-Sea Episcopal Church was consecrated as ""the Cathedral of Alaska"" in 1900.[10]
Sitka was the site of the transfer ceremony for the Alaska purchase on October 18, 1867. Russia was going through economic and political turmoil after it lost the Crimean War to Britain, France, and the Ottoman Empire in 1856 and decided it wanted to sell Alaska before it was taken over by Britain. Russia offered to sell it to the United States. Secretary of State William Seward had wanted to purchase Alaska for quite some time as he saw it as an integral part of Manifest Destiny and America's reach to the Pacific Ocean.[11] While the agreement to purchase Alaska was made in April 1867, the actual purchase and transfer of control took place on October 18, 1867. The cost to purchase Alaska was $7.2 million, 2 cents per acre.
Sitka served as the U.S. Government Capital of the Department of Alaska (1867–1884) and District of Alaska (1884–1906).
The seat of government was relocated north to Juneau in 1906 due to declining economic importance of Sitka relative to Juneau, which gained population in the Klondike Gold Rush.
The Alaska Native Brotherhood was founded in Sitka in 1912 to address racism against Alaska Native people in Alaska.[12] By 1914 the organization had constructed the Alaska Native Brotherhood Hall on Katlian Street, which was named after a Tlingit war chief in the early period of Russian colonization.[13]
In 1937, the United States Navy established the first seaplane base in Alaska on Japonski Island.[14] In 1941, construction began on Fort Ray, an army garrison to protect the Naval air station.[14] Both the Army and Navy remained in Sitka until the end of WWII, when the Army base was put into caretaker status. The naval station in Sitka was deactivated in June 1944.[14]
The Alaska Pulp Corporation was the first Japanese investment in the United States after WWII. In 1959 it began to produce pulp harvested from the Tongass National Forest under a 50-year contract with the US Forest Service.[15] At its peak, the mill employed around 450 people before closing in 1993.
Sitka's Filipino community established itself in Sitka before 1929. It later became institutionalized as the Filipino Community of Sitka in 1981.[16]
Gold mining and fish canning paved the way for the town's initial growth. Today Sitka encompasses portions of Baranof Island and the smaller Japonski Island (across the Sitka Channel from the town), which is connected to Baranof Island by the O'Connell Bridge. The John O'Connell Bridge was the first cable-stayed bridge built in the Western Hemisphere. Japonski Island is home to Sitka Rocky Gutierrez Airport (IATA: SIT; ICAO: PASI), the Sitka branch campus of the University of Alaska Southeast, Mt. Edgecumbe High School (a state-run boarding school for rural Alaskans), Southeast Alaska Regional Health Consortium's Mt. Edgecumbe Hospital, a U.S. Coast Guard Air Station Sitka, and the port and facilities for the USCGC Maple.[17]
According to the United States Census Bureau, the borough is the largest incorporated city by area in the U.S., with a total area of 4,811 square miles (12,460.4 km2), of which 2,870 square miles (7,400 km2) is land and 1,941 square miles (5,030 km2) (40.3%) is water.  As a comparison, this is almost four times the size of the state of Rhode Island.
Sitka displaced Juneau, Alaska as the largest incorporated city by area in the United States upon the 2000 incorporation with 2,874 square miles (7,440 km2) of incorporated area. Juneau's incorporated area is 2,717 square miles (7,040 km2). Jacksonville, Florida, is the largest city in area in the contiguous 48 states at 758 square miles (1,960 km2).
Sitka has an oceanic climate (Köppen Cfb) with moderate, but generally cool, temperatures and abundant precipitation. The average annual precipitation is 131.74 inches (3,350 mm); average seasonal snowfall is 33 inches (84 cm), falling on 233 and 19 days respectively. The mean annual temperature is 45.3 °F (7.4 °C), with monthly means ranging from 36.4 °F (2.4 °C) in January to 57.2 °F (14.0 °C) in August.
The climate is relatively mild when compared to other parts of the state. Only 5.1 days per year see highs at or above 70 °F (21 °C); conversely, there are only 10 days with the high not exceeding freezing.[18] The winters are extremely mild compared to inland areas of similar and much more southerly parallels due to the intense maritime moderation. The relatively mild nights ensure that four months stay above the 50 °F (10 °C) isotherm that normally separates inland areas from being boreal in nature. Due to the mild winter nights, plant hardiness is low for the latitude.
The highest temperature ever recorded was 88 °F (31.1 °C) on July 30, 1976 and July 31, 2020. The lowest temperature ever recorded was −1 °F (−18.3 °C) on February 16–17, 1948.[18]
Mount Edgecumbe, a 3,200-foot (980 m) dormant stratovolcano, is located on southern Kruzof Island. It can be seen from Sitka on a clear day.
Sitka first reported on the 1880 U.S. Census as an unincorporated village. Of 916 residents, there were 540 Tlingit, 219 Creole (Mixed Russian & Native) and 157 Whites reported.[20] It was the largest community in Alaska at that census. In 1890, it fell to second place behind Juneau. It reported 1,190 residents, of whom 861 were Native, 280 were White, 31 were Asian, 17 Creole, and 1 Other.[21] In 1900, it fell to 4th place behind Nome, Skagway & Juneau. It did not report a racial breakdown.[22]
In 1910, Sitka was reported as two separate communities based on race: the village with mostly non-natives (population 539) and the part of the village with natives (population 500).[23] Separately, they placed as the 15th and 17th largest communities. United, they would be 8th largest. For the purposes of comparison and the fact that the village was not officially politically/racially divided except by the census bureau report, the combined total (1,039) is reported on the historic population list. In 1913, Sitka was incorporated as a city, rendering the division by the census bureau for 1910 moot. In 1920, Sitka became the 4th largest city in the territory.[24] In 1930, it fell to 7th place with 1,056 residents. Of those, 567 reported as Native, 480 as White and 9 as Other.[25] In 1940, it rose to 5th place, but did not report a racial breakdown.[26]
In 1950, it reported as the 9th largest community in Alaska (6th largest incorporated city).[27] It did not report a racial breakdown. At statehood in 1960, it became the 6th largest community (5th largest incorporated city). With the annexations increasing its population to 3,237, it reported a White majority for its first time: 2,160 Whites, 1,054 Others (including Natives) & 23 Blacks.[28] In 1970, it fell to 14th place overall (though 7th largest incorporated city) with 3,370 residents. Of those, 2,503 were White, 676 Native Americans, 95 Others, 74 Asians and 22 Blacks.[29] In 1980, Sitka rose to 4th largest city with 7,803 residents (of whom 5,718 were non-Hispanic White, 1,669 were Native American, 228 were Asian, 108 were Hispanic (of any race), 87 were Other, 44 were Black & 7 were Pacific Islander).[30]
In 1990, Sitka fell to 5th largest (4th largest incorporated) with 8,588 residents. 6,270 were non-Hispanic White; 1,797 were Native American; 315 were Asian; 209 were Hispanic (of any race); 60 were Other; 39 were Black and 18 Pacific Islanders.[31] In 2000, Sitka retained its 5th largest (and 4th largest incorporated) position. In 2010, it slipped to 7th largest community overall (but still remained the 4th largest incorporated city).
As of the 2010 census, there were 8,881 people living in the borough. The racial makeup of the borough, based on one race alone or in combination with one or more other races, was, 64.6% White (including White Hispanic and Latino Americans), 1% Black or African American, 24.6% Native American, 8.1% Asian, 0.9% Pacific Islander, 1.8% from other races. In addition, 4.9% of the population were Hispanic and Latino Americans of any race.
There were 3,545 households, out of which 29.5% had children under the age of 18 living with them, 45.5% were married couples living together, 10.7% had a female householder with no husband present, 6.1% had a male householder with no wife present, and 37.6% were non-families. The average household size was 2.43 and the average family size was 3.01.[32]
In 2010, Sitka's two largest employers were the South East Alaska Regional Health Consortium (SEARHC), employing 482 people, and the Sitka School District which employs 250 people. However, there are more people employed in the seafood industry than in any other sector. An estimated 18% of Sitka's population earns at least a portion of their income from fishing and seafood harvesting and processing. Many Sitkans hunt and gather subsistence foods such as fish, deer, berries, seaweeds and mushrooms for personal use.[33]
Within the total 2010 population of 8,881 residents, an estimated 7,161 were over 16 years of age. Of residents aged 16 and over, an estimated 4,692 were employed within the civilian labor force, 348 were unemployed (looking for work), 192 were employed in the armed forces (U.S. Coast Guard), and 1,929 were not in the labor force. The average unemployment rate between 2006 and 2010 was 6.9%. The median household income in 2010 inflation adjusted dollars was $62,024. An estimated 4.3% of all families / 7% of all residents had incomes below the poverty level ""in the past twelve months""(2010).[34]
Sitka's electrical power is generated by dams at Blue Lake and Green Lake, with supplemental power provided by burning diesel when electric demand exceeds hydro capacity. In December 2012 the Blue Lake Expansion project began, which added 27 percent more electricity for the residents of Sitka. The project was completed in November 2014.[35]
Sitka is the 6th largest port by value of seafood harvest in the United States.[33] International trade is relatively minor, with total exports and imports valued at $474,000 and $146,000, respectively, in 2005 by the American Association of Port Authorities.[36] The port has the largest harbor system in Alaska with 1,347 permanent slips.
During Russian rule, Sitka was a busy seaport on the west coast of North America,[37] mentioned a number of times by Dana in his popular account of an 1834 sailing voyage Two Years Before the Mast. After the transfer of Alaska to U.S. rule, the Pacific Coast Steamship Company began tourist cruises to Sitka in 1884. By 1890, Sitka was receiving 5,000 tourist passengers a year.[38]
Old Sitka Dock,[39] located at Halibut Point, one mile south of the Old Sitka State Historical Park, commemorating the 1800s Russian settlement, and six miles north of downtown Sitka, is a private deep water port offering moorage facilities.[40]  A 470-foot-long floating dock for vessels up to 1100 feet was constructed there by its owners in 2012 and was first used in 2013.[41] In Spring 2016, Holland America Line agreed to dock its ships at the Old Sitka Dock.[42]  Since then, the majority of the cruise ships calling on Sitka berth at the Old Sitka Dock, with the remainder anchoring offshore in Crescent Harbor and tendering their passengers to downtown Sitka.  In the 2017 season, there were 136 cruise ship calls at Sitka with more than 150,000 passengers in total; of these fewer than 30,000 were tendered.[43]
The United States Coast Guard plans to homeport one of its Sentinel class cutters in Sitka.[44]
There are 22 buildings and sites in Sitka that appear in the National Register of Historic Places.[45]
On October 18, Alaska celebrates Alaska Day to commemorate the Alaska purchase. The City of Sitka holds an annual Alaska Day Festival. This week-long event includes a reenactment ceremony of the signing of the Alaska purchase, as well as interpretive programs at museums and parks, special exhibits, aircraft displays and film showings, receptions, historic sites and buildings tours, food, prose writing contest essays, Native and other dancing, and entertainment and more. The first recorded Alaska Day Festival was held in 1949.[46]
The City and Borough of Sitka is a Unified Home Rule[47][48] city.
The home rule charter of the City and Borough of Sitka was adopted on December 2, 1971[49] for the region of the Greater Sitka Borough, which included Mt. Edgecumbe on Japonski Island and Port Alexander and Baranof Warm Springs on Baranof Island. The city was incorporated on September 24, 1963.[50] On October 23, 1973, the city of Port Alexander was detached from the borough.[51]
Sitka hosts one active post-secondary institution, the University of Alaska Southeast-Sitka Campus, located on Japonski Island in an old World War II hangar. Sheldon Jackson College, a small Presbyterian-affiliated private college, suspended operations in June 2007, after several years of financial stress.[52] Outer Coast College, a private liberal arts college established in 2015, is currently in development as an undergraduate institution founded on the former campus of Sheldon Jackson College.
The Sitka School District runs several schools in Sitka, including Sitka High School and Pacific High School, as well as the town's only middle school, Blatchley Middle School. They also run a home school assistance program through Terry's Learning Center.
Mt. Edgecumbe High School, a State of Alaska-run boarding high school for rural, primarily Native, students, is located on Japonski Island adjacent to University of Alaska Southeast.
Two private schools are available in Sitka: Sitka Adventist School,[53] and The SEER School.[54]
The Alaska State Trooper Academy — the academy for all Alaska State Troopers — is located in Sitka.
Sitka Public Library, formerly Kettleson Memorial Library is the public library for Sitka. It receives about 100,000 guests annually and houses a collection of 75,000 books, audiobooks, music recordings, reference resources, videos (DVD and VHS) as well as an assortment of Alaskan and national periodicals. Its annual circulation is 133,000. The library is well known by visitors for its view. The large windows in front of the reading area look south across Eastern Channel towards the Pyramids.
Until its closing, Sitka was also home to Stratton Library, the academic library of Sheldon Jackson College.[55]
Sitka is served by the Daily Sitka Sentinel, one of the few remaining independently owned daily newspapers in the state. Sitka also receives circulation of the Capital City Weekly — a weekly regional newspaper based out of Juneau.
Alaska's first newspaper following the Alaska purchase, the Sitka Times, was published by Barney O. Ragan on September 19, 1868. Only four issues were published that year, as Ragan cited a lack of resources available at the time. The paper resumed publishing the following year as the Alaska Times. In 1870, it moved to Seattle, where the year following it was renamed the Seattle Times (not to be confused with the modern-day newspaper of the same name).[56]
Sitka has three radio stations, public radio station KCAW (Raven Radio), and commercial radio stations KIFW, and KSBZ.
KTNL-TV (CBS) broadcasts out of Sitka on Channel 13 (Cable 6) serving Southeast Alaska. Additionally, KSCT-LP (NBC) Channel 5, KTOO (PBS) Channel 10,[57] and KJUD (ABC/CW) serve the region. There was a previous NBC affiliate in the Region, KSA-TV, available to cable systems, which is now defunct.
Sitka is only accessible by boat or plane as it is a series of islands in the Pacific Ocean. Vehicles are usually brought to Sitka via the Alaska Marine Highway ferry system. However, a vehicle is not an absolute necessity in Sitka, as there are only 14 miles (23 kilometres) of road from one end of the island to another. Most everything is within walking distance from the downtown area which is where the majority of employers are situated. Public transportation is also available.
By air, Sitka Rocky Gutierrez Airport offers scheduled passenger jet service operated year-round by Alaska Airlines and seasonally by Delta Connection as well as commuter, charter, and bush air service provided by Harris Aircraft Services.  Harris Air provides scheduled service to several smaller communities in the southeast as well as to Juneau.
Delays in fall and winter due to Sitka's weather are frequent. The airport is located on Japonski Island, which is connected to Baranof Island by the O'Connell Bridge. The O'Connell Bridge, completed in 1972, was the first vehicular cable-stayed bridge in the United States. The Sitka Seaplane Base is seaplane landing area situated in the Sitka Channel, adjacent to the airport.
Ferry travel back and forth to Juneau, Ketchikan and other towns in Southeast Alaska is provided through the Alaska Marine Highway System. The ferry terminal is located 7 miles (11 km) north of downtown and a ferry ticket costs about $49 per person each way to Juneau (as of July 2013). Vehicles, pets and bicycles can also be taken on the ferry for an additional charge.
Sitka's location on the outer coast of the Alaska Panhandle is removed from routes run through Chatham Strait. The tides of Peril Strait allow mainline vessels through only at slack tide.[citation needed]
Alaska Marine Lines, a barge and freight company, has the ability to move cars to other communities connected to the mainland by road systems.
A three-way partnership of non-profits (Center for Community, Sitka Tribe of Alaska, and Southeast Senior Services) offers public bus transit, funded by the Federal Transit Administration and the Alaska Department of Transportation. All buses are fully accessible, with service from 6:30 a.m. to 7:30 p.m., Monday through Friday.
In 2008, the League of American Bicyclists awarded Sitka the bronze level in bicycle friendliness making Sitka the first bicycle-friendly community in Alaska. In 2013, the Walk Friendly Communities[58] program awarded Sitka with a bronze award, making Sitka the first Alaska community with a Walk Friendly Communities designation. Sitka is the only Alaska community to have both a Bicycle Friendly Community and a Walk Friendly Communities designation.
There are no longer two hospitals in Sitka:
Sitka has the following sister city:[60]
Sitka's attractions include:
The flora and fauna of Sitka and its surrounding area are popular. Day cruises and guided day trips (hiking) are large enterprises in Sitka. Floatplane ""flightseeing"" excursions are a way to view the area's sights from above.
Sitka's position between the Pacific Ocean and the most mountainous island in the Alexander Archipelago creates a variety of outdoor opportunities:
"
North American Numbering Plan - Wikipedia," The North American Numbering Plan (NANP) is a telephone numbering plan for World Zone 1, which comprises twenty-five distinct regions in twenty countries primarily in North America, including the Caribbean. Some North American countries, most notably Mexico, do not participate in the NANP.
The NANP was originally devised in the 1940s by AT&T for the Bell System and independent telephone operators in North America to unify the diverse local numbering plans that had been established in the preceding decades. AT&T continued to administer the numbering plan until the breakup of the Bell System, when administration was delegated to the North American Numbering Plan Administrator (NANPA), a service that has been procured from the private sector by the Federal Communications Commission (FCC) in the United States. Each participating country forms a regulatory authority that has plenary control over local numbering resources.[1] The FCC also serves as the U.S. regulator. Canadian numbering decisions are made by the Canadian Numbering Administration Consortium.[2]
The NANP divides the territories of its members into numbering plan areas (NPAs) which are encoded numerically with a three-digit telephone number prefix, commonly called the area code.[3] Each telephone is assigned a seven-digit telephone number unique only within its respective plan area. The telephone number consists of a three-digit central office code and a four-digit station number. The combination of an area code and the telephone number serves as a destination routing address in the public switched telephone network (PSTN). For international call routing, the NANP has been assigned the international calling code 1 by the International Telecommunications Union (ITU). The North American Numbering Plan conforms with ITU Recommendation E.164, which establishes an international numbering framework.[4]
From its beginnings in 1876 and throughout the first part of the 20th century, the Bell System grew from essentially local or regional telephone systems. These systems expanded by growing their subscriber bases, as well as increasing their service areas by implementing additional local exchanges that were interconnected with tie trunks. It was the responsibility of each local administration to design telephone numbering plans that accommodated the local requirements and growth.[5] As a result, the Bell System as a whole developed into an unorganized system of many differing local numbering systems. The diversity impeded the efficient operation and interconnection of exchanges into a nationwide system for long-distance telephone communication. By the 1940s, the Bell System set out to unify the various numbering plans in existence to provide a unified, systematic approach to route telephone calls across the nation and provide efficient long-distance service that eventually did not require the involvement of switchboard operators.
A new nationwide numbering plan was officially published in October 1947 in cooperation of the Bell System with the independent telephone operators. The plan divided most of North America into eighty-six numbering plan areas (NPAs). Each NPA was assigned a unique three-digit code, typically called NPA code or simply area code. These codes were first used in Operator Toll Dialing by long-distance operators in establishing calls between toll offices. The first customer-dialed direct call using area codes was made on November 10, 1951, from Englewood, New Jersey, to Alameda, California.[6] Direct distance dialing (DDD) was subsequently introduced across the country. By the early 1960s, most areas of the Bell System had been converted and DDD had become commonplace in cities and most towns in the United States and Canada. By 1967, the number of assigned area codes had grown to 129.[7]
The status of the network of the 1960s was reflected in a new name used in technical documentation: North American Integrated Network.[7] By 1975, the numbering plan was referred to as the North American Numbering Plan,[8] leading to the well-known acronym NANP, as other countries sought or considered joining in the standardization.
Although Bermuda and the Caribbean islands had been assigned the area code 809 as early as 1958 by the administrators at AT&T, individual participating countries or territories had no autonomy over their numbering plan as they received centrally assigned central office prefixes that needed to be unique from those of other countries with the same area code. Regions in Mexico with high call volumes to and from the US were assigned functional area codes as early as 1963, for the purpose of call routing, but a nationwide system of participation in the NANP eventually failed.
In the following decades, the NANP expanded to include all of the United States and its territories, Canada, Bermuda, and seventeen nations of the Caribbean.[9][10]
At the request of the British Colonial Office, the numbering plan was first expanded to Bermuda and the British West Indies because of their historic telecommunications administration through Canada as parts of the British Empire and their continued associations with Canada, especially during the years of the telegraph and the All Red Line system.
Not all North American polities participate in the NANP. Exceptions include Mexico, Greenland, Saint Pierre and Miquelon, the Central American countries and some Caribbean countries (Cuba, Haiti, the French Caribbean and the Dutch Caribbean, except for Sint Maarten, which uses the 721 area code). The only Spanish-speaking state in the system is the Dominican Republic. Mexican participation was planned,[11] but implementation stopped after three area codes (706, 903 and 905) had been assigned, and Mexico opted for an international numbering format, using country code 52.[12] The area codes in use were subsequently withdrawn in 1991. Area code 905, formerly used for Mexico City, was reassigned to a split of area code 416 in the Greater Toronto Area; area code 706, which had formerly served Mexico's Baja Peninsula, was reassigned to a portion of northern Georgia[13] surrounding the Atlanta region, which retained 404; and area code 903, which served a small portion of northern Mexico, was reassigned to northeastern Texas when it split from area code 214.
The Dutch Caribbean territory of Sint Maarten joined the NANP in September 2011, receiving area code 721.[14] Sint Maarten shares the island with the French Collectivity of Saint Martin which, like the rest of the French Caribbean, is not part of the NANP.
The NANP is administered by the North American Numbering Plan Administrator (NANPA, formerly Administration).[15] This function is overseen by the Federal Communications Commission, which assumed the responsibility upon the breakup of the Bell System. The FCC solicits private sector contracts for the role of the administrator.
Before the breakup of the Bell System, administration of the North American Numbering Plan was performed by AT&T's Central Services Organization. In 1984, this function was transferred to Bell Communications Research (Bellcore), a company created by the divestiture mandate to perform services for the newly created local exchange carriers. On January 19, 1998, the NANPA function was transferred to the IMS division of Lockheed Martin in Washington, D.C.[16] In 1999, the contract was awarded to Neustar, a company spun off from Lockheed for this purpose. In 2004, and again in 2012, the contract was renewed.[17] On January 1, 2019, Somos assumed the NANPA function under a one-year bridge contract granted by the FCC with the goal of consolidating the NANPA function with the Pooling Administrator and identifying a long-term contract holder.[18][19]
The vision and goal of the architects of the North American Numbering Plan was a system by which telephone subscribers in the United States and Canada could themselves dial and establish a telephone call to any other subscriber without the assistance of switchboard operators. While this required an expansion of most existing local numbering plans, many of which required only four or five digits to be dialed, or even fewer in small communities, the plan was designed to enable local telephone companies to make as few changes as possible in their systems.

The new numbering plan divided the North American continent into regional service areas, called numbering plan areas (NPAs), primarily following the jurisdictional boundaries of U.S. states and Canadian provinces.[20] States or provinces could be divided into multiple areas. NPAs were created in accordance with principles deemed to maximize customer understanding and minimize dialing effort while reducing plant cost.[21] Each NPA was identified by a unique three-digit code number, that was prefixed to the local telephone number. If a call's destination was within the same numbering plan area, dialing the area code was not necessary.
Existing telephone exchanges and central offices became local exchange points in the nationwide system, each of which was also assigned a three-digit number, unique within its NPA. The combination of NPA code and central office code served as a destination routing code for use by operators and subscribers to reach any central office through the switching network.[20] Due to the structure of the numbering plan, each NPA was technically limited to 540 central offices.[21]
Although the limitation to 540 central offices required the most populous states to be divided into multiple NPAs, it was not the sole reason to subdivide a state. An important aspect was the existing infrastructure for call routing, which had developed in preceding decades independently of state boundaries. Divisions also attempted to avoid cutting across busy toll traffic routes, so that most toll traffic remained within an area, and outgoing traffic in one area would not be tributary to toll offices in an adjacent area.[22][20] As are result, New York state was initially divided into five areas, the most of any state. Illinois, Ohio, Pennsylvania, and Texas were assigned four NPAs each, and California, Iowa, and Michigan received three. Eight states and provinces were split into two NPAs.
Traditionally, central office switching systems were designed to serve up to ten thousand subscriber numbers. Thus, subscribers were assigned four-digit line or station numbers. This rounded out the total number of digits in a subscriber telephone number to ten: a three-digit area code, three-digit central office code, and four digits for each line. This defined the North American Numbering Plan as a closed numbering plan,[23] as opposed to developments in some other countries where the number of digits was not fixed.
However, the closed numbering plan did not require the subscriber to dial all digits. When making a local call or a call within their numbering plan area, the area code was omitted, resulting in seven-digit dialing. Ten-digit dialing was only necessary when placing foreign area calls to subscribers in another state or numbering plan area.[24] Exceptions existed for communities located on NPA boundaries, so that uniform local dialing was still possible in historically established communities.
In 1947, AT&T completed a new design for a nationwide toll network that established the original North American area codes. The new organization provided for 152 area codes, each with a capacity to serve up to 540 central offices.[25] Originally, only eighty-six area codes were assigned. New Jersey received the first NPA code in the new system, area code 201.[26] The second area code, 202, was assigned to the District of Columbia. The allocation of area codes was readjusted as early as 1948 to account for inadequacies in some metropolitan areas. For example, the Indiana numbering plan area 317 was divided to provide a larger numbering pool in the Indiana suburbs of Chicago (area code 219).
Initially, states divided into multiple numbering plan areas were assigned area codes with the digit 1 in the second position, while areas that comprised entire states or provinces received codes with 0 as the middle digit. This rule was broken by the early 1950s,[20] as NPAs with digit 0 in the middle had to be split, but until 1995 all area codes assigned had none other than the digits 0 and 1 in this position.
The eight codes of the form N11 (N = 2–9) were reserved as service codes. The easily recognizable codes of the form N00 were available in the numbering plan, but were not initially included in assignments.[7] Additional area code patterns were later assigned for other services; for example, the area codes N10 were implemented for the Teletypewriter Exchange Service (TWX).[27]
It was already common practice for decades that the digits 0 and 1 could not appear in the first two digits of the central office codes, because the system of using the first two letters of familiar names for central offices did not assign letters to these digits. The digit 0 was used for operator assistance, and 1, which is essentially a single pulse of loop interruption, was automatically ignored by most switching equipment of the time.[20] Therefore, the 0/1 rule for the area code provided a convenient means to distinguish seven-digit dialing from ten-digit dialing.
The use of telephone exchange names as part of telephone numbers had been a well-established practice, and this was preserved for convenience and expediency in the new network design. The digit-to-letter translations were printed on the face of every rotary dial in the metropolitan areas, according to the scheme designed by W.G. Blauvelt in 1917, and had been used in the Bell System in large metropolitan areas since the early 1920s.[28] The network reorganization standardized this system to using a two-letter, five-digit (2L-5N) representation of telephone numbers in most exchanges in North America,[29] or to using an equivalent all-numeric seven-digit numbering plans, as was practiced by some telephone companies.
The original plan of 1947 had been projected to be usable beyond the year 2000. However, by the late 1950s it became apparent that it would be outgrown by about 1975.[30] The limitations for the usable leading digits of central office code, imposed by using common names for central office names, and their leading two characters as guides for customer dialing could no longer be maintained when opening new central offices. By 1962 it was forecast that in 1985 the number of telephones in the nation would equal its population of 280 million and increase to 600 million telephones for 340 million people in 2000.[29] As a result, the North American telephone administrations first introduced letter combinations that could not be linked to a familiar pronounceable central office name. Finally, they sought the removal of the memorable central office names and the introduction of all-number calling (ANC).
Under all-number calling, the number of central office prefixes increased from 540 to potentially 800, but the first two digits of the central office code were still restricted to the range 2 to 9, and the eight combinations that ended in 11 were reserved as special calling codes.[29] This increased the numbering pool for central office codes to 640, and resulted in the partitioning of the prefix space (000—999) according to the table at the right.[31]
As the numbering plan grew in the 1960s under all-number calling, plan administrators at AT&T identified that by c. 1973 some of the largest area codes in urban centers might run out of central office prefixes to install more individual access lines. For relief in these cases, they finally removed the requirement that the middle digit of the central office code could not be 0 or 1. This resulted in the format of interchangeable central office codes, N X X, where N=2–9, and X is any digit. The first cities that required this action in 1974, were the city of Los Angeles with area code 213, and New York City (212). This change also required modification of the local dialing procedures to distinguish local call from long-distance calls with area code.
Requiring 1 to be dialed before the full number in some areas provided for area codes of the form N10, such as 210 in the San Antonio, Texas, area and 410 in eastern Maryland. Therefore, someone calling from San Jose, California, to Los Angeles before the change would have dialed 213-555-0123 and after the change 1-213-555-0123, which permitted the use of 213 as an exchange prefix in the San Jose area. The preceding 1 also ideally indicates a toll call; however, this is inconsistent across the NANP because the FCC has left it to the U.S. state public utilities commissions to regulate for traditional landlines, and it has since become moot for mobile phones and digital VoIP services that now offer nationwide calling without the extra digit.[citation needed]
In 1995, the North American Numbering Plan Administrator removed the requirement that the middle digit of an area code had to be either 0 or 1, implementing fully interchangeable NPA and central office codes, that had already been anticipated since the 1960s, when interchangeable central office codes were sanctioned.
The NANP numbering format may be summarized in the notation NPA-NXX-XXXX:
For example, 234-235-5678 is a valid telephone number with area code 234, central office prefix (exchange) 235, and line number 5678. The number 234-911-5678 is invalid, because the central office code must not be in the form N11. 314-159-2653 is invalid, because the office code must not begin with 1. 123-234-5678 is invalid, because the NPA must not begin with 0 or 1.[citation needed]
Each three-digit area code has a capacity of 7,919,900 telephone numbers.
Using 0 or 1 as the first digit of an area code or seven-digit local number is invalid, as is a 9 as the middle digit of an area code; these are trunk prefixes or reserved for North American Numbering Plan expansion.
The country calling code for all countries participating in the NANP is 1. In international format, an NANP telephone number is listed as +19995550100, where 999 stands in for the area code.
While the national numbering plan of the NANP was designed as a 10-digit closed plan, international direct distance dialing (IDDD) was accomplished by extensive modifications in switching systems to accommodate an open international numbering plan for telephone numbers from seven to twelve digits.[33]
Canada and the United States have experienced rapid growth in the number of area codes, particularly between 1990 and 2005. The widespread adoption of fax, modem, and mobile phone communication, as well as the deregulation of local telecommunication services in the United States in the mid-1990s, increased the demand for telephone numbers.
The Federal Communications Commission allowed telecommunication companies to compete with the incumbent local exchange carriers for services, usually by forcing the existing sole service provider to lease infrastructure to other local providers. Because of the original design of the numbering plan and the telephone switching network that assumed only a single provider, number allocations had to be made in 10,000-number blocks even when much fewer numbers were required for each new vendor. Due to the proliferation of service providers in some numbering plan areas, many area codes fell into jeopardy, facing exhaustion of numbering resources. The number blocks of failed service providers often remained unused, as no regulatory mechanism existed to reclaim and reassign these numbers.
Area codes are added by two principal methods, number plan area splits and overlay plans. Splits were implemented by dividing an area into two or more regions, one of which retained the existing area code and the other areas receiving a new code. In an overlay, multiple codes are assigned to the same geographical area, obviating the need for renumbering of existing services. Subtle variations of these techniques have been used as well, such as dedicated overlays, in which the new code is reserved for a particular type of service, such as cellular phones and pagers, and concentrated overlays, in which a part of the area retained a single code while the rest of the region received an overlay code. The only service-specific overlay in the NANP was area code 917 (New York City) when it was first installed; such service-specific area code assignments were later prohibited by the Federal Communications Commission.
Most area codes of the form N10, originally reserved for AT&T's Teletypewriter eXchange (TWX) service, were transferred to Western Union in 1969 and were freed up for other use in 1981 after conversion to Telex II service was complete. The last of these, 610, was assigned to Canada, but reassigned in 1992. These new area codes, as well as a few other codes used for routing calls to Mexico, were used for telephone area code splits in the late 1980s and early 1990s, as all other area codes under the original plan had been consumed.
After the remaining valid area codes were used up by expansion, in 1995 the rapid increase in the need for more area codes forced the NANPA to allow the digits 2 through 8 to be used as a middle digit in new area code assignments, with 9 being reserved as a last resort for potential future expansion. At the same time, local exchanges were allowed to use 1 or 0 as a middle digit. The first area codes without a 1 or 0 as the middle digit were area code 334 in Alabama and area code 360 in Washington, which both began service on January 15, 1995. This was quickly followed by area code 520 serving Arizona on March 19, 1995.
Codes ending in double digits are reserved as easily recognizable codes (ERCs), to be used for special purposes such as toll-free numbers, personal 500 numbers, Canadian non-geographic area code 600, carrier-specific 700 numbers, and high-toll 900 numbers, rather than for geographic areas. Nevada was denied 777 (""lucky 7s"", a reference to the state's legalized gambling) for this reason;[34] it received 775 instead when most of Nevada split from 702, which continues to serve the Las Vegas metropolitan area.
By 1995, many cities in the United States and Canada had more than one area code, either from dividing a city into different areas (NPA split) or having more than one code for the same area (NPA overlay). The overlay method requires that the area code must be dialed in all cases, even for local calls, while the split plan may permit seven-digit dialing within the same area. The transition to ten-digit dialing typically starts with a permissive dialing phase, which is widely publicized, during which dialing all ten digits is optional. After a period of several months, mandatory dialing begins, when seven-digit dialing is no longer permissible. Atlanta was the first U.S. city to require mandatory ten-digit dialing throughout the metropolitan area, roughly coinciding with the 1996 Summer Olympics held there. Atlanta was used as the test case not only because of its size, but also because it had the world's largest fiber optic network at the time, five times larger than that of New York, and it was home to BellSouth (now part of AT&T), then the Southeastern Regional Bell Operating Company, with AT&T's fiber optics manufacturing facility within the city.
Depending on the techniques used for area code expansion, the effect on telephone users varies. In areas in which overlays were used, this generally avoids the need for converting telephone numbers, so existing directories, business records, letterheads, business cards, advertising, and ""speed-dialing"" settings can retain the same phone numbers, while the overlay is used for new number allocations. The primary impact on telephone users is the necessity of remembering and dialing 10- or 11-digit numbers when only 7-digit dialing was previously permissible.
Splitting instead of overlaying generally avoids the requirement for mandatory area-code dialing, but at the expense of having to convert a region to the new code. In addition to the requirements of updating records and directories to accommodate the new numbers, for efficient conversion this requires a period of ""permissive dialing"" in which the new and old codes are both allowed to work. Also, many splittings involved significant technical issues, especially when the area splittings occurred over boundaries other than phone network divisions.
In 1998 area code 612, which had covered the Minneapolis – Saint Paul Twin Cities, was split to create area code 651 for St. Paul and the eastern metropolitan area. The Minnesota Public Utilities Commission mandated that the new boundary exactly follow municipal boundaries, which were distinctly different from telephone exchange boundaries, and that all subscribers keep their 7-digit numbers. These two goals were directly at odds with the reason for the split, namely to provide additional phone numbers. More than 40 exchanges had territory that straddled the new boundary. As a result, prefixes were duplicated in both area codes, which counteracted much of the benefit of the splitting, with only 200 of 700 prefixes in area 612 moving entirely to area 651. In less than two years area code 612 again exhausted its supply of phone numbers, and required a three-way split in 2000, creating the new area codes 763 and 952. Again, the split followed political boundaries rather than rate center boundaries, resulting in additional split prefixes; a few numbers moved from 612 to 651 and then to 763 in less than two years.
Recognizing that the proliferation of area codes was largely due to the telecom regulation act and the assignment of numbers in blocks of 10,000, the FCC instructed NANPA, by then administered by Neustar, to alleviate the numbering shortage. As a result, number pooling was piloted in 2001 as a system for allocating local numbers to carriers in blocks of 1,000 rather than 10,000. Because of the then design of the switched telephone network, this was a considerable technical obstacle. Number pooling was implemented with another technical obstacle, local number portability.
The program has been implemented in much of the United States by state regulators. A limited number of cities have also implemented rate center consolidation; fewer rate centers resulted in more efficient use of numbers, as carriers would reserve blocks of 1,000 or 10,000 numbers in each of multiple rate centers in the same area even if they had relatively few clients in the area.[35] (A rate center is a geographical area used by a Local Exchange Carrier (LEC) to determine the boundaries for local calling, billing and assigning phone numbers. Typically a call within a rate center is local, while a call from one rate center to another is a long-distance call.) Together with aggressive reclamation of unused number blocks from telecom providers, number pooling has reduced the need for additional area codes, so that many previously designated area splits and overlays have been postponed indefinitely.
There is no number pooling in Canada. Number allocation remains highly inefficient as even the tiniest village is a rate center and every CLEC is assigned blocks of ten thousand numbers in every place it offers new local service. As a result, dialing seven digits even in remote locations like James Bay is more likely to produce an intercept message (""dial the area code"") than an actual voice connection.
Before 1995, all NANP countries and territories outside the contiguous United States, Alaska, Hawaii and Canada shared the area code 809. This included Puerto Rico and the U.S. Virgin Islands. Each has since been assigned one or more distinct numbering plan areas; area code 809 now exclusively serves the Dominican Republic (along with area codes 829 and 849). The United States Pacific territories of the Northern Mariana Islands and Guam joined the NANP in 1997, and American Samoa became an NANP member in October 2004. The Dutch possession of Sint Maarten was originally scheduled to join the NANP on May 31, 2010, but the changeover was postponed to September 30, 2011.[14]
2001: overlaid with area code 939
The NANP exhaust analysis estimates that the existing numbering system is sufficient beyond 2049, based on the assumptions that a maximum of 674 NPAs continue to be available, and that on average 3990 central office codes are needed per year.[36]
In case of exhaustion, various plans are discussed for expanding the numbering plan. One option is to add the digit 1 or 0 either at the beginning or at the end of the area code, or prefixing it to the seven-digit subscriber number. This would require eleven-digit dialing even for local calls between any two NANP numbers. Another proposal introduces the digit 9 into the area code in the format x9xx, so that, for example, San Francisco's 415 would become 4915. Other proposals include reallocating blocks of numbers assigned to smaller long-distance carriers or unused reserved services.[citation needed]
Of all states or territories, the U.S. state of California has the largest number of area codes assigned, followed by Texas, Florida and New York, while most countries of the Caribbean use only one.[37] Many Caribbean codes were assigned based on alphabetic abbreviations of the territory name, as indicated in the third column of the following table (Letter code). This follows the traditional letter assignments on telephone dials.  For some Pacific islands, the NANPA area code is the same as the country code that was discontinued upon membership in the NANP.
Membership in the NANP brings significant advantages for countries in the vicinity of the United States and Canada, which usually are already the top dialing destinations. Both countries also originate most of the tourism business for the Caribbean. This is enhanced by the integration from sharing the same dialing procedures, without international access codes, and the toll-free number system of the NANP, as businesses in all member countries are eligible to participate.
The structure of the North American Numbering Plan permits implementation of local dial plans in each plan area, depending on requirements. When multiple NPA codes serve an area in an overlay arrangement, ten-digit (10D) dialing is required. Seven-digit (7D) dialing may be permissible in areas with single area codes.[39] Depending on the requirement of toll alerting, it may be necessary to prefix a telephone number with 1. The NANPA publishes dial plan information for individual area codes.[40]
The standard dial plans in most cases are as follows:
The number of digits dialed is unrelated to being a local call or a toll call when there is no toll alerting. Allowing 7D local dial across an area code boundary, which is uncommon today, requires central office code protection, locally if using toll alerting, across the entire area code otherwise, to avoid assignment of the same seven-digit number on both sides.
Most areas permit local calls as 1+10D except for Texas, Georgia, and some jurisdictions in Canada which require that landline callers know which numbers are local and which are toll, dialing 10D for local calls and 1+10D for all toll calls.
In almost all cases, domestic operator-assisted calls are dialed 0+10D.
Some common special numbers in the North American system:
There are also special codes, such as:
Note: The four-digit numbers are not implemented in some areas. The codes prefixed with an asterisk (*) symbol are intended for use on Touch-Tone telephones, whereas the four-digit numbers prefixed 11xx are intended for use on rotary dial telephones, where the Touch-Tone * symbol is not available.
Not all NANP countries use the same codes. For example, the emergency telephone number is not always 911: Trinidad and Tobago and Dominica use 999, as in the United Kingdom. The country of Barbados uses 211 for police force, 311 for fire, and 511 for ambulance, while Jamaica uses 114 for directory assistance, 119 for police force, and 110 for fire and ambulance services.
Despite its early importance as a share of the worldwide telephone system, few of the NANP's codes, such as 911, have been adopted outside the system. Determining that 911 requires unnecessary rotation time on rotary dial telephones, the European Union has adopted its own standardized number of 112, while countries in Asia and the rest of the world use a variety of other two- or three-digit emergency telephone number combinations. The 112 code is gaining prevalence because of its preprogrammed presence in mobile telephones that conform to the European GSM standard. The European Union and many other countries have chosen the International Telecommunication Union's 00 as their international access number instead of 011. The toll-free prefix 800 has been widely adopted elsewhere, including as the international toll-free country code. It is often preceded by a 0 rather than a 1 in many countries where 0 is the trunk prefix.
Many dials on modern telephones in use in the NANP service areas maintain the tradition of alphabetic dialing. Usually each pushbutton from digit 2 to 9 also displays three letters, which is standardized in ISO 9995-8 and, in Europe, E.161. Historically, the letters Q and Z were omitted, although some modern telephones contain them. SMS-capable devices have all 26 letters. The alphabet is apportioned to the buttons as follows:
No letters are typically mapped to keys 1 and 0, although some corporate voicemail systems use 1 for Q and Z, and some old telephones assigned the Z to the digit 0.
Originally, this scheme was used as a mnemonic device for telephone number prefixes. When telephone numbers in the United States were standardized in the mid-20th century to seven digits, the first two digits of the exchange prefix were expressed as letters rather than numbers, using the telephone exchange name. Before World War II, the largest cities used three letters and four or five numbers, while in most cities with customer dialing, phone numbers had only six digits (2L-4N). The prefix was a name, and the first two or three letters, usually shown in capital letters, were dialed. Later, the third letter, where implemented, was replaced by a digit, or an extra digit was added. This generally happened after World War II, although New York City converted in 1930. The adoption of seven-digit local numbers (2L-5N) was chosen as the requirement for direct distance dialing and progressively deployed starting the late 1940s.
The famous Glenn Miller tune PEnnsylvania 6-5000 refers to telephone number PE6-5000, a number still in service at the Hotel Pennsylvania (212 736-5000) in New York. Similarly, the classic film BUtterfield 8 is set in the East Side of Manhattan between roughly 64th and 86th Streets, where the telephone prefixes include 288. In some works of fiction, phone numbers will begin with ""KLondike 5"" or KLamath 5, which translates to 555, an exchange that is reserved for information numbers in North America.
The letter system was phased out, beginning before 1965, although it persisted ten years later in some places. It was included in Bell of Pennsylvania directories until 1983. Even today, some businesses still display a 2L-5N number in advertisements, e.g., the Belvedere Construction Company in Detroit, Michigan not only still uses the 2L-5N format for its number (TYler 8-7100), it uses the format for the toll-free number (1-800-TY8-7100).
Despite the phasing out of the letter system otherwise, alphabetic phonewords remain as a commercial mnemonic gimmick, particularly for toll-free numbers. For example, one can dial 1-800-FLOWERS to order flowers, or 1-800-DENTIST to find a local dentist.
In addition to commercial uses, alphabetic dialing has occasionally influenced the choice of regional area codes in the United States. For example, when area 423 (East Tennessee) was split in 1999, the region surrounding Knoxville was assigned area code 865, chosen to represent the word VOL (Volunteers), the nickname of Tennessee (The Volunteer State), as well as athletic teams at the University of Tennessee.[42][43] Another example of this is area code 859 in Kentucky, which was chosen to represent 'UKY' as a nod to the University of Kentucky in Lexington, Kentucky, the code's principal city.
Several Caribbean area codes were chosen as an alphabetic abbreviation of the country name, which are indicated in the table of NANP regions.
The North American Numbering Plan does not set aside special non-geographic area codes exclusively for cellular phones. Only one regional exception exists in area code 600 in Canada.
In many other national numbering plans outside the NANP, mobile services are assigned separate prefixes. Cell phone numbers in the NANP are allocated within each area code from special central office prefixes and calls to them are billed at the same rate as any other call. Consequently, the caller pays pricing model adopted in other countries, in which calls to cellular phones are charged at a higher nationwide rate, but incoming mobile calls are not charged to the mobile user, could not be implemented. Instead, North American cellular telephone subscribers are also generally charged for receiving calls (subscriber pays). This has discouraged mobile users from publishing their telephone number. However, price competition among carriers has reduced the average price per call minute for contract customers for both inbound and outbound calls, which compare favorably to those in caller-pays countries.[citation needed] Most users select bundle pricing plans that include an allotment of minutes expected to be used in the billing period. Of the four major national carriers in the United States, all four (AT&T, T-Mobile/Sprint, Verizon) offer free calling between mobile phones on the carrier's network, and Sprint also offers its customers free calling to mobile phones on other networks.[citation needed]
Industry observers have attributed the relatively low mobile phone penetration rate in the United States, compared to that of Europe, to the subscriber-pays model.[citation needed] In this model the convenience of the mobility is charged to the subscriber. Callers from outside the local-calling region of the assigned number, however, pay for a long-distance call, although domestic long-distance rates are generally lower than the rates in caller-pays systems. Conversely, an advantage of caller-pays is the relative absence of telemarketing and nuisance calls to mobile numbers. The integrated numbering plan also enables local number portability between fixed and wireless services within a region, allowing users to switch to mobile service while keeping their telephone number.
The initial plan for area code overlays did allow for providing separate area codes for use by mobile devices, although these were still assigned to a specific geographical area, and were charged at the same rate as other area codes. Initially, the area code 917 for New York City was specifically assigned for this purpose within the boroughs; however, a Federal court overturned the practice and the use of an area code for a specific telephony purpose.[citation needed] Since mobile telephony has been expanding faster than landline use, new area codes typically have a disproportionately large fraction of mobile and nomadic numbers, although landline and other services rapidly follow and local network portability can blur these distinctions.
The experience of Hurricane Katrina and similar events revealed a possible disadvantage of the methods employed in the geographic assignment of cellular numbers. Many mobile phone users could not be reached, even when they were far from the stricken areas, because the routing of calls to their phones depended on equipment in the affected area. They could make calls but not receive them.[44]
The use of geographic numbers may also lead to tromboning; one can take a handset with a Vancouver number into St. John's and outbound calls to St. John's numbers while in that city will be local, but incoming calls must make the cross-country trip to Vancouver and back. This adds costs for subscribers, as an 8,000 km cross-country call (as a worst case) incurs long-distance tolls in both directions. AMPS subscribers used to be provided with a local number (such as 1-NPA-NXX-ROAM) in each city, allowing them to be reached by dialing that number plus the ten-digit mobile telephone number; this is no longer supported.
Calls between different countries and territories of the NANP are not typically charged at domestic rates. For example, most long-distance plans may charge a California subscriber a higher rate for a call to British Columbia than for a call to New York, even though both destinations are within the NANP. Similarly, calls from Bermuda to U.S. numbers (including 1-800 numbers, which are normally thought of as toll-free) incur international rates. This is because many of the island nations implemented a plan of subsidizing the cost of local phone services by directly charging higher pricing levies on international long-distance services.[citation needed]
Because of these higher fees, scams had taken advantage of customers' unfamiliarity with pricing structure to call the legacy regional area code 809. Some scams lured customers from the United States and Canada into placing expensive calls to the Caribbean, by representing area code 809 as a regular domestic, low-cost, or toll-free call. The split of 809 (which formerly covered all of the Caribbean NANP points) into multiple new area codes created many new, unfamiliar prefixes which could be mistaken for U.S. or Canada domestic area codes but carried high tariffs. In various island nations, premium exchanges such as +1-876-HOT-, +1-876-WET- or +1-876-SEX- (where 876 is Jamaica) became a means to circumvent consumer-protection laws governing area code 900 or similar U.S.-domestic premium numbers.
These scams are on the decline, with many of the Cable and Wireless service monopolies being opened up to competition, hence bringing rates down. Additionally, many Caribbean territories have implemented local government agencies to regulate telecommunications rates of providers.[45][46]
The Telecommunications Act of 1996 (47 U.S.C. § 251 (b)(2)) authorizes the Federal Communications Commission (FCC) to require all local exchange carriers (LECs) to offer local number portability.[47] The FCC regulations were enacted on June 27, 1996, with changes to take effect in the one hundred largest Metropolitan Statistical Areas by October 1, 1997 and elsewhere by December 31, 1998.[48]
The FCC directed the North American Numbering Council (NANC) to select one or more private-sector candidates for the local number portability administrator (LNPA) function,[49] in a manner akin to the selection of the North American Numbering Plan Administrator (NANPA).[50]
The toll-free telephone numbers in NPA 800, 888, 877, 866, 855, and 844 have been portable through the RespOrg system since 1993.[51]
American television programs and films often use the central office code 555, or KLamath 5 and KLondike 5 in older movies and shows, for fictitious telephone numbers, to prevent disturbing actual telephone subscribers if anyone is tempted to dial a telephone number seen or referred to on screen.
Occasionally, valid telephone numbers are used in contexts such as songs with varying intents and consequences. An example is the 1981 song ""867-5309/Jenny"" by Tommy Tutone, which is the cause of a large number of calls.[52]
Not all numbers beginning with 555 are fictional. For example, 555-1212 is the standard number for directory assistance. Only 555-0100 through 555-0199 are reserved for fictional use. Where used, these are often routed to information services; Canadian telephone companies briefly promoted 555-1313 as a pay-per-use ""name that number"" reverse lookup in the mid-1990s.[53]
"
FidoNet - Wikipedia," FidoNet is a worldwide computer network that is used for communication between bulletin board systems (BBSes). It uses a store-and-forward system to exchange private (email) and public (forum) messages between the BBSes in the network, as well as other files and protocols in some cases.
The FidoNet system was based on several small interacting programs, only one of which needed to be ported to support other BBS software. FidoNet was one of the few networks that was supported by almost all BBS software, as well as a number of non-BBS online services. This modular construction also allowed FidoNet to easily upgrade to new data compression systems, which was important in an era using modem-based communications over telephone links with high long-distance calling charges.
The rapid improvement in modem speeds during the early 1990s, combined with the rapid decrease in price of computer systems and storage, made BBSes increasingly popular. By the mid-1990s there were almost 40,000 FidoNet systems in operation, and it was possible to communicate with millions of users around the world. Only UUCPNET came close in terms of breadth or numbers; FidoNet's user base far surpassed other networks like BITNET.[1]
The broad availability of low-cost Internet connections starting in the mid-1990s lessened the need for FidoNet's store-and-forward system, as any system in the world could be reached for equal cost. Direct dialing into local BBS systems rapidly declined. Although FidoNet has shrunk considerably since the late 1990s, it has remained in use even today[2] despite internet connectivity becoming universally available.
There are two major accounts of the development of the FidoNet, differing only in small details.
Around Christmas 1983, Tom Jennings started work on a new MS-DOS–hosted bulletin board system that would emerge as Fido BBS. Jennings set up the system in San Francisco sometime in early 1984. Another early user was John Madil, who was trying to set up a similar system in Baltimore on his Rainbow 100. Fido started spreading to new systems, and Jennings eventually started keeping an informal list of their phone numbers, with Jennings becoming #1 and Madil #2.[3]
Jennings released the first version of the FidoNet software in June 1984. In early 1985 he wrote a document explaining the operations of the FidoNet, along with a short portion on the history of the system. In this version, FidoNet was developed as a way to exchange mail between the first two Fido BBS systems, Jennings' and Madil's, to ""see if it could be done, merely for the fun of it"". This was first supported in Fido V7, ""sometime in June 84 or so"".[4][5][6]
In early 1984, Ben Baker was planning on starting a BBS for the newly forming computer club at the McDonnell Douglas automotive division in St. Louis. Baker was part of the CP/M special interest group within the club.[7] He intended to use the seminal, CP/M-hosted, CBBS system, and went looking for a machine to run it on. The club's president told Baker that DEC would be giving them a Rainbow 100 computer on indefinite loan, so he made plans to move the CBBS onto this machine. The Rainbow contained two processors, an Intel 8088 and a Zilog Z80, allowing it to run both MS-DOS and CP/M, with the BBS running on the latter. When the machine arrived, they learned that the Z80 side had no access to the I/O ports, so CBBS could not communicate with a modem. While searching for software that would run on the MS-DOS side of the system, Baker learned of Fido through Madil.[3]
The Fido software required changes to the serial drivers to work properly on the Rainbow. A porting effort started, involving Jennings, Madil and Baker. This caused all involved to rack up considerable long distance charges as they all called each other during development, or called into each other's BBSes to leave email. During one such call ""in May or early June"", Baker and Jennings discussed how great it would be if the BBS systems could call each other automatically, exchanging mail and files between them.[3] This would allow them to compose mail on their local machines, and then deliver it quickly, as opposed to calling in and typing the message in while on a long-distance telephone connection.[3]
Jennings responded by calling into Baker's system that night and uploading a new version of the software consisting of three files: FIDO_DECV6, a new version of the BBS program itself, FIDONET, a new program, and NODELIST.BBS, a text file. The new version of FIDO BBS had a timer that caused it to exit at a specified time, normally at night. As it exited it would run the separate FIDONET program. NODELIST was the list of Fido BBS systems, which Jennings had already been compiling.[3]
The FIDONET program was what later became known as a mailer. The FIDO BBS software was modified to use a previously unused numeric field in the message headers to store a node number for the machine the message should be delivered to. When FIDONET ran, it would search through the email database for any messages with a number in this field. FIDONET collected all of the messages for a particular node number into a file known as a message packet. After all the packets were generated, one for each node, the FIDONET program would look up the destination node's phone number in NODELIST.BBS, and call the remote system. Provided that FIDONET was running on that system, the two systems would handshake and, if this succeeded, the calling system would upload its packet, download a return packet if there was one, and disconnect. FIDONET would then unpack the return packet, place the received messages into the local system's database, and move onto the next packet. When there were no remaining packets, FIDONET would exit, and run the FIDO BBS program.[8]
In order to lower long-distance charges, the mail exchanges were timed to run late at night, normally 4 AM.[4] This would later be known as national mail hour,[9] and, later still, as Zone Mail Hour.
By June 1984 Version 7 of the system was being run in production, and nodes were rapidly being added to the network. By August there were almost 30 systems in the nodelist, 50 by September, and over 160 by January 1985. As the network grew, the maintenance of the nodelist became prohibitive, and errors were common. In these cases, people would start receiving phone calls at 4 AM, from a caller that would say nothing and then hang up. In other cases the system would be listed before it was up and running, resulting in long-distance calls that accomplished nothing.[4]
In August 1984 Jennings handed off control of the nodelist to the group in St. Louis, mostly Ken Kaplan and Ben Baker. Kaplan had come across Fido as part of finding a BBS solution for his company, which worked with DEC computers and had been given a Rainbow computer and a USRobotics 1200bit/s modem.[10] From then on, joining FidoNet required one to set up their system and use it to deliver a netmail message to a special system, Node 51. The message contained various required contact information. If this message was transmitted successfully, it ensured that at least some of the system was working properly. The nodelist team would then reply with another netmail message back to the system in question, containing the assigned node number. If delivery succeeded, the system was considered to be working properly, and it was added to the nodelist.[4] The first new nodelist was published on 21 September 1984.[3]
Growth continued to accelerate, and by the spring of 1985, the system was already reaching its limit of 250 nodes. In addition to the limits on the growth of what was clearly a popular system, nodelist maintenance continued to grow more and more time-consuming.[3]
It was also realized that Fido systems were generally clustered – of the 15 systems running by the start of June 1984, 5 of them were in St. Louis.[3]  A user on Jennings's system in San Francisco that addressed emails to different systems in St. Louis would cause calls to be made to each of those BBSes in turn. In the United States, local calls were normally free, and in most other countries were charged at a low rate. Additionally, the initial call setup, generally the first minute of the call, was normally billed at a higher rate than continuing an existing connection. Therefore, it would be less expensive to deliver all the messages from all the users in San Francisco to all of the users in St. Louis in a single call. Packets were generally small enough to be delivered within a minute or two, so delivering all the messages in a single call could greatly reduce costs by avoiding multiple first-minute charges. Once delivered, the packet would be broken out into separate packets for local systems, and delivered using multiple local free calls.
The team settled on the concept of adding a new network number patterned on the idea of area codes.[N 1] A complete network address would now consist of the network and node number pair, which would be written with a slash between them. All mail travelling between networks would first be sent to their local network host, someone who volunteered to pay for any long distance charges. That single site would collect up all the netmail from all of the systems in their network, then re-package it into single packets destined to each network. They would then call any required network admin sites and deliver the packet to them. That site would then process the mail as normal, although all of the messages in the packet would be guaranteed to be local calls.[3]
The network address was placed in an unused field in the Fido message database, which formerly always held a zero. Systems running existing versions of the software already ignored the fields containing the new addressing, so they would continue to work as before; when noticing a message addressed to another node they would look it up and call that system. Newer systems would recognize the network number and instead deliver that message to the network host. To ensure backward compatibility, existing systems retained their original node numbers through this period.[3]
A huge advantage of the new scheme was that node numbers were now unique only within their network, not globally. This meant the previous 250 node limit was gone, but for a variety of reasons this was initially limited to about 1,200. This change also devolved the maintenance of the nodelists down to the network hosts, who then sent updated lists back to Node 51 to be collected into the master list. The St. Louis group now had to only maintain their own local network, and do basic work to compile the global list.[3]
At a meeting held in Kaplan's living room in St. Louis on 11 April 1985[N 2] the various parties hammered out all of the details of the new concept. As part of this meeting, they also added the concept of a region, a purely administrative level that was not part of the addressing scheme. Regional hosts would handle any stragglers in the network maps, remote systems that had no local network hosts. They then divided up the US into ten regions that they felt would have roughly equal populations.[3]
By May, Jennings had early versions of the new software running. These early versions specified the routing manually through a new ROUTE.BBS file that listed network hosts for each node. For instance, an operator might want to forward all mail to St. Louis through a single node, node 10. ROUTE.BBS would then include a list of all the known systems in that area, with instructions to forward mail to each of those nodes through node 10. This process was later semi-automated by John Warren's NODELIST program.[11] Over time, this information was folded into updated versions of the nodelist format, and the ROUTES file is no longer used.[12]
A new version of FIDO and FIDONET, 10C, was released containing all of these features. On 12 June 1985 the core group brought up 10C, and most Fido systems had upgraded within a few months.[11] The process went much smoother than anyone imagined, and very few nodes had any problems.[3]
Sometime during the evolution of Fido, file attachments were added to the system, allowing a file to be referenced from an email message. During the normal exchange between two instances of FIDONET, any files attached to the messages in the packets were delivered after the packet itself had been up or downloaded. It is not clear when this was added, but it was already a feature of the basic system when the 8 February 1985 version of the FidoNet standards document was released, so this was added very early in Fido's history.
At a sysop meeting in Dallas, the idea was raised that it would be nice if there was some way for the sysops to post messages that would be shared among the systems.[13] In February 1986 Jeff Rush, one of the group members, introduced a new mailer that extracted messages from public forums that the sysop selected, like the way the original mailer handled private messages. The new program was known as a tosser/scanner. The tosser produced a file that was similar (or identical) to the output from the normal netmail scan, however, these files were then compressed and attached to a normal netmail message as an attachment. This message was then sent to a special address on the remote system. After receiving netmail as normal, the scanner on the remote system looked for these messages, unpacked them, and put them into the same public forum on the original system.[9]
In this fashion, Rush's system implemented a store and forward public message system similar to Usenet, but based on, and hosted by, the FidoNet system. The first such echomail forum was one created by the Dallas area sysops to discuss business, known as SYSOP. Another called TECH soon followed. Several public echos soon followed, including GAYNET and CLANG. These spawned hundreds of new echos, and led to the creation of the Echomail Conference List (Echolist) by Thomas Kenny in January 1987.[14] Echomail produced world-spanning shared forums, and its traffic volume quickly surpassed the original netmail system. By the early 1990s, echo mail was carrying over 8 MB of compressed message traffic a day, many times that when uncompressed.[9]
Echomail did not necessarily use the same distribution pathways as normal netmail, and the distribution routing was stored in a separate setup file not unlike the original ROUTES.BBS. At the originating site a header line was added to the message indicating the origin system's name and address. After that, each system that the message traveled through added itself to a growing PATH header, as well as a SEENBY header. SEENBY prevented the message from looping around the network in the case of misconfigured routing information.[9]
Echomail was not the only system to use the file attachment feature of netmail to implement store-and-forward capabilities. Similar concepts were used by online games and other systems as well.
The evolution towards the net/node addressing scheme was also useful for reducing communications costs between continents, where time zone differences on either end of the connection might also come into play. For instance, the best time to forward mail in the US was at night, but that might not be the best time for European hosts to exchange. Efforts towards introducing a continental level to the addressing system started in 1986.[9]
At the same time, it was noted that some power users were interested in using FidoNet protocols as a way of delivering the large quantities of echomail to their local machines where it could be read offline. These users did not want their systems to appear in the nodelist - they did not (necessarily) run a bulletin board system and were not publicly accessible.[9] A mechanism allowing netmail delivery to these systems without the overhead of nodelist maintenance was desirable.
In October 1986 the last major change to the FidoNet network was released, adding zones and points. Zones represented major geographical areas roughly corresponding to continents. There were six zones in total, North America, South America, Europe, Oceania, Asia, and Africa. Points represented non-public nodes, which were created privately on a BBS system. Point mail was delivered to a selected host BBS as normal, but then re-packaged into a packet for the point to pick up on-demand. The complete addressing format was now zone:net/node.point, so a real example might be Bob Smith@1:250/250.10.[9] Points were widely used only for a short time, the introduction of offline reader systems filled this role with systems that were much easier to use. Points remain in use to this day but are less popular than when they were introduced.
Although FidoNet supported file attachments from even the earliest standards, this feature tended to be rarely used and was often turned off. File attachments followed the normal mail routing through multiple systems and could back up transfers all along the line as the files were copied. A solution was offered in the form of file requests, which made file transfers driven by the calling system and used one-time point-to-point connections instead of the traditional routing. Two such standards became common, ""WaZOO"" and ""Bark"", which saw varying support among different mailers. Both worked similarly, with the mailer calling the remote system and sending a new handshake packet to request the files.[15][16]
Although FidoNet was, by far, the best known BBS-based network, it was by no means the only one. From 1988 on, PCBoard systems were able to host similar functionality known as RelayNet, while other popular networks included RBBSNet from the Commodore 64 world, and AlterNet. Late in the evolution of the FidoNet system, there was a proposal to allow mail (but not forum messages) from these systems to switch into the FidoNet structure.[17] This was not adopted, and the rapid rise of the internet made this superfluous as these networks rapidly added internet exchange, which acted as a lingua franca.
FidoNet started in 1984 and listed 100 nodes by the end of that year. Steady growth continued through the 1980s, but a combination of factors led to rapid growth after 1988. These included faster and less expensive modems and rapidly declining costs of hard drives and computer systems in general. By April 1993, the FidoNet nodelist contained over 20,000 systems. At that time it was estimated that each node had, on average, about 200 active users. Of these 4 million users in total, 2 million users commonly used echomail, the shared public forums, while about 200,000 used the private netmail system.[9] At its peak, FidoNet listed approximately 39,000 systems.[4][N 3]
Throughout its lifetime, FidoNet was beset with management problems and infighting. Much of this can be traced to the fact that the inter-net delivery cost real money, and the traffic grew more rapidly than decreases caused by improving modem speeds and downward trending long-distance rates. As they increased, various methods of recouping the costs were attempted, all of which caused friction in the groups. The problems were so bad that Jennings came to refer to the system as the ""fight-o-net"".[18]
As modems reached speeds of 28.8 kbit/s, the overhead of the TCP/IP protocols were no longer so egregious and dial-up Internet became increasingly common. By 1995, the bulletin board market was reeling as users abandoned local BBS systems in favour of larger sites and web pages, which could be accessed worldwide for the same cost as accessing a local BBS system. This also made FidoNet less expensive to implement, because inter-net transfers could be delivered over the Internet as well, at little or no marginal cost. But this seriously diluted the entire purpose of the store-and-forward model, which had been built up specifically to address a long-distance problem that no longer existed.
The FidoNet nodelist started shrinking, especially in areas with a widespread availability of internet connections. This downward trend continues but has levelled out at approximately 2,500 nodes.[N 4] FidoNet remains popular in areas where Internet access is difficult to come by, or expensive.
There is now (~2014) a retro movement which is resulting in a slow increase in internet-connected BBS and nodes. Telnet, Rlogin, and SSH are being used between systems. This means the user can telnet to any BBS worldwide as cheaply as ones next door. Also, Usenet and internet mail has been added, along with long file names to many newer versions of BBS software, some being free-ware, resulting in increasing use. Nodelists are no longer declining in all cases.
FidoNet is governed in a hierarchical structure according to FidoNet policy, with designated coordinators at each level to manage the administration of FidoNet nodes and resolve disputes between members.[1] This structure is very similar to the organization structure of the Sicilian Mafia. Network coordinators (referred to as ""Button Men"") are responsible for managing the individual nodes within their area, usually a city or similar sized area. Regional coordinators (referred to as ""Underbosses"") are responsible for managing the administration of the network coordinators within their region, typically the size of a state, or small country. Zone coordinators (referred to as either ""Dons"" or ""Godfathers"") are responsible for managing the administration of all of the regions within their zone. The world is divided into six zones, the coordinators of which appoint themselves or representatives to the positions of ""International Coordinators"" of FidoNet (referred to as ""La Cosa Nostra""). The six zone ""International Coordinators"", along with their Counselors (also known as their ""Consiglieres""), form the twelve person body known as ""FidoNet Central"".
FidoNet was historically designed to use modem-based dial-up (POTS) access between bulletin board systems, and much of its policy and structure reflected this.
The FidoNet system officially referred only to the transfer of Netmail—the individual private messages between people using bulletin boards—including the protocols and standards with which to support it. A netmail message would contain the name of the person sending, the name of the intended recipient, and the respective FidoNet addresses of each. The FidoNet system was responsible for routing the message from one system to the other (details below), with the bulletin board software on each end being responsible for ensuring that only the intended recipient could read it. Due to the hobbyist nature of the network, any privacy between the sender and recipient was only the result of politeness from the owners of the FidoNet systems involved in the mail's transfer. It was common, however, for system operators to reserve the right to review the content of mail that passed through their system.
Netmail allowed for the attachment of a single file to every message. This led to a series of piggyback protocols that built additional features onto FidoNet by passing information back and forth as file attachments. These included the automated distribution of files and transmission of data for inter-BBS games.
By far the most commonly used of these piggyback protocols was Echomail, public discussions similar to Usenet newsgroups in nature. Echomail was supported by a variety of software that collected up new messages from the local BBSes' public forums (the scanner), compressed it using ARC or ZIP, attached the resulting archive to a Netmail message, and sent that message to a selected system. On receiving such a message, identified because it was addressed to a particular user, the reverse process was used to extract the messages, and a tosser put them back into the new system's forums.
Echomail was so popular that for many users, Echomail was the FidoNet. Private person-to-person Netmail was relatively rare.
FidoNet is politically organized into a tree structure, with different parts of the tree electing their respective coordinators. The FidoNet hierarchy consists of zones, regions, networks, nodes and points broken down more-or-less geographically.
The highest level is the zone, which is largely continent-based:
Each zone is broken down into regions, which are broken down into nets, which consist of individual nodes.  Zones 7-4095 are used for othernets; groupings of nodes that use Fido-compatible software to carry their own independent message areas without being in any way controlled by FidoNet's political structure. Using un-used zone numbers would ensure that each network would have a unique set of addresses, avoiding potential routing conflicts and ambiguities for systems that belonged to more than one network.
FidoNet addresses explicitly consist of a zone number, a network number (or region number), and a node number. They are written in the form Zone:Network/Node.[20] The FidoNet structure also allows for semantic designation of region, host, and hub status for particular nodes, but this status is not directly indicated by the main address.
For example, consider a node located in Tulsa, Oklahoma, United States with an assigned node number is 918, located in Zone 1 (North America), Region 19, and Network 170. The full FidoNet address for this system would be 1:170/918.  The region was used for administrative purposes, and was only part of the address if the node was listed directly underneath the Regional Coordinator, rather than one of the networks that were used to divide the region further.
FidoNet policy requires that each FidoNet system maintain a nodelist of every other member system. Information on each node includes the name of the system or BBS, the name of the node operator, the geographic location, the telephone number, and software capabilities. The nodelist is updated weekly, to avoid unwanted calls to nodes that had shut down, with their phone numbers possibly having been reassigned for voice use by the respective telephone company.
To accomplish regular updates, coordinators of each network maintain the list of systems in their local areas. The lists are forwarded back to the International Coordinator via automated systems on a regular basis. The International Coordinator would then compile a new nodelist, and generate the list of changes (nodediff) to be distributed for node operators to apply to their existing nodelist.
In a theoretical situation, a node would normally forward messages to a hub. The hub, acting as a distribution point for mail, might then send the message to the Net Coordinator. From there it may be sent through a Regional Coordinator, or to some other system specifically set up for the function. Mail to other zones might be sent through a Zone Gate.
For example, a FidoNet message might follow the path:
Originally there was no specific relationship between network numbers and the regions they reside in. In some areas of FidoNet, most notably in Zone 2, the relationship between region number and network number are entwined. For example, 2:201/329 is in Net 201 which is in Region 20 while 2:2410/330 is in Net 2410 which is in Region 24. Zone 2 also relates the node number to the hub number if the network is large enough to contain any hubs. This effect may be seen in the nodelist by looking at the structure of Net 2410 where node 2:2410/330 is listed under Hub 300. This is not the case in other zones.
In Zone 1, things are much different. Zone 1 was the starting point and when Zones and Regions were formed, the existing nets were divided up regionally with no set formula. The only consideration taken was where they were located geographically with respect to the region's mapped outline. As net numbers got added, the following formula was used.
Region number × 20
Then when some regions started running out of network numbers, the following was also used.
Region number × 200
Region 19, for instance, contains nets 380-399 and 3800-3999 in addition to those that were in Region 19 when it was formed.
Part of the objective behind the formation of local nets was to implement cost reduction plans by which all messages would be sent to one or more hubs or hosts in compressed form (ARC was nominally standard, but PKZIP is universally supported); one toll call could then be made during off-peak hours to exchange entire message-filled archives with an out-of-town uplink for further redistribution.
In practice, the FidoNet structure allows for any node to connect directly to any other, and node operators would sometimes form their own toll-calling arrangements on an ad-hoc basis, allowing for a balance between collective cost saving and timely delivery. For instance, if one node operator in a network offered to make regular toll calls to a particular system elsewhere, other operators might arrange to forward all of their mail destined for the remote system, and those near it, to the local volunteer.  Operators within individual networks would sometimes have cost-sharing arrangements, but it was also common for people to volunteer to pay for regular toll calls either out of generosity or to build their status in the community.
This ad-hoc system was particularly popular with networks that were built on top of FidoNet. Echomail, for instance, often involved relatively large file transfers due to its popularity. If official FidoNet distributors refused to transfer Echomail due to additional toll charges, other node operators would sometimes volunteer. In such cases, Echomail messages would be routed to the volunteers' systems instead.
The FidoNet system was best adapted to an environment in which local telephone service was inexpensive and long-distance calls (or intercity data transfer via packet-switched networks) costly. Therefore, it fared somewhat poorly in Japan, where even local lines are expensive, or in France, where tolls on local calls and competition with Minitel or other data networks limited its growth.
As the number of messages in Echomail grew over time, it became very difficult for users to keep up with the volume while logged into their local BBS. Points were introduced to address this, allowing technically-savvy users to receive the already compressed and batched Echomail (and Netmail) and read it locally on their own machines.[21]
To do this, the FidoNet addressing scheme was extended with the addition of a final address segment, the point number. For instance, a user on the example system above might be given point number 10, and thus could be sent mail at the address 1:170/918.10.
In real-world use, points are fairly difficult to set up. The FidoNet software typically consisted of a number of small utility programs run by manually edited scripts that required some level of technical ability. Reading and editing the mail required either a ""sysop editor"" program or a BBS program to be run locally.
In North America (Zone 1), where local calls are generally free, the benefits of the system were offset by its complexity. Points were used only briefly, and even then only to a limited degree. Dedicated offline mail reader programs such as Blue Wave, Squiggy and Silver Xpress (OPX) were introduced in the mid-1990s and quickly rendered the point system obsolete. Many of these packages supported the QWK offline mail standard.
In other parts of the world, especially Europe, this was different. In Europe, even local calls are generally metered, so there was a strong incentive to keep the duration of the calls as short as possible. Point software employs standard compression (ZIP, ARJ, etc.) and so keeps the calls down to a few minutes a day at most. In contrast to North America, pointing saw rapid and fairly widespread uptake in Europe.
Many regions distribute a pointlist in parallel with the nodelist. The pointlist segments are maintained by Net- and Region Pointlist Keepers and the Zone Point List Keeper assembles them into the Zone pointlist. At the peak of FidoNet there were over 120,000 points listed in the Zone 2 pointlist. Listing points is on a voluntary basis and not every point is listed, so how many points there really were is anybody's guess. As of June 2006, there are still some 50,000 listed points. Most of them are in Russia and Ukraine.
FidoNet contained several technical specifications for compatibility between systems.  The most basic of all is FTS-0001,[22] with which all FidoNet systems are required to comply as a minimum requirement.  FTS-0001 defined:
Other specifications that were commonly used provided for echomail, different transfer protocols and handshake methods (e.g.: Yoohoo/Yoohoo2u2, EMSI), file compression, nodelist format, transfer over reliable connections such as the Internet (Binkp), and other aspects.
Since computer bulletin boards historically used the same telephone lines for transferring mail as were used for dial-in human users of the BBS, FidoNet policy dictates that at least one designated line of each FidoNet node must be available for accepting mail from other FidoNet nodes during a particular hour of each day.[23]
Zone Mail Hour, as it was named, varies depending on the geographic location of the node, and was designated to occur during the early morning.  The exact hour varies depending on the time zone, and any node with only one telephone line is required to reject human callers. In practice, particularly in later times, most FidoNet systems tend to accept mail at any time of day when the phone line is not busy, usually during night.
Most FidoNet deployments were designed in a modular fashion.  A typical deployment would involve several applications that would communicate through shared files and directories, and switch between each other through carefully designed scripts or batch files.  However, monolithic software that encompassed all required functions in one package is available, such as D'Bridge.  Such software eliminated the need for custom batch files and is tightly integrated in operation.  The preference for deployment was that of the operator and there were both pros and cons of running in either fashion.
Arguably the most important piece of software on a DOS-based Fido system was the  FOSSIL driver, which was a small device driver which provided a standard way for the Fido software to talk to the modem.[24]  This driver needed to be loaded before any Fido software would work.  An efficient FOSSIL driver meant faster, more reliable connections.
Mailer software was responsible for transferring files and messages between systems, as well as passing control to other applications, such as the BBS software, at appropriate times.  The mailer would initially answer the phone and, if necessary, deal with incoming mail via FidoNet transfer protocols.  If the mailer answered the phone and a human caller was detected rather than other mailer software, the mailer would exit, and pass control to the BBS software, which would then initialise for interaction with the user.  When outgoing mail was waiting on the local system, the mailer software would attempt to send it from time to time by dialing and connecting to other systems who would accept and route the mail further.  Due to the costs of toll calls which often varied between peak and off-peak times, mailer software would usually allow its operator to configure the optimal times in which to attempt to send mail to other systems.
BBS software was used to interact with human callers to the system.  BBS software would allow dial-in users to use the system's message bases and write mail to others, locally or on other BBSes.  Mail directed to other BBSes would later be routed and sent by the mailer, usually after the user had finished using the system.  Many BBSes also allowed users to exchange files, play games, and interact with other users in a variety of ways (i.e.: node to node chat).
A scanner/tosser application, such as FastEcho, FMail, TosScan and Squish, would normally be invoked when a BBS user had entered a new FidoNet message that needed to be sent, or when a mailer had received new mail to be imported into the local messages bases.  This application would be responsible for handling the packaging of incoming and outgoing mail, moving it between the local system's message bases and the mailer's inbound and outbound directories.  The scanner/tosser application would generally be responsible for basic routing information, determining which systems to forward mail to.
In later times, message readers or editors that were independent of BBS software were also developed.  Often the System Operator of a particular BBS would use a devoted message reader, rather than the BBS software itself, to read and write FidoNet and related messages. One of the most popular editors in 2008 was GoldED+.  In some cases, FidoNet nodes, or more often FidoNet points, had no public bulletin board attached and existed only for the transfer of mail for the benefit of the node's operator. Most nodes in 2009 had no BBS access, but only points, if anything.
The original Fido BBS software, and some other FidoNet-supporting software from the 1980s, is no longer functional on modern systems.  This is for several reasons, including problems related to the Y2K bug.  In some cases, the original authors have left the BBS or shareware community, and the software, much of which was closed source, has been rendered abandonware.
Several DOS-based legacy FidoNet Mailers such as FrontDoor, Intermail, MainDoor and D'Bridge from the early 1990s can still be run today under Windows without a modem, by using the freeware NetFoss Telnet FOSSIL driver, and by using a Virtual Modem such as NetSerial. This allows the mailer to dial an IP address or hostname via Telnet, rather than dialing a real POTS phone number. There are similar solutions for Linux such as MODEMU (modem emulator) which has limited success when combined with DOSEMU (DOS emulator).
Mail Tossers such as FastEcho and FMail are still used today under both Windows and Linux/DOSEMU.
There are several modern Windows based FidoNet Mailers available today with source code, including Argus, Radius, and Taurus. MainDoor is another Windows based Fidonet mailer, which also can be run using either a modem or directly over TCP/IP. Two popular free and open source software FidoNet mailers for Unix-like systems are the binkd (cross-platform, IP-only, uses the binkp protocol) and qico (supports modem communication as well as the IP protocol of ifcico and binkp).
On the hardware side, Fido systems were usually well-equipped machines, for their day, with quick CPUs, high-speed modems and 16550 UARTs, which were at the time an upgrade.  As a Fidonet system was usually a BBS, it needed to quickly process any new mail events before returning to its 'waiting for call' state. In addition, the BBS itself usually necessitated lots of storage space.  Finally, a FidoNet system usually had at least one dedicated phone line.  Consequently, operating a Fidonet system often required significant financial investment, a cost usually met by the owner of the system.
While the use of FidoNet has dropped dramatically compared with its use up to the mid-1990s, it is still used in many countries and especially Russia and former republics of the USSR.[citation needed]  Some BBSes, including those that are now available for users with Internet connections via telnet, also retain their FidoNet netmail and echomail feeds.
Some of FidoNet's echomail conferences are available via gateways with the Usenet news hierarchy using software like UFGate.  There are also mail gates for exchanging messages between Internet and FidoNet.  Widespread net abuse and e-mail spam on the Internet side has caused some gateways (such as the former 1:1/31 IEEE fidonet.org gateway) to become unusable or cease operation entirely.
FidoNews is the newsletter of the FidoNet community.  Affectionately nicknamed The Snooze, it is published weekly.  It was first published in 1984.  Throughout its history, it has been published by various people and entities, including the short-lived International FidoNet Association.
"
"File:Pre-nodelist ""fido list"" June 1984.jpg - Wikipedia"," Original file ‎(1,256 × 1,670 pixels, file size: 255 KB, MIME type: image/jpeg)

digital photograph of my own work
 
https://creativecommons.org/licenses/by-sa/3.0
CC BY-SA 3.0 
Creative Commons Attribution-Share Alike 3.0 
truetrue
Click on a date/time to view the file as it appeared at that time.
This file contains additional information, probably added from the digital camera or scanner used to create or digitize it.

If the file has been modified from its original state, some details may not fully reflect the modified file."
FidoNet - Wikipedia," FidoNet is a worldwide computer network that is used for communication between bulletin board systems (BBSes). It uses a store-and-forward system to exchange private (email) and public (forum) messages between the BBSes in the network, as well as other files and protocols in some cases.
The FidoNet system was based on several small interacting programs, only one of which needed to be ported to support other BBS software. FidoNet was one of the few networks that was supported by almost all BBS software, as well as a number of non-BBS online services. This modular construction also allowed FidoNet to easily upgrade to new data compression systems, which was important in an era using modem-based communications over telephone links with high long-distance calling charges.
The rapid improvement in modem speeds during the early 1990s, combined with the rapid decrease in price of computer systems and storage, made BBSes increasingly popular. By the mid-1990s there were almost 40,000 FidoNet systems in operation, and it was possible to communicate with millions of users around the world. Only UUCPNET came close in terms of breadth or numbers; FidoNet's user base far surpassed other networks like BITNET.[1]
The broad availability of low-cost Internet connections starting in the mid-1990s lessened the need for FidoNet's store-and-forward system, as any system in the world could be reached for equal cost. Direct dialing into local BBS systems rapidly declined. Although FidoNet has shrunk considerably since the late 1990s, it has remained in use even today[2] despite internet connectivity becoming universally available.
There are two major accounts of the development of the FidoNet, differing only in small details.
Around Christmas 1983, Tom Jennings started work on a new MS-DOS–hosted bulletin board system that would emerge as Fido BBS. Jennings set up the system in San Francisco sometime in early 1984. Another early user was John Madil, who was trying to set up a similar system in Baltimore on his Rainbow 100. Fido started spreading to new systems, and Jennings eventually started keeping an informal list of their phone numbers, with Jennings becoming #1 and Madil #2.[3]
Jennings released the first version of the FidoNet software in June 1984. In early 1985 he wrote a document explaining the operations of the FidoNet, along with a short portion on the history of the system. In this version, FidoNet was developed as a way to exchange mail between the first two Fido BBS systems, Jennings' and Madil's, to ""see if it could be done, merely for the fun of it"". This was first supported in Fido V7, ""sometime in June 84 or so"".[4][5][6]
In early 1984, Ben Baker was planning on starting a BBS for the newly forming computer club at the McDonnell Douglas automotive division in St. Louis. Baker was part of the CP/M special interest group within the club.[7] He intended to use the seminal, CP/M-hosted, CBBS system, and went looking for a machine to run it on. The club's president told Baker that DEC would be giving them a Rainbow 100 computer on indefinite loan, so he made plans to move the CBBS onto this machine. The Rainbow contained two processors, an Intel 8088 and a Zilog Z80, allowing it to run both MS-DOS and CP/M, with the BBS running on the latter. When the machine arrived, they learned that the Z80 side had no access to the I/O ports, so CBBS could not communicate with a modem. While searching for software that would run on the MS-DOS side of the system, Baker learned of Fido through Madil.[3]
The Fido software required changes to the serial drivers to work properly on the Rainbow. A porting effort started, involving Jennings, Madil and Baker. This caused all involved to rack up considerable long distance charges as they all called each other during development, or called into each other's BBSes to leave email. During one such call ""in May or early June"", Baker and Jennings discussed how great it would be if the BBS systems could call each other automatically, exchanging mail and files between them.[3] This would allow them to compose mail on their local machines, and then deliver it quickly, as opposed to calling in and typing the message in while on a long-distance telephone connection.[3]
Jennings responded by calling into Baker's system that night and uploading a new version of the software consisting of three files: FIDO_DECV6, a new version of the BBS program itself, FIDONET, a new program, and NODELIST.BBS, a text file. The new version of FIDO BBS had a timer that caused it to exit at a specified time, normally at night. As it exited it would run the separate FIDONET program. NODELIST was the list of Fido BBS systems, which Jennings had already been compiling.[3]
The FIDONET program was what later became known as a mailer. The FIDO BBS software was modified to use a previously unused numeric field in the message headers to store a node number for the machine the message should be delivered to. When FIDONET ran, it would search through the email database for any messages with a number in this field. FIDONET collected all of the messages for a particular node number into a file known as a message packet. After all the packets were generated, one for each node, the FIDONET program would look up the destination node's phone number in NODELIST.BBS, and call the remote system. Provided that FIDONET was running on that system, the two systems would handshake and, if this succeeded, the calling system would upload its packet, download a return packet if there was one, and disconnect. FIDONET would then unpack the return packet, place the received messages into the local system's database, and move onto the next packet. When there were no remaining packets, FIDONET would exit, and run the FIDO BBS program.[8]
In order to lower long-distance charges, the mail exchanges were timed to run late at night, normally 4 AM.[4] This would later be known as national mail hour,[9] and, later still, as Zone Mail Hour.
By June 1984 Version 7 of the system was being run in production, and nodes were rapidly being added to the network. By August there were almost 30 systems in the nodelist, 50 by September, and over 160 by January 1985. As the network grew, the maintenance of the nodelist became prohibitive, and errors were common. In these cases, people would start receiving phone calls at 4 AM, from a caller that would say nothing and then hang up. In other cases the system would be listed before it was up and running, resulting in long-distance calls that accomplished nothing.[4]
In August 1984 Jennings handed off control of the nodelist to the group in St. Louis, mostly Ken Kaplan and Ben Baker. Kaplan had come across Fido as part of finding a BBS solution for his company, which worked with DEC computers and had been given a Rainbow computer and a USRobotics 1200bit/s modem.[10] From then on, joining FidoNet required one to set up their system and use it to deliver a netmail message to a special system, Node 51. The message contained various required contact information. If this message was transmitted successfully, it ensured that at least some of the system was working properly. The nodelist team would then reply with another netmail message back to the system in question, containing the assigned node number. If delivery succeeded, the system was considered to be working properly, and it was added to the nodelist.[4] The first new nodelist was published on 21 September 1984.[3]
Growth continued to accelerate, and by the spring of 1985, the system was already reaching its limit of 250 nodes. In addition to the limits on the growth of what was clearly a popular system, nodelist maintenance continued to grow more and more time-consuming.[3]
It was also realized that Fido systems were generally clustered – of the 15 systems running by the start of June 1984, 5 of them were in St. Louis.[3]  A user on Jennings's system in San Francisco that addressed emails to different systems in St. Louis would cause calls to be made to each of those BBSes in turn. In the United States, local calls were normally free, and in most other countries were charged at a low rate. Additionally, the initial call setup, generally the first minute of the call, was normally billed at a higher rate than continuing an existing connection. Therefore, it would be less expensive to deliver all the messages from all the users in San Francisco to all of the users in St. Louis in a single call. Packets were generally small enough to be delivered within a minute or two, so delivering all the messages in a single call could greatly reduce costs by avoiding multiple first-minute charges. Once delivered, the packet would be broken out into separate packets for local systems, and delivered using multiple local free calls.
The team settled on the concept of adding a new network number patterned on the idea of area codes.[N 1] A complete network address would now consist of the network and node number pair, which would be written with a slash between them. All mail travelling between networks would first be sent to their local network host, someone who volunteered to pay for any long distance charges. That single site would collect up all the netmail from all of the systems in their network, then re-package it into single packets destined to each network. They would then call any required network admin sites and deliver the packet to them. That site would then process the mail as normal, although all of the messages in the packet would be guaranteed to be local calls.[3]
The network address was placed in an unused field in the Fido message database, which formerly always held a zero. Systems running existing versions of the software already ignored the fields containing the new addressing, so they would continue to work as before; when noticing a message addressed to another node they would look it up and call that system. Newer systems would recognize the network number and instead deliver that message to the network host. To ensure backward compatibility, existing systems retained their original node numbers through this period.[3]
A huge advantage of the new scheme was that node numbers were now unique only within their network, not globally. This meant the previous 250 node limit was gone, but for a variety of reasons this was initially limited to about 1,200. This change also devolved the maintenance of the nodelists down to the network hosts, who then sent updated lists back to Node 51 to be collected into the master list. The St. Louis group now had to only maintain their own local network, and do basic work to compile the global list.[3]
At a meeting held in Kaplan's living room in St. Louis on 11 April 1985[N 2] the various parties hammered out all of the details of the new concept. As part of this meeting, they also added the concept of a region, a purely administrative level that was not part of the addressing scheme. Regional hosts would handle any stragglers in the network maps, remote systems that had no local network hosts. They then divided up the US into ten regions that they felt would have roughly equal populations.[3]
By May, Jennings had early versions of the new software running. These early versions specified the routing manually through a new ROUTE.BBS file that listed network hosts for each node. For instance, an operator might want to forward all mail to St. Louis through a single node, node 10. ROUTE.BBS would then include a list of all the known systems in that area, with instructions to forward mail to each of those nodes through node 10. This process was later semi-automated by John Warren's NODELIST program.[11] Over time, this information was folded into updated versions of the nodelist format, and the ROUTES file is no longer used.[12]
A new version of FIDO and FIDONET, 10C, was released containing all of these features. On 12 June 1985 the core group brought up 10C, and most Fido systems had upgraded within a few months.[11] The process went much smoother than anyone imagined, and very few nodes had any problems.[3]
Sometime during the evolution of Fido, file attachments were added to the system, allowing a file to be referenced from an email message. During the normal exchange between two instances of FIDONET, any files attached to the messages in the packets were delivered after the packet itself had been up or downloaded. It is not clear when this was added, but it was already a feature of the basic system when the 8 February 1985 version of the FidoNet standards document was released, so this was added very early in Fido's history.
At a sysop meeting in Dallas, the idea was raised that it would be nice if there was some way for the sysops to post messages that would be shared among the systems.[13] In February 1986 Jeff Rush, one of the group members, introduced a new mailer that extracted messages from public forums that the sysop selected, like the way the original mailer handled private messages. The new program was known as a tosser/scanner. The tosser produced a file that was similar (or identical) to the output from the normal netmail scan, however, these files were then compressed and attached to a normal netmail message as an attachment. This message was then sent to a special address on the remote system. After receiving netmail as normal, the scanner on the remote system looked for these messages, unpacked them, and put them into the same public forum on the original system.[9]
In this fashion, Rush's system implemented a store and forward public message system similar to Usenet, but based on, and hosted by, the FidoNet system. The first such echomail forum was one created by the Dallas area sysops to discuss business, known as SYSOP. Another called TECH soon followed. Several public echos soon followed, including GAYNET and CLANG. These spawned hundreds of new echos, and led to the creation of the Echomail Conference List (Echolist) by Thomas Kenny in January 1987.[14] Echomail produced world-spanning shared forums, and its traffic volume quickly surpassed the original netmail system. By the early 1990s, echo mail was carrying over 8 MB of compressed message traffic a day, many times that when uncompressed.[9]
Echomail did not necessarily use the same distribution pathways as normal netmail, and the distribution routing was stored in a separate setup file not unlike the original ROUTES.BBS. At the originating site a header line was added to the message indicating the origin system's name and address. After that, each system that the message traveled through added itself to a growing PATH header, as well as a SEENBY header. SEENBY prevented the message from looping around the network in the case of misconfigured routing information.[9]
Echomail was not the only system to use the file attachment feature of netmail to implement store-and-forward capabilities. Similar concepts were used by online games and other systems as well.
The evolution towards the net/node addressing scheme was also useful for reducing communications costs between continents, where time zone differences on either end of the connection might also come into play. For instance, the best time to forward mail in the US was at night, but that might not be the best time for European hosts to exchange. Efforts towards introducing a continental level to the addressing system started in 1986.[9]
At the same time, it was noted that some power users were interested in using FidoNet protocols as a way of delivering the large quantities of echomail to their local machines where it could be read offline. These users did not want their systems to appear in the nodelist - they did not (necessarily) run a bulletin board system and were not publicly accessible.[9] A mechanism allowing netmail delivery to these systems without the overhead of nodelist maintenance was desirable.
In October 1986 the last major change to the FidoNet network was released, adding zones and points. Zones represented major geographical areas roughly corresponding to continents. There were six zones in total, North America, South America, Europe, Oceania, Asia, and Africa. Points represented non-public nodes, which were created privately on a BBS system. Point mail was delivered to a selected host BBS as normal, but then re-packaged into a packet for the point to pick up on-demand. The complete addressing format was now zone:net/node.point, so a real example might be Bob Smith@1:250/250.10.[9] Points were widely used only for a short time, the introduction of offline reader systems filled this role with systems that were much easier to use. Points remain in use to this day but are less popular than when they were introduced.
Although FidoNet supported file attachments from even the earliest standards, this feature tended to be rarely used and was often turned off. File attachments followed the normal mail routing through multiple systems and could back up transfers all along the line as the files were copied. A solution was offered in the form of file requests, which made file transfers driven by the calling system and used one-time point-to-point connections instead of the traditional routing. Two such standards became common, ""WaZOO"" and ""Bark"", which saw varying support among different mailers. Both worked similarly, with the mailer calling the remote system and sending a new handshake packet to request the files.[15][16]
Although FidoNet was, by far, the best known BBS-based network, it was by no means the only one. From 1988 on, PCBoard systems were able to host similar functionality known as RelayNet, while other popular networks included RBBSNet from the Commodore 64 world, and AlterNet. Late in the evolution of the FidoNet system, there was a proposal to allow mail (but not forum messages) from these systems to switch into the FidoNet structure.[17] This was not adopted, and the rapid rise of the internet made this superfluous as these networks rapidly added internet exchange, which acted as a lingua franca.
FidoNet started in 1984 and listed 100 nodes by the end of that year. Steady growth continued through the 1980s, but a combination of factors led to rapid growth after 1988. These included faster and less expensive modems and rapidly declining costs of hard drives and computer systems in general. By April 1993, the FidoNet nodelist contained over 20,000 systems. At that time it was estimated that each node had, on average, about 200 active users. Of these 4 million users in total, 2 million users commonly used echomail, the shared public forums, while about 200,000 used the private netmail system.[9] At its peak, FidoNet listed approximately 39,000 systems.[4][N 3]
Throughout its lifetime, FidoNet was beset with management problems and infighting. Much of this can be traced to the fact that the inter-net delivery cost real money, and the traffic grew more rapidly than decreases caused by improving modem speeds and downward trending long-distance rates. As they increased, various methods of recouping the costs were attempted, all of which caused friction in the groups. The problems were so bad that Jennings came to refer to the system as the ""fight-o-net"".[18]
As modems reached speeds of 28.8 kbit/s, the overhead of the TCP/IP protocols were no longer so egregious and dial-up Internet became increasingly common. By 1995, the bulletin board market was reeling as users abandoned local BBS systems in favour of larger sites and web pages, which could be accessed worldwide for the same cost as accessing a local BBS system. This also made FidoNet less expensive to implement, because inter-net transfers could be delivered over the Internet as well, at little or no marginal cost. But this seriously diluted the entire purpose of the store-and-forward model, which had been built up specifically to address a long-distance problem that no longer existed.
The FidoNet nodelist started shrinking, especially in areas with a widespread availability of internet connections. This downward trend continues but has levelled out at approximately 2,500 nodes.[N 4] FidoNet remains popular in areas where Internet access is difficult to come by, or expensive.
There is now (~2014) a retro movement which is resulting in a slow increase in internet-connected BBS and nodes. Telnet, Rlogin, and SSH are being used between systems. This means the user can telnet to any BBS worldwide as cheaply as ones next door. Also, Usenet and internet mail has been added, along with long file names to many newer versions of BBS software, some being free-ware, resulting in increasing use. Nodelists are no longer declining in all cases.
FidoNet is governed in a hierarchical structure according to FidoNet policy, with designated coordinators at each level to manage the administration of FidoNet nodes and resolve disputes between members.[1] This structure is very similar to the organization structure of the Sicilian Mafia. Network coordinators (referred to as ""Button Men"") are responsible for managing the individual nodes within their area, usually a city or similar sized area. Regional coordinators (referred to as ""Underbosses"") are responsible for managing the administration of the network coordinators within their region, typically the size of a state, or small country. Zone coordinators (referred to as either ""Dons"" or ""Godfathers"") are responsible for managing the administration of all of the regions within their zone. The world is divided into six zones, the coordinators of which appoint themselves or representatives to the positions of ""International Coordinators"" of FidoNet (referred to as ""La Cosa Nostra""). The six zone ""International Coordinators"", along with their Counselors (also known as their ""Consiglieres""), form the twelve person body known as ""FidoNet Central"".
FidoNet was historically designed to use modem-based dial-up (POTS) access between bulletin board systems, and much of its policy and structure reflected this.
The FidoNet system officially referred only to the transfer of Netmail—the individual private messages between people using bulletin boards—including the protocols and standards with which to support it. A netmail message would contain the name of the person sending, the name of the intended recipient, and the respective FidoNet addresses of each. The FidoNet system was responsible for routing the message from one system to the other (details below), with the bulletin board software on each end being responsible for ensuring that only the intended recipient could read it. Due to the hobbyist nature of the network, any privacy between the sender and recipient was only the result of politeness from the owners of the FidoNet systems involved in the mail's transfer. It was common, however, for system operators to reserve the right to review the content of mail that passed through their system.
Netmail allowed for the attachment of a single file to every message. This led to a series of piggyback protocols that built additional features onto FidoNet by passing information back and forth as file attachments. These included the automated distribution of files and transmission of data for inter-BBS games.
By far the most commonly used of these piggyback protocols was Echomail, public discussions similar to Usenet newsgroups in nature. Echomail was supported by a variety of software that collected up new messages from the local BBSes' public forums (the scanner), compressed it using ARC or ZIP, attached the resulting archive to a Netmail message, and sent that message to a selected system. On receiving such a message, identified because it was addressed to a particular user, the reverse process was used to extract the messages, and a tosser put them back into the new system's forums.
Echomail was so popular that for many users, Echomail was the FidoNet. Private person-to-person Netmail was relatively rare.
FidoNet is politically organized into a tree structure, with different parts of the tree electing their respective coordinators. The FidoNet hierarchy consists of zones, regions, networks, nodes and points broken down more-or-less geographically.
The highest level is the zone, which is largely continent-based:
Each zone is broken down into regions, which are broken down into nets, which consist of individual nodes.  Zones 7-4095 are used for othernets; groupings of nodes that use Fido-compatible software to carry their own independent message areas without being in any way controlled by FidoNet's political structure. Using un-used zone numbers would ensure that each network would have a unique set of addresses, avoiding potential routing conflicts and ambiguities for systems that belonged to more than one network.
FidoNet addresses explicitly consist of a zone number, a network number (or region number), and a node number. They are written in the form Zone:Network/Node.[20] The FidoNet structure also allows for semantic designation of region, host, and hub status for particular nodes, but this status is not directly indicated by the main address.
For example, consider a node located in Tulsa, Oklahoma, United States with an assigned node number is 918, located in Zone 1 (North America), Region 19, and Network 170. The full FidoNet address for this system would be 1:170/918.  The region was used for administrative purposes, and was only part of the address if the node was listed directly underneath the Regional Coordinator, rather than one of the networks that were used to divide the region further.
FidoNet policy requires that each FidoNet system maintain a nodelist of every other member system. Information on each node includes the name of the system or BBS, the name of the node operator, the geographic location, the telephone number, and software capabilities. The nodelist is updated weekly, to avoid unwanted calls to nodes that had shut down, with their phone numbers possibly having been reassigned for voice use by the respective telephone company.
To accomplish regular updates, coordinators of each network maintain the list of systems in their local areas. The lists are forwarded back to the International Coordinator via automated systems on a regular basis. The International Coordinator would then compile a new nodelist, and generate the list of changes (nodediff) to be distributed for node operators to apply to their existing nodelist.
In a theoretical situation, a node would normally forward messages to a hub. The hub, acting as a distribution point for mail, might then send the message to the Net Coordinator. From there it may be sent through a Regional Coordinator, or to some other system specifically set up for the function. Mail to other zones might be sent through a Zone Gate.
For example, a FidoNet message might follow the path:
Originally there was no specific relationship between network numbers and the regions they reside in. In some areas of FidoNet, most notably in Zone 2, the relationship between region number and network number are entwined. For example, 2:201/329 is in Net 201 which is in Region 20 while 2:2410/330 is in Net 2410 which is in Region 24. Zone 2 also relates the node number to the hub number if the network is large enough to contain any hubs. This effect may be seen in the nodelist by looking at the structure of Net 2410 where node 2:2410/330 is listed under Hub 300. This is not the case in other zones.
In Zone 1, things are much different. Zone 1 was the starting point and when Zones and Regions were formed, the existing nets were divided up regionally with no set formula. The only consideration taken was where they were located geographically with respect to the region's mapped outline. As net numbers got added, the following formula was used.
Region number × 20
Then when some regions started running out of network numbers, the following was also used.
Region number × 200
Region 19, for instance, contains nets 380-399 and 3800-3999 in addition to those that were in Region 19 when it was formed.
Part of the objective behind the formation of local nets was to implement cost reduction plans by which all messages would be sent to one or more hubs or hosts in compressed form (ARC was nominally standard, but PKZIP is universally supported); one toll call could then be made during off-peak hours to exchange entire message-filled archives with an out-of-town uplink for further redistribution.
In practice, the FidoNet structure allows for any node to connect directly to any other, and node operators would sometimes form their own toll-calling arrangements on an ad-hoc basis, allowing for a balance between collective cost saving and timely delivery. For instance, if one node operator in a network offered to make regular toll calls to a particular system elsewhere, other operators might arrange to forward all of their mail destined for the remote system, and those near it, to the local volunteer.  Operators within individual networks would sometimes have cost-sharing arrangements, but it was also common for people to volunteer to pay for regular toll calls either out of generosity or to build their status in the community.
This ad-hoc system was particularly popular with networks that were built on top of FidoNet. Echomail, for instance, often involved relatively large file transfers due to its popularity. If official FidoNet distributors refused to transfer Echomail due to additional toll charges, other node operators would sometimes volunteer. In such cases, Echomail messages would be routed to the volunteers' systems instead.
The FidoNet system was best adapted to an environment in which local telephone service was inexpensive and long-distance calls (or intercity data transfer via packet-switched networks) costly. Therefore, it fared somewhat poorly in Japan, where even local lines are expensive, or in France, where tolls on local calls and competition with Minitel or other data networks limited its growth.
As the number of messages in Echomail grew over time, it became very difficult for users to keep up with the volume while logged into their local BBS. Points were introduced to address this, allowing technically-savvy users to receive the already compressed and batched Echomail (and Netmail) and read it locally on their own machines.[21]
To do this, the FidoNet addressing scheme was extended with the addition of a final address segment, the point number. For instance, a user on the example system above might be given point number 10, and thus could be sent mail at the address 1:170/918.10.
In real-world use, points are fairly difficult to set up. The FidoNet software typically consisted of a number of small utility programs run by manually edited scripts that required some level of technical ability. Reading and editing the mail required either a ""sysop editor"" program or a BBS program to be run locally.
In North America (Zone 1), where local calls are generally free, the benefits of the system were offset by its complexity. Points were used only briefly, and even then only to a limited degree. Dedicated offline mail reader programs such as Blue Wave, Squiggy and Silver Xpress (OPX) were introduced in the mid-1990s and quickly rendered the point system obsolete. Many of these packages supported the QWK offline mail standard.
In other parts of the world, especially Europe, this was different. In Europe, even local calls are generally metered, so there was a strong incentive to keep the duration of the calls as short as possible. Point software employs standard compression (ZIP, ARJ, etc.) and so keeps the calls down to a few minutes a day at most. In contrast to North America, pointing saw rapid and fairly widespread uptake in Europe.
Many regions distribute a pointlist in parallel with the nodelist. The pointlist segments are maintained by Net- and Region Pointlist Keepers and the Zone Point List Keeper assembles them into the Zone pointlist. At the peak of FidoNet there were over 120,000 points listed in the Zone 2 pointlist. Listing points is on a voluntary basis and not every point is listed, so how many points there really were is anybody's guess. As of June 2006, there are still some 50,000 listed points. Most of them are in Russia and Ukraine.
FidoNet contained several technical specifications for compatibility between systems.  The most basic of all is FTS-0001,[22] with which all FidoNet systems are required to comply as a minimum requirement.  FTS-0001 defined:
Other specifications that were commonly used provided for echomail, different transfer protocols and handshake methods (e.g.: Yoohoo/Yoohoo2u2, EMSI), file compression, nodelist format, transfer over reliable connections such as the Internet (Binkp), and other aspects.
Since computer bulletin boards historically used the same telephone lines for transferring mail as were used for dial-in human users of the BBS, FidoNet policy dictates that at least one designated line of each FidoNet node must be available for accepting mail from other FidoNet nodes during a particular hour of each day.[23]
Zone Mail Hour, as it was named, varies depending on the geographic location of the node, and was designated to occur during the early morning.  The exact hour varies depending on the time zone, and any node with only one telephone line is required to reject human callers. In practice, particularly in later times, most FidoNet systems tend to accept mail at any time of day when the phone line is not busy, usually during night.
Most FidoNet deployments were designed in a modular fashion.  A typical deployment would involve several applications that would communicate through shared files and directories, and switch between each other through carefully designed scripts or batch files.  However, monolithic software that encompassed all required functions in one package is available, such as D'Bridge.  Such software eliminated the need for custom batch files and is tightly integrated in operation.  The preference for deployment was that of the operator and there were both pros and cons of running in either fashion.
Arguably the most important piece of software on a DOS-based Fido system was the  FOSSIL driver, which was a small device driver which provided a standard way for the Fido software to talk to the modem.[24]  This driver needed to be loaded before any Fido software would work.  An efficient FOSSIL driver meant faster, more reliable connections.
Mailer software was responsible for transferring files and messages between systems, as well as passing control to other applications, such as the BBS software, at appropriate times.  The mailer would initially answer the phone and, if necessary, deal with incoming mail via FidoNet transfer protocols.  If the mailer answered the phone and a human caller was detected rather than other mailer software, the mailer would exit, and pass control to the BBS software, which would then initialise for interaction with the user.  When outgoing mail was waiting on the local system, the mailer software would attempt to send it from time to time by dialing and connecting to other systems who would accept and route the mail further.  Due to the costs of toll calls which often varied between peak and off-peak times, mailer software would usually allow its operator to configure the optimal times in which to attempt to send mail to other systems.
BBS software was used to interact with human callers to the system.  BBS software would allow dial-in users to use the system's message bases and write mail to others, locally or on other BBSes.  Mail directed to other BBSes would later be routed and sent by the mailer, usually after the user had finished using the system.  Many BBSes also allowed users to exchange files, play games, and interact with other users in a variety of ways (i.e.: node to node chat).
A scanner/tosser application, such as FastEcho, FMail, TosScan and Squish, would normally be invoked when a BBS user had entered a new FidoNet message that needed to be sent, or when a mailer had received new mail to be imported into the local messages bases.  This application would be responsible for handling the packaging of incoming and outgoing mail, moving it between the local system's message bases and the mailer's inbound and outbound directories.  The scanner/tosser application would generally be responsible for basic routing information, determining which systems to forward mail to.
In later times, message readers or editors that were independent of BBS software were also developed.  Often the System Operator of a particular BBS would use a devoted message reader, rather than the BBS software itself, to read and write FidoNet and related messages. One of the most popular editors in 2008 was GoldED+.  In some cases, FidoNet nodes, or more often FidoNet points, had no public bulletin board attached and existed only for the transfer of mail for the benefit of the node's operator. Most nodes in 2009 had no BBS access, but only points, if anything.
The original Fido BBS software, and some other FidoNet-supporting software from the 1980s, is no longer functional on modern systems.  This is for several reasons, including problems related to the Y2K bug.  In some cases, the original authors have left the BBS or shareware community, and the software, much of which was closed source, has been rendered abandonware.
Several DOS-based legacy FidoNet Mailers such as FrontDoor, Intermail, MainDoor and D'Bridge from the early 1990s can still be run today under Windows without a modem, by using the freeware NetFoss Telnet FOSSIL driver, and by using a Virtual Modem such as NetSerial. This allows the mailer to dial an IP address or hostname via Telnet, rather than dialing a real POTS phone number. There are similar solutions for Linux such as MODEMU (modem emulator) which has limited success when combined with DOSEMU (DOS emulator).
Mail Tossers such as FastEcho and FMail are still used today under both Windows and Linux/DOSEMU.
There are several modern Windows based FidoNet Mailers available today with source code, including Argus, Radius, and Taurus. MainDoor is another Windows based Fidonet mailer, which also can be run using either a modem or directly over TCP/IP. Two popular free and open source software FidoNet mailers for Unix-like systems are the binkd (cross-platform, IP-only, uses the binkp protocol) and qico (supports modem communication as well as the IP protocol of ifcico and binkp).
On the hardware side, Fido systems were usually well-equipped machines, for their day, with quick CPUs, high-speed modems and 16550 UARTs, which were at the time an upgrade.  As a Fidonet system was usually a BBS, it needed to quickly process any new mail events before returning to its 'waiting for call' state. In addition, the BBS itself usually necessitated lots of storage space.  Finally, a FidoNet system usually had at least one dedicated phone line.  Consequently, operating a Fidonet system often required significant financial investment, a cost usually met by the owner of the system.
While the use of FidoNet has dropped dramatically compared with its use up to the mid-1990s, it is still used in many countries and especially Russia and former republics of the USSR.[citation needed]  Some BBSes, including those that are now available for users with Internet connections via telnet, also retain their FidoNet netmail and echomail feeds.
Some of FidoNet's echomail conferences are available via gateways with the Usenet news hierarchy using software like UFGate.  There are also mail gates for exchanging messages between Internet and FidoNet.  Widespread net abuse and e-mail spam on the Internet side has caused some gateways (such as the former 1:1/31 IEEE fidonet.org gateway) to become unusable or cease operation entirely.
FidoNews is the newsletter of the FidoNet community.  Affectionately nicknamed The Snooze, it is published weekly.  It was first published in 1984.  Throughout its history, it has been published by various people and entities, including the short-lived International FidoNet Association.
"
Email - Wikipedia," 
Electronic mail (email or e-mail) is a method of exchanging messages (""mail"") between people using electronic devices. Email entered limited use in the 1960s, but users could only send to users of the same computer, and some early email systems required the author and the recipient to both be online simultaneously, similar to instant messaging. Ray Tomlinson is credited as the inventor of email; in 1971, he developed the first system able to send mail between users on different hosts across the ARPANET, using the @ sign to link the user name with a destination server. By the mid-1970s, this was the form recognized as email.
Email operates across computer networks, primarily the Internet. Today's email systems are based on a store-and-forward model. Email servers accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need to connect, typically to a mail server or a webmail interface to send or receive messages or download it.
Originally an ASCII text-only communications medium, Internet email was extended by Multipurpose Internet Mail Extensions (MIME) to carry text in other character sets and multimedia content attachments. International email, with internationalized email addresses using UTF-8, is standardized but not widely adopted.[2]
The history of modern Internet email services reaches back to the early ARPANET, with standards for encoding email messages published as early as 1973 (RFC 561). An email message sent in the early 1970s is similar to a basic email sent today. 
Historically, the term electronic mail is any electronic document transmission. For example, several writers in the early 1970s used the term to refer to fax document transmission.[3][4] As a result, finding its first use is difficult with the specific meaning it has today.
The term electronic mail has been in use with its current meaning since at least 1975, and variations of the shorter E-mail have been in use since at least 1979:[5][6]
In the original protocol, RFC 524, none of these forms was used. The service is simply referred to as mail, and a single piece of electronic mail is called a message.
An Internet e-mail consists of an envelope and content;[21] the content consists of a header and a body.[22]
Computer-based mail and messaging became possible with the advent of time-sharing computers in the early 1960s, and informal methods of using shared files to pass messages were soon expanded into the first mail systems. Most developers of early mainframes and minicomputers developed similar, but generally incompatible, mail applications. Over time, a complex web of gateways and routing systems linked many of them. Many US universities were part of the ARPANET (created in the late 1960s), which aimed at software portability between its systems.  In 1971 the first ARPANET network email was sent, introducing the now-familiar address syntax with the '@' symbol designating the user's system address.[23] The Simple Mail Transfer Protocol (SMTP) protocol was introduced in 1981.
For a time in the late 1980s and early 1990s, it seemed likely that either a proprietary commercial system or the X.400 email system, part of the Government Open Systems Interconnection Profile (GOSIP), would predominate.[nb 1] However, once the final restrictions on carrying commercial traffic over the Internet ended in 1995,[24][25] a combination of factors made the current Internet suite of SMTP, POP3 and IMAP email protocols the standard.
The following is a typical sequence of events that takes place when sender Alice transmits a message using a mail user agent (MUA) addressed to the email address of the recipient.[26]
In addition to this example, alternatives and complications exist in the email system:
Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called open mail relays. This was very important in the early days of the Internet when network connections were unreliable.[28][29]  However, this mechanism proved to be exploitable by originators of unsolicited bulk email and as a consequence open mail relays have become rare,[30] and many MTAs do not accept messages from open mail relays.
The basic Internet message format used for email[31] is defined by RFC 5322, with encoding of non-ASCII data and multimedia content attachments defined in RFC 2045 through RFC 2049, collectively called Multipurpose Internet Mail Extensions or MIME. The extensions in International email apply only to email. RFC 5322 replaced the earlier RFC 2822 in 2008, then RFC 2822 in 2001 replaced RFC 822 – the standard for Internet email for decades. Published in 1982, RFC 822 was based on the earlier RFC 733 for the ARPANET.[32]
Internet email messages consist of two sections, 'header' and 'body'. These are known as 'content'.[33][34]  
The header is structured into fields such as From, To, CC, Subject, Date, and other information about the email. In the process of transporting email messages between systems, SMTP communicates delivery parameters and information using message header fields. The body contains the message, as unstructured text, sometimes containing a signature block at the end. The header is separated from the body by a blank line.
RFC 5322 specifies the syntax of the email header. Each email message has a header (the ""header section"" of the message, according to the specification), comprising a number of fields (""header fields""). Each field has a name (""field name"" or ""header field name""), followed by the separator character "":"", and a value (""field body"" or ""header field body"").
Each field name begins in the first character of a new line in the header section, and begins with a non-whitespace printable character. It ends with the separator character "":"". The separator follows the field value (the ""field body""). The value can continue onto subsequent lines if those lines have space or tab as their first character. Field names and, without SMTPUTF8, field bodies are restricted to 7-bit ASCII characters. Some non-ASCII values may be represented using MIME encoded words.
Email header fields can be multi-line, with each line recommended to be no more than 78 characters, although the limit is 998 characters.[35] Header fields defined by RFC 5322 contain only US-ASCII characters; for encoding characters in other sets, a syntax specified in RFC 2047 may be used.[36] In some examples, the IETF EAI working group defines some standards track extensions,[37][38] replacing previous experimental extensions so UTF-8 encoded Unicode characters may be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some government agents.[39]
The message header must include at least the following fields:[40][41]
RFC 3864 describes registration procedures for message header fields at the IANA; it provides for permanent and provisional field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:[42]
The To: field may be unrelated to the addresses to which the message is delivered. The delivery list is supplied separately to the transport protocol, SMTP, which may be extracted from the header content. The ""To:"" field is similar to the addressing at the top of a conventional letter delivered according to the address on the outer envelope. In the same way, the ""From:"" field may not be the sender. Some mail servers apply email authentication systems to messages relayed. Data pertaining to the server's activity is also part of the header, as defined below.
SMTP defines the trace information of a message saved in the header using the following two fields:[44]
Other fields added on top of the header by the receiving server may be called trace fields.[45]
Internet email was designed for 7-bit ASCII.[51] Most email software is 8-bit clean, but must assume it will communicate with 7-bit servers and mail readers. The MIME standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: quoted printable for mostly 7-bit content with a few characters outside that range and base64 for arbitrary binary data. The 8BITMIME and BINARY extensions were introduced to allow transmission of mail without the need for these encodings, but many mail transport agents may not support them. In some countries, several encoding schemes co-exist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is a coincidence if the sender and receiver use the same encoding scheme). Therefore, for international character sets, Unicode is growing in popularity.[citation needed]
Most modern graphic email clients allow the use of either plain text or HTML for the message body at the option of the user. HTML email messages often include an automatic-generated plain text copy for compatibility. Advantages of HTML include the ability to include in-line links and images, set apart previous messages in block quotes, wrap naturally on any display, use emphasis such as underlines and italics, and change font styles. Disadvantages include the increased size of the email, privacy concerns about web bugs, abuse of HTML email as a vector for phishing attacks and the spread of malicious software.[52]
Some web-based mailing lists recommend all posts be made in plain-text, with 72 or 80 characters per line for all the above reasons,[53][54] and because they have a significant number of readers using text-based email clients such as Mutt. Some Microsoft email clients may allow rich formatting using their proprietary Rich Text Format (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible email client.[55]
Messages are exchanged between hosts using the Simple Mail Transfer Protocol with software programs called mail transfer agents (MTAs); and delivered to a mail store by programs called mail delivery agents (MDAs, also sometimes called local delivery agents, LDAs). Accepting a message obliges an MTA to deliver it,[56] and when a message cannot be delivered, that MTA must send a bounce message back to the sender, indicating the problem.
Users can retrieve their messages from servers using standard protocols such as POP or IMAP, or, as is more likely in a large corporate environment, with a proprietary protocol specific to Novell Groupwise, Lotus Notes or Microsoft Exchange Servers.  Programs used by users for retrieving, reading, and managing email are called mail user agents (MUAs).
Mail can be stored on the client, on the server side, or in both places. Standard formats for mailboxes include Maildir and mbox. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as IMAP, moving email from one server to another can be done with any MUA supporting the protocol.
Many current email users do not run MTA, MDA or MUA programs themselves, but use a web-based email platform, such as Gmail or Yahoo! Mail, that performs the same tasks.[57] Such webmail interfaces allow users to access their mail with any standard web browser, from any computer, rather than relying on a local email client.
Upon reception of email messages, email client applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the mbox format. The specific format used is often indicated by special filename extensions:
Some applications (like Apple Mail) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.
The URI scheme, as registered with the IANA, defines the mailto: scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the To: field.[58][59] Many clients also support query string parameters for the other email fields, such as its subject line or carbon copy recipients.[60]
Many email providers have a web-based email client (e.g. AOL Mail, Gmail, Outlook.com and Yahoo! Mail). This allows users to log into the email account by using any compatible web browser to send and receive their email. Mail is typically not downloaded to the web client, so can't be read without a current Internet connection.
The Post Office Protocol 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the server. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's).[61]POP3 allows you to download email messages on your local computer and read them even when you are offline.[62][63]
The Internet Message Access Protocol (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like smartphones are increasingly used to check email while traveling and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually, the mail is left in folders in the mail server.
Messaging Application Programming Interface (MAPI) is used by Microsoft Outlook to communicate to Microsoft Exchange Server - and to a range of other email server products such as Axigen Mail Server, Kerio Connect, Scalix, Zimbra, HP OpenMail, IBM Lotus Notes, Zarafa, and Bynari where vendors have added MAPI support to allow their products to be accessed directly via Outlook.
Email has been widely accepted by businesses, governments and non-governmental organizations in the developed world, and it is one of the key parts of an 'e-revolution' in workplace communication (with the other key plank being widespread adoption of highspeed Internet). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.[64]
It has some key benefits to business and other organizations, including:
Email marketing via ""opt-in"" is often successfully used to send special sales offerings and new product information.[65] Depending on the recipient's culture,[66] email sent without permission—such as an ""opt-in""—is likely to be viewed as unwelcome ""email spam"".
Many users access their personal emails from friends and family members using a personal computer in their house or apartment.
Email has become used on smartphones and on all types of computers. Mobile ""apps"" for email increase accessibility to the medium for users who are out of their homes. While in the earliest years of email, users could only access email on desktop computers, in the 2010s, it is possible for users to check their email when they are away from home, whether they are across town or across the world. Alerts can also be sent to the smartphone or other devices to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. As of 2011[update], there were approximately 1.4 billion email users worldwide and 50 billion non-spam emails that were sent daily.[59]
Individuals often check emails on smartphones for both personal and work-related messages. It was found that US adults check their email more than they browse the web or check their Facebook accounts, making email the most popular activity for users to do on their smartphones. 78% of the respondents in the study revealed that they check their email on their phone.[67] It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on a smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.[68]
As of 2010[update], the number of Americans visiting email web sites had fallen 6 percent after peaking in November 2009. For persons 12 to 17, the number was down 18 percent. Young people preferred instant messaging, texting and social media. Technology writer Matt Richtel said in The New York Times that email was like the VCR, vinyl records and film cameras—no longer cool and something older people do.[69][70]
A 2015 survey of Android users showed that persons 13 to 24 used messaging apps 3.5 times as much as those over 45, and were far less likely to use email.[71]
Email messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include Microsoft Word documents, PDF documents and scanned images of paper documents. In principle there is no technical restriction on the size or number of attachments, but in practice email clients, servers and Internet service providers implement various limitations on the size of files, or complete email - typically to 25MB or less.[72][73][74] Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ from what the user sees,[75] which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, various file hosting services are available and commonly used.[76][77]
The ubiquity of email for knowledge workers and ""white collar"" employees has led to concerns that recipients face an ""information overload"" in dealing with increasing volumes of email.[78][79] With the growth in mobile devices, by default employees may also receive work-related emails outside of their working day. This can lead to increased stress, decreased satisfaction with work, and some observers even argue it could have a significant negative economic effect,[80] as efforts to read the many emails could reduce productivity.
Email ""spam"" is unsolicited bulk email. The low cost of sending such email meant that, by 2003, up to 30% of total email traffic was spam,[81][82][83] and was threatening the usefulness of email as a practical tool. The US CAN-SPAM Act of 2003 and similar laws elsewhere[84] had some impact, and a number of effective anti-spam techniques now largely mitigate the impact of spam by filtering or rejecting it for most users,[85] but the volume sent is still very high—and increasingly consists not of advertisements for products, but malicious content or links.[86] In September 2017, for example, the proportion of spam to legitimate email rose to 59.56%.[87]
A range of malicious email types exist. These range from various types of email scams, including ""social engineering"" scams such as advance-fee scam ""Nigerian letters"", to phishing, email bombardment and email worms.
Email spoofing occurs when the email message header is designed to make the message appear to come from a known or trusted source. Email spam and phishing methods typically use spoofing to mislead the recipient about the true message origin. Email spoofing may be done as a prank, or as part of a criminal effort to defraud an individual or organization. An example of a potentially fraudulent email spoofing is if an individual creates an email that appears to be an invoice from a major company, and then sends it to one or more recipients. In some cases, these fraudulent emails incorporate the logo of the purported organization and even the email address may appear legitimate.
Email bombing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.
Today it can be important to distinguish between the Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although information technology personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.
Email privacy, without some security precautions, can be compromised because:
There are cryptography applications that can serve as a remedy to one or more of the above. For example, Virtual Private Networks or the Tor anonymity network can be used to encrypt traffic from the user machine to a safer network while GPG, PGP, SMEmail,[88] or S/MIME can be used for end-to-end message encryption, and SMTP STARTTLS or SMTP over Transport Layer Security/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.
Additionally, many mail user agents do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as SASL prevent this. Finally, the attached files share many of the same hazards as those found in peer-to-peer filesharing. Attached files may contain trojans or viruses.
Emails can now often be considered as binding contracts as well, so users must be careful about what they send through email correspondence.[89][90][91]
Flaming occurs when a person sends a message (or many messages) with angry or antagonistic content. The term is derived from the use of the word incendiary to describe particularly heated email discussions. The ease and impersonality of email communications mean that the social norms that encourage civility in person or via telephone do not exist and civility may be forgotten.[92]
Also known as ""email fatigue"", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a ""boilerplate"" message explaining that their email inbox is full, and that they are in the process of clearing out all the messages. Harvard University law professor Lawrence Lessig is credited with coining this term, but he may only have popularized it.[93]
Originally Internet email was completely ASCII text-based. MIME now allows body content text and some header content text in international character sets, but other headers and email addresses using UTF-8, while standardized[94] have yet to be widely adopted.[2][95]
The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the IETF introduced Delivery Status Notifications (delivery receipts) and Message Disposition Notifications (return receipts); however, these are not universally deployed in production.[nb 2]
Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:
In the absence of standard methods, a range of system based around the use of web bugs have been developed. However, these are often seen as underhand or raising privacy concerns,[98][99] and only work with email clients that support rendering of HTML. Many mail clients now default to not showing ""web content"".[100] Webmail providers can also disrupt web bugs by pre-caching images.[101]
"
Balsa (email client) - Wikipedia," Balsa is a lightweight email client written in C for the GNOME desktop environment.
Balsa has a graphical front end, support for MIME attachments coming and going, directly supports POP3 and IMAP protocols. It has a spell checker and direct support for PGP and GPG for encryption. It has some basic filtering capabilities, and natively supports several email storage protocols.[2] It also has some internationalization support, including Japanese fonts.
It builds on top of these other open source packages: GNOME, libtool, libESMTP, aspell, and gmime. It also can optionally use libgtkhtml for HTML rendering, libkrb5 for GSS-API, and openldap for LDAP functionality. It can optionally be configured to use gpg-error and gpgme libraries.
Balsa is packaged for a wide range of Linux distributions, including Arch Linux, Debian, Fedora, openSUSE, Slackware and Ubuntu, as well as for FreeBSD.[3]

"
Netscape Mail & Newsgroups - Wikipedia," 
Netscape Mail and Newsgroups, commonly known as just Netscape Mail, was an email and news client produced by Netscape Communications Corporation as part of the Netscape series of suites between versions 4.5 to 7.2. It was previously two separate programs known as  Netscape Messenger and Netscape Collabra between Netscape Communicator 4 and 4.5.
Netscape Mail & Newsgroups features support for relevant protocols such as IMAP, POP3 and SMTP, a built-in Bayesian spam filter, support for multiple accounts, etc.
Initially its development was overseen in-house, but following AOL's purchase of Netscape in 1998, its codebase development was handed over to the Mozilla Foundation, originally initiated by Netscape, and therefore became based upon the Mozilla Mail & Newsgroups component of the open-source Mozilla Application Suite. Mozilla ceased development of the suite between 2004 and 2006 in favour of stand-alone applications, and as a consequence Netscape's series of suites were also discontinued. In 2005, Netscape released Netscape Browser 8, based upon Mozilla Firefox, which did not include an email client, therefore the latest version inclusive of a mail client, version 7.2, became unsupported.
The development of Mozilla Mail and Newsgroups has now been continued as SeaMonkey Mail and Newsgroups.
In 2007, after the release of stand-alone browser Netscape Navigator 9, Netscape confirmed that it would once again develop an e-mail client, now named Netscape Messenger 9. The new release was to be based upon Mozilla Mail's successor, Mozilla Thunderbird.[1]
Additionally, after an official poll posted on Netscape's community support board in late 2006, speculation arose of the Netscape 7 series of suites (including the Mail client) being fully supported and updated by Netscape's in-house development team once more, including major bug fixes and security issues.[2][3][4]
However, development of Netscape Messenger (and any speculation of an update of Netscape 7) was ended when AOL announced they would end development and support of Netscape Navigator and Netscape Messenger.[5] Support ended on March 1, 2008.
"
