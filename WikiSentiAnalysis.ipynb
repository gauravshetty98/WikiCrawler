{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis is a NLP technique which helps in determining whether the given text data has a positive senitment or not. It can categorize a given text into three categories: Positive, Negative and Neutral. Here we make use of the nltk library for sentiment analysis. <br>\n",
    "We give each wikipedia page a sentiment score using sentiment analysis and then sort them according to those scores. This way we can see which are the top positive and negative pages from the list of website the crawler extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import all the necessary libraries required for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\gaura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gaura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\gaura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentimentAnalyzerScore() function is used to calculate the sentiment score of a given text passed to it. It returns the score of the text passed to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "def sentimentAnalyzerScore(text):\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stopWordListDel() function is used to delete all the stopwords present in the dataframe. It traverses the dataframe and removes the stopwords present in the wikipedia page data. For loop is used for this traversal. The stopwords are predefined in the nltk library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopWordListDel(cleanedData):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in range (0,len(cleanedData.index)):\n",
    "        sentence = cleanedData.iloc[i,1]\n",
    "        filteredSent = \" \"\n",
    "        pageWord = sentence.split()\n",
    "        for r in pageWord:\n",
    "            if not r in stop_words:\n",
    "                filteredSent = filteredSent + \" \" + r\n",
    "        cleanedData.iloc[i,1] = filteredSent\n",
    "    return cleanedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "listSentScore() uses a for loop to loop through the whole dataframe and return the sentiment score of each wikipedia page. This is stored in a new dataframe which contains the title of the page, the sentiment score and the compound sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listSentScore(data):\n",
    "    sentList = pd.DataFrame(columns=['Title','Sentiment Score','Compound Score'])\n",
    "    for i in range (0,len(data)):\n",
    "        score = sentimentAnalyzerScore(data.iloc[i,1])\n",
    "        sentList = sentList.append({'Title':data.iloc[i,0],'Sentiment Score':score,'Compound Score':score['compound']},ignore_index = True)\n",
    "    return sentList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sortAndPrint() function is used to sort the wikipedia pages according to the compound sentiment score of the wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortAndPrint(data):\n",
    "    data.sort_values(by=['Compound Score'],inplace = True)\n",
    "    data.head()\n",
    "    data.tail()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
