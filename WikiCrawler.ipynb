{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries.\n",
    "requests library for making HTTP requests.\n",
    "BeautifulSoup for extracting data from the http request.\n",
    "Pandas for storing data in DataFrames.\n",
    "CSV for writing the results into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count is the current count of the loop.\n",
    "totalPages is the total number of pages the crawler should visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "totalPages = 100\n",
    "csvName = \"dataFile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initResp is the function that initializes the whole process. The get method indicates that you are trying to get or retrieve data from a specified url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initResp(url):\n",
    "    response = requests.get(url)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to extract the title and content of each page. titleExtractor and contextExtractor do the job of extracting title and content respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titleExtractor(soup):\n",
    "    title = soup.find('title')\n",
    "    return title.string\n",
    "\n",
    "def contextExtractor(soup):\n",
    "    context = \" \"\n",
    "    for i in soup.select('p'):\n",
    "        context = context + i.getText()\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the contents are extracted, we need to store the results in a file. writeCSV is a function which writes the data into  a csv file. The csv file consists of two columns: Title of the page , Contents of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(title, para, fname):\n",
    "    with open(fname, 'a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter = ',')\n",
    "            field = [ ''+title.string , ''+para]\n",
    "            writer.writerow(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function which helps us jump from the current page to another wikipedia page. nextLink function collects all the links in the HTML page and returns a random link for another wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextLink(soup):\n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\",href =True)\n",
    "    random.shuffle(allLinks)\n",
    "    linkToScrape = 0  \n",
    "    print(\"No. of links : \",len(allLinks))\n",
    "    \n",
    "    for link in allLinks:\n",
    "        # We are only interested in other wiki articles\n",
    "        print('before loop 1: '+str(link['href']))\n",
    "        \n",
    "        if link['href'].find(\"/wiki/\") == -1:\n",
    "            #print('c')\n",
    "            continue\n",
    "\n",
    "        # Use this link to scrape\n",
    "        print('before loop 2: '+link['href'])\n",
    "        \n",
    "        if link['href'].find(\".org\") == -1:\n",
    "            linkToScrape = link\n",
    "            #print('b')\n",
    "            break\n",
    "    \n",
    "    print(\"Link returned : \", linkToScrape)\n",
    "    return linkToScrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape function is the function that calls all the other functions in a loop. A 7 second sleep has been introduced as the wikipedia server will block the program if we hit one page after another in a loop. scrape is a recursion function which calls itself until the count exceeds the total number of pages we want to hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    \n",
    "    global count\n",
    "    count = count+1\n",
    "    \n",
    "    global totalPages\n",
    "    \n",
    "    global csvName\n",
    "    \n",
    "    response=initResp(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = titleExtractor(soup)\n",
    "    print(title)\n",
    "    \n",
    "    context = contextExtractor(soup)\n",
    "    print(context)\n",
    "    \n",
    "    writeCSV(title,context, csvName)\n",
    "    \n",
    "    linkToScrape = nextLink(soup)\n",
    "    \n",
    "    print(\"Current Link cout: \" , count)\n",
    "    print('Next Link href: '+ str(linkToScrape['href']))\n",
    "    \n",
    "    time.sleep(7)\n",
    "    \n",
    "    if(count<totalPages):\n",
    "        scrape(\"https://en.wikipedia.org\" + str(linkToScrape['href']))\n",
    "    \n",
    "scrape(\"https://en.wikipedia.org/wiki/Web_scraping\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collecting part is done. Now we need to write functions to clean this data. We convert all the characters into lowercase and remove any special characters present in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(fileName):\n",
    "    data = pd.read_csv(fileName)\n",
    "    return data\n",
    "\n",
    "def lowerOnly(data):\n",
    "    for i in range (0,99):\n",
    "        data.iloc[i,0] = data.iloc[i,0].lower()\n",
    "        data.iloc[i,1] = data.iloc[i,1].lower()\n",
    "        #print(i)\n",
    "        \n",
    "def alphaCharOnly(data):\n",
    "    for j in range (0,99):\n",
    "        strTitleTemp = \" \"\n",
    "        for k in data.iloc[j,0].split(\"\\n\"):\n",
    "            final = \" \".join(re.findall(r\"[a-zA-Z0-9]+\", k))\n",
    "            strTitleTemp = strTitleTemp +\" \"+ final\n",
    "        data.iloc[j,0] = strTitleTemp\n",
    "        strContTemp = \" \"\n",
    "        for k in data.iloc[j,1].split(\"\\n\"):\n",
    "            final = \" \".join(re.findall(r\"[a-zA-Z0-9]+\", k))\n",
    "            strContTemp = strContTemp +\" \"+ final\n",
    "        data.iloc[j,1] = strContTemp\n",
    "        #print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(fileName):\n",
    "    data = readFile(fileName)\n",
    "    lowerOnly(data)\n",
    "    alphaCharOnly(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that cleaner function returns doesnot contain any special characters and all the characters are in lowercase"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
