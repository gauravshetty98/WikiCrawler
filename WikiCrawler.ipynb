{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Wikipedia data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in this project is to extract the data required. We extract this data from the Wikipedia webiste. We make use of BeautifulSoup library to scrape the data from the website. This data is then processed and used for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole notebook is divided into different parts:\n",
    "* Importing Libraries.\n",
    "* Defining different variables.\n",
    "* Defining the different functions neended. \n",
    "* Defining the main scraping function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the required libraries.\n",
    "requests library for making HTTP requests.\n",
    "BeautifulSoup for extracting data from the http request.\n",
    "Pandas for storing data in DataFrames.\n",
    "CSV for writing the results into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the different variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Count is the current count of the loop.\n",
    "* totalPages is the total number of pages from which the data should be extracted.\n",
    "* csvName is the name of the file in which we save the extracted raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "totalPages = 99\n",
    "csvName = \"finalDataFile\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the different functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each function is given a seperate task to perform. All these functions are then used by the scrape function to extract the data. The different functions used are:\n",
    "* initResp - initializes the web crawling process.\n",
    "* titleExtractor - extracts the title of the topic.\n",
    "* contextExtractor - extracts the data written about the topic.\n",
    "* writeCSV - stores the data into a csv file.\n",
    "* nextLink - jumps to the next Wikipedia link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initResp is the function that initializes the whole process. The get method indicates that you are trying to get or retrieve data from a specified url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initResp(url):\n",
    "    response = requests.get(url)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function to extract the title and content of each page. titleExtractor and contextExtractor do the job of extracting title and content respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def titleExtractor(soup):\n",
    "    title = soup.find('title')\n",
    "    return title.string\n",
    "\n",
    "def contextExtractor(soup):\n",
    "    context = \" \"\n",
    "    for i in soup.select('p'):\n",
    "        context = context + i.getText()\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the contents are extracted, we need to store the results in a file. writeCSV is a function which writes the data into  a csv file. The csv file consists of two columns: Title of the page , Contents of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeCSV(title, para, fname):\n",
    "    with open(fname, 'a', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file, delimiter = ',')\n",
    "            field = [ ''+title.string , ''+para]\n",
    "            writer.writerow(field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a function which helps us jump from the current page to another wikipedia page. nextLink function collects all the links in the HTML page and returns a random link for another wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nextLink(soup):\n",
    "    allLinks = soup.find(id=\"bodyContent\").find_all(\"a\",href =True)\n",
    "    random.shuffle(allLinks)\n",
    "    linkToScrape = 0  \n",
    "    #print(\"No. of links : \",len(allLinks))\n",
    "    \n",
    "    for link in allLinks:\n",
    "        # We are only interested in other wiki articles\n",
    "        #print('before loop 1: '+str(link['href']))\n",
    "        \n",
    "        if link['href'].find(\"/wiki/\") == -1:\n",
    "            #print('c')\n",
    "            continue\n",
    "\n",
    "        # Use this link to scrape\n",
    "        #print('before loop 2: '+link['href'])\n",
    "        \n",
    "        if link['href'].find(\".org\") == -1:\n",
    "            linkToScrape = link\n",
    "            #print('b')\n",
    "            break\n",
    "    \n",
    "    print(\"Link returned : \", linkToScrape)\n",
    "    return linkToScrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the scrape function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrape function is the function that calls all the other functions in a loop. A 7 second sleep has been introduced as the wikipedia server will block the program if we hit one page after another in a loop. scrape is a recursion function which calls itself until the count exceeds the total number of pages we want to hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape(url):\n",
    "    \n",
    "    global count\n",
    "    count = count+1\n",
    "    \n",
    "    global totalPages\n",
    "    \n",
    "    global csvName\n",
    "    \n",
    "    response=initResp(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    title = titleExtractor(soup)\n",
    "    #print(title)\n",
    "    \n",
    "    context = contextExtractor(soup)\n",
    "    #print(context)\n",
    "    \n",
    "    writeCSV(title,context, csvName)\n",
    "    \n",
    "    linkToScrape = nextLink(soup)\n",
    "    \n",
    "    print(\"Current Link count: \" , count)\n",
    "    #print('Next Link href: '+ str(linkToScrape['href']))\n",
    "    \n",
    "    time.sleep(7)\n",
    "    \n",
    "    if(count<totalPages):\n",
    "        scrape(\"https://en.wikipedia.org\" + str(linkToScrape['href']))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "#scrape(\"https://en.wikipedia.org/wiki/Web_scraping\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
